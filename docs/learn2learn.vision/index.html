<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Seb Arnold">
        <link rel="canonical" href="http://learn2learn.net/docs/learn2learn.vision/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>learn2learn.vision - learn2learn</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-68693545-3', 'seba-1511.github.com');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">learn2learn</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Tutorials <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../tutorials/getting_started/">Getting Started</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Documentation <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../learn2learn/">learn2learn</a>
</li>
                                    
<li >
    <a href="../learn2learn.algorithms/">learn2learn.algorithms</a>
</li>
                                    
<li >
    <a href="../learn2learn.data/">learn2learn.data</a>
</li>
                                    
<li >
    <a href="../learn2learn.gym/">learn2learn.gym</a>
</li>
                                    
<li class="active">
    <a href="./">learn2learn.vision</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="https://github.com/learnables/learn2learn/tree/master/examples">Examples</a>
                            </li>
                            <li >
                                <a href="https://github.com/learnables/learn2learn/">GitHub</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../learn2learn.gym/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="disabled">
                                <a rel="prev" >
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#learn2learnvisionmodels">learn2learn.vision.models</a></li>
            <li><a href="#omniglotfc">OmniglotFC</a></li>
            <li><a href="#omniglotcnn">OmniglotCNN</a></li>
            <li><a href="#miniimagenetcnn">MiniImagenetCNN</a></li>
        <li class="main "><a href="#learn2learnvisiondatasets">learn2learn.vision.datasets</a></li>
            <li><a href="#fullomniglot">FullOmniglot</a></li>
            <li><a href="#miniimagenet">MiniImagenet</a></li>
            <li><a href="#tieredimagenet">TieredImagenet</a></li>
            <li><a href="#fc100">FC100</a></li>
            <li><a href="#cifarfs">CIFARFS</a></li>
            <li><a href="#vggflower102">VGGFlower102</a></li>
            <li><a href="#fgvcaircraft">FGVCAircraft</a></li>
        <li class="main "><a href="#learn2learnvisiontransforms">learn2learn.vision.transforms</a></li>
            <li><a href="#randomclassrotation">RandomClassRotation</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="learn2learnvisionmodels">learn2learn.vision.models</h1>
<p><strong>Description</strong></p>
<p>A set of commonly used models for meta-learning vision tasks.</p>
<h2 id="omniglotfc">OmniglotFC</h2>
<pre><code class="python">OmniglotFC(input_size, output_size, sizes=None)
</code></pre>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016.</p>
<p><strong>References</strong></p>
<ol>
<li>Santoro et al. 2016. “Meta-Learning with Memory-Augmented Neural Networks.” ICML.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>input_size</strong> (int) - The dimensionality of the input.</li>
<li><strong>output_size</strong> (int) - The dimensionality of the output.</li>
<li><strong>sizes</strong> (list, <em>optional</em>, default=None) - A list of hidden layer sizes.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">net = OmniglotFC(input_size=28**2,
                 output_size=10,
                 sizes=[64, 64, 64])
</code></pre>

<h2 id="omniglotcnn">OmniglotCNN</h2>
<pre><code class="python">OmniglotCNN(output_size=5, hidden_size=64, layers=4)
</code></pre>

<p><a href="">Source</a></p>
<p><strong>Description</strong></p>
<p>The convolutional network commonly used for Omniglot, as described by Finn et al, 2017.</p>
<p>This network assumes inputs of shapes (1, 28, 28).</p>
<p><strong>References</strong></p>
<ol>
<li>Finn et al. 2017. “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.” ICML.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>output_size</strong> (int) - The dimensionality of the network's output.</li>
<li><strong>hidden_size</strong> (int, <em>optional</em>, default=64) - The dimensionality of the hidden representation.</li>
<li><strong>layers</strong> (int, <em>optional</em>, default=4) - The number of convolutional layers.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">model = OmniglotCNN(output_size=20, hidden_size=128, layers=3)
</code></pre>

<h2 id="miniimagenetcnn">MiniImagenetCNN</h2>
<pre><code class="python">MiniImagenetCNN(output_size, hidden_size=32, layers=4)
</code></pre>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017.</p>
<p>This network assumes inputs of shapes (3, 84, 84).</p>
<p><strong>References</strong></p>
<ol>
<li>Ravi and Larochelle. 2017. “Optimization as a Model for Few-Shot Learning.” ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>output_size</strong> (int) - The dimensionality of the network's output.</li>
<li><strong>hidden_size</strong> (int, <em>optional</em>, default=32) - The dimensionality of the hidden representation.</li>
<li><strong>layers</strong> (int, <em>optional</em>, default=4) - The number of convolutional layers.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">model = MiniImagenetCNN(output_size=20, hidden_size=128, layers=3)
</code></pre>

<h1 id="learn2learnvisiondatasets">learn2learn.vision.datasets</h1>
<p><strong>Description</strong></p>
<p>Some datasets commonly used in meta-learning vision tasks.</p>
<h2 id="fullomniglot">FullOmniglot</h2>
<pre><code class="python">FullOmniglot(root, transform=None, target_transform=None, download=False)
</code></pre>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>This class provides an interface to the Omniglot dataset.</p>
<p>The Omniglot dataset was introduced by Lake et al., 2015.
Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples.
While the original dataset is separated in background and evaluation sets,
this class concatenates both sets and leaves to the user the choice of classes splitting
as was done in Ravi and Larochelle, 2017.
The background and evaluation splits are available in the <code>torchvision</code> package.</p>
<p><strong>References</strong></p>
<ol>
<li>Lake et al. 2015. “Human-Level Concept Learning through Probabilistic Program Induction.” Science.</li>
<li>Ravi and Larochelle. 2017. “Optimization as a Model for Few-Shot Learning.” ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">omniglot = l2l.vision.datasets.FullOmniglot(root='./data',
                                            transform=transforms.Compose([
                                                transforms.Resize(28, interpolation=LANCZOS),
                                                transforms.ToTensor(),
                                                lambda x: 1.0 - x,
                                            ]),
                                            download=True)
omniglot = l2l.data.MetaDataset(omniglot)
</code></pre>

<h2 id="miniimagenet">MiniImagenet</h2>
<pre><code class="python">MiniImagenet(root, mode='train', transform=None, target_transform=None)
</code></pre>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/mini_imagenet.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The <em>mini</em>-ImageNet dataset was originally introduced by Vinyals et al., 2016.</p>
<p>It consists of 60'000 colour images of sizes 84x84 pixels.
The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples.
The classes are sampled from the ImageNet dataset, and we use the splits from Ravi &amp; Larochelle, 2017.</p>
<p><strong>References</strong></p>
<ol>
<li>Vinyals et al. 2016. “Matching Networks for One Shot Learning.” NeurIPS.</li>
<li>Ravi and Larochelle. 2017. “Optimization as a Model for Few-Shot Learning.” ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">train_dataset = l2l.vision.datasets.MiniImagenet(root='./data', mode='train')
train_dataset = l2l.data.MetaDataset(train_dataset)
train_generator = l2l.data.TaskGenerator(dataset=train_dataset, ways=ways)
</code></pre>

<h2 id="tieredimagenet">TieredImagenet</h2>
<pre><code class="python">TieredImagenet(root,
               mode='train',
               transform=None,
               target_transform=None,
               download=False)
</code></pre>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/tiered_imagenet.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The <em>tiered</em>-ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository.</p>
<p>Like <em>mini</em>-ImageNet, <em>tiered</em>-ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100.
The train-validation-test split is made such that classes from similar categories are in the same splits.
There are 34 categories each containing between 10 and 30 classes.
Of these categories, 20 (351 classes; 448,695 images) are used for training,
6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing.</p>
<p><strong>References</strong></p>
<ol>
<li>Ren et al, 2018. "Meta-Learning for Semi-Supervised Few-Shot Classification." ICLR '18.</li>
<li>Ren Mengye. 2018. "few-shot-ssl-public". <a href="https://github.com/renmengye/few-shot-ssl-public">https://github.com/renmengye/few-shot-ssl-public</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">train_dataset = l2l.vision.datasets.TieredImagenet(root='./data', mode='train', download=True)
train_dataset = l2l.data.MetaDataset(train_dataset)
train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)
</code></pre>

<h2 id="fc100">FC100</h2>
<pre><code class="python">FC100(root, mode='train', transform=None, target_transform=None)
</code></pre>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/fc100.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The FC100 dataset was originally introduced by Oreshkin et al., 2018.</p>
<p>It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are
split so as to minimize the information overlap between splits.
The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training,
4 (20 classes) for validation, and 4 (20 classes) for testing.
Each class contains 600 images.
The specific splits are provided in the Supplementary Material of the paper.
Our data is downloaded from the link provided by [2].</p>
<p><strong>References</strong></p>
<ol>
<li>Oreshkin et al. 2018. "TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning." NeurIPS.</li>
<li>Kwoonjoon Lee. 2019. "MetaOptNet." <a href="https://github.com/kjunelee/MetaOptNet">https://github.com/kjunelee/MetaOptNet</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">train_dataset = l2l.vision.datasets.FC100(root='./data', mode='train')
train_dataset = l2l.data.MetaDataset(train_dataset)
train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)
</code></pre>

<h2 id="cifarfs">CIFARFS</h2>
<pre><code class="python">CIFARFS(root, mode='train', transform=None, target_transform=None)
</code></pre>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/cifarfs.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019.</p>
<p>It consists of 60'000 colour images of sizes 32x32 pixels.
The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples.
The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019.</p>
<p><strong>References</strong></p>
<ol>
<li>Bertinetto et al. 2019. "Meta-learning with differentiable closed-form solvers". ICLR.</li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">train_dataset = l2l.vision.datasets.CIFARFS(root='./data', mode='train')
train_dataset = l2l.data.MetaDataset(train_dataset)
train_generator = l2l.data.TaskGenerator(dataset=train_dataset, ways=ways)
</code></pre>

<h2 id="vggflower102">VGGFlower102</h2>
<pre><code class="python">VGGFlower102(root,
             mode='all',
             transform=None,
             target_transform=None,
             download=False)
</code></pre>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/vgg_flowers.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020.</p>
<p>The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images.
We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al.</p>
<p><strong>References</strong></p>
<ol>
<li>Nilsback, M. and A. Zisserman. 2006. "A Visual Vocabulary for Flower Classification." CVPR '06.</li>
<li>Triantafillou et al. 2019. "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples." ICLR '20.</li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/data/flowers/">https://www.robots.ox.ac.uk/~vgg/data/flowers/</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">train_dataset = l2l.vision.datasets.VGGFlower102(root='./data', mode='train')
train_dataset = l2l.data.MetaDataset(train_dataset)
train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)
</code></pre>

<h2 id="fgvcaircraft">FGVCAircraft</h2>
<pre><code class="python">FGVCAircraft(root,
             mode='all',
             transform=None,
             target_transform=None,
             download=False)
</code></pre>

<p><a href="https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/fgvc_aircraft.py">[Source]</a></p>
<p><strong>Description</strong></p>
<p>The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020.</p>
<p>The dataset consists of 10,200 images of aircraft (102 classes, each 100 images).
We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al.</p>
<p><strong>References</strong></p>
<ol>
<li>Maji et al. 2013. "Fine-Grained Visual Classification of Aircraft." arXiv [cs.CV].</li>
<li>Triantafillou et al. 2019. "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples." ICLR '20.</li>
<li><a href="http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/">http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/</a></li>
</ol>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>root</strong> (str) - Path to download the data.</li>
<li><strong>mode</strong> (str, <em>optional</em>, default='train') - Which split to use.
    Must be 'train', 'validation', or 'test'.</li>
<li><strong>transform</strong> (Transform, <em>optional</em>, default=None) - Input pre-processing.</li>
<li><strong>target_transform</strong> (Transform, <em>optional</em>, default=None) - Target pre-processing.</li>
<li><strong>download</strong> (bool, <em>optional</em>, default=False) - Whether to download the dataset.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">train_dataset = l2l.vision.datasets.FGVCAircraft(root='./data', mode='train', download=True)
train_dataset = l2l.data.MetaDataset(train_dataset)
train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)
</code></pre>

<h1 id="learn2learnvisiontransforms">learn2learn.vision.transforms</h1>
<p><strong>Description</strong></p>
<p>A set of transformations commonly used in meta-learning vision tasks.</p>
<h2 id="randomclassrotation">RandomClassRotation</h2>
<pre><code class="python">RandomClassRotation(dataset, degrees)
</code></pre>

<p><a href="">[Source]</a></p>
<p><strong>Description</strong></p>
<p>Samples rotations from a given list uniformly at random, and applies it to
all images from a given class.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>degrees</strong> (list) - The rotations to be sampled.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">transform = RandomClassRotation([0, 90, 180, 270])
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.js" defer></script>
        <script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/mathtex-script-type.min.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
