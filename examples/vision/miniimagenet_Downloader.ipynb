{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "def mkdir(dir):\n",
    "    try:\n",
    "        os.mkdir(dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class MiniImageNet(Dataset):\n",
    "    def __init__(self, subset, transform=None, target_transform=None, data_path ='./data/'):\n",
    "        \"\"\"Dataset class representing miniImageNet dataset\n",
    "        # Arguments:\n",
    "            subset: Whether the dataset represents the background or evaluation set\n",
    "        \"\"\"\n",
    "                 \n",
    "        file_id = '0B3Irx3uQNoBMQ1FlNXJsZUdYWEE'\n",
    "        self.data_path = data_path\n",
    "        destination_for_zip = self.data_path + '/miniImageNet.zip'\n",
    "        destination_to_extract = self.data_path + '/miniImageNet/images'\n",
    "        mkdir(self.data_path + '/miniImageNet/images_background')\n",
    "        mkdir(self.data_path + '/miniImageNet/images_evaluation')\n",
    "\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        if not self.transform:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.Resize(84),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        \n",
    "        if not os.path.exists(destination_to_extract):\n",
    "            os.makedirs(destination_to_extract)\n",
    "            download_file_from_google_drive(file_id, destination_for_zip)\n",
    "            with zipfile.ZipFile(destination_for_zip, 'r') as zip_ref:\n",
    "                zip_ref.extractall(destination_to_extract)\n",
    "            \n",
    "            # Clean up folders\n",
    "            mkdir(self.data_path + '/miniImageNet/images_background')\n",
    "            mkdir(self.data_path + '/miniImageNet/images_evaluation')\n",
    "\n",
    "            # Find class identities\n",
    "            classes = []\n",
    "            for root, _, files in os.walk(self.data_path + '/miniImageNet/images/'):\n",
    "                for f in files:\n",
    "                    if f.endswith('.jpg'):\n",
    "                        classes.append(f[:-12])\n",
    "\n",
    "            classes = list(set(classes))\n",
    "\n",
    "            # Train/test split\n",
    "            np.random.seed(0)\n",
    "            np.random.shuffle(classes)\n",
    "            background_classes, evaluation_classes = classes[:80], classes[80:]\n",
    "\n",
    "            # Create class folders\n",
    "            for c in background_classes:\n",
    "                mkdir(self.data_path+ f'/miniImageNet/images_background/{c}/')\n",
    "\n",
    "            for c in evaluation_classes:\n",
    "                mkdir(self.data_path + f'/miniImageNet/images_evaluation/{c}/')\n",
    "\n",
    "            # Move images to correct location\n",
    "            for root, _, files in os.walk(self.data_path + '/miniImageNet/images'):\n",
    "                for f in files:\n",
    "                    if f.endswith('.jpg'):\n",
    "                        class_name = f[:-12]\n",
    "                        image_name = f[-12:]\n",
    "                        # Send to correct folder\n",
    "                        subset_folder = 'images_evaluation' if class_name in evaluation_classes else 'images_background'\n",
    "                        src = f'{root}/{f}'\n",
    "                        dst = self.data_path + f'/miniImageNet/{subset_folder}/{class_name}/{image_name}'\n",
    "                        shutil.copy(src, dst)\n",
    "                        \n",
    "            if subset not in ('background', 'evaluation'):\n",
    "                raise(ValueError, 'subset must be one of (background, evaluation)')\n",
    "            self.subset = subset\n",
    "\n",
    "            self.df = pd.DataFrame(self.index_subset(self.subset))\n",
    "\n",
    "            # Index of dataframe has direct correspondence to item in dataset\n",
    "            self.df = self.df.assign(id=self.df.index.values)\n",
    "\n",
    "            # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n",
    "            self.unique_characters = sorted(self.df['class_name'].unique())\n",
    "            self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n",
    "            self.df = self.df.assign(class_id=self.df['class_name'].apply(lambda c: self.class_name_to_id[c]))\n",
    "\n",
    "            # Create dicts\n",
    "            self.datasetid_to_filepath = self.df.to_dict()['filepath']\n",
    "            self.datasetid_to_class_id = self.df.to_dict()['class_id']\n",
    "        else:\n",
    "            print(\"Da\")\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = Image.open(self.datasetid_to_filepath[item])\n",
    "        instance = self.transform(instance)\n",
    "        if self.target_transform == None:\n",
    "            label = self.datasetid_to_class_id[item]\n",
    "        else:\n",
    "            label = self.datasetid_to_class_id[item]\n",
    "            label = self.target_transform(label)\n",
    "        return instance, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.df['class_name'].unique())\n",
    "\n",
    "    def index_subset(self, sbs):\n",
    "        \"\"\"Index a subset by looping through all of its files and recording relevant information.\n",
    "        # Arguments\n",
    "            subset: Name of the subset\n",
    "        # Returns\n",
    "            A list of dicts containing information about all the image files in a particular subset of the\n",
    "            miniImageNet dataset\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        print('Indexing {}...'.format(sbs))\n",
    "        subset_len = 0\n",
    "        for root, folders, files in os.walk(self.data_path + '/miniImageNet/images_{}/'.format(sbs)):\n",
    "            subset_len += len([f for f in files if f.endswith('.png')])\n",
    "\n",
    "        for root, folders, files in os.walk(self.data_path + '/miniImageNet/images_{}/'.format(sbs)):\n",
    "            if len(files) == 0:\n",
    "                continue\n",
    "\n",
    "            class_name = root.split('/')[-1]\n",
    "\n",
    "            for f in files:\n",
    "                images.append({\n",
    "                    'subset': sbs,\n",
    "                    'class_name': class_name,\n",
    "                    'filepath': os.path.join(root, f)\n",
    "                })\n",
    "\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders already exist\n",
      "Indexing background...\n",
      "Folders already exist\n",
      "Indexing evaluation...\n"
     ]
    }
   ],
   "source": [
    "import learn2learn as l2l\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "\n",
    "\n",
    "data_path = '/datadrive/test/'\n",
    "minimagenet_background = l2l.vision.datasets.MiniImageNet(subset ='background', data_path=data_path)\n",
    "minimagenet_evaluation = l2l.vision.datasets.MiniImageNet(subset ='evaluation', data_path=data_path)\n",
    "minimagenet = ConcatDataset((minimagenet_background, minimagenet_evaluation))\n",
    "minimagenet = l2l.data.MetaDataset(minimagenet)\n",
    "\n",
    "\n",
    "eval_generator = l2l.data.TaskGenerator(dataset=minimagenet, ways=5)\n",
    "support_t = eval_generator.sample(shots=8)\n",
    "query_t = eval_generator.sample(shots=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "36_fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
