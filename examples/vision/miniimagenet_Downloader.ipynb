{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders already exist\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "def mkdir(dir):\n",
    "    try:\n",
    "        os.mkdir(dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_id = '0B3Irx3uQNoBMQ1FlNXJsZUdYWEE'\n",
    "    \n",
    "    destination_for_zip = '/datadrive/test/miniImageNet.zip'\n",
    "    destination_to_extract = '/datadrive/test/miniImageNet/images'\n",
    "\n",
    "    if not os.path.exists(destination_to_extract):\n",
    "        os.makedirs(destination_to_extract)\n",
    "        download_file_from_google_drive(file_id, destination_for_zip)\n",
    "        with zipfile.ZipFile(destination_for_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(destination_to_extract)\n",
    "        DATA_PATH = '/datadrive/test/'\n",
    "        # Clean up folders\n",
    "        mkdir(DATA_PATH + '/miniImageNet/images_background')\n",
    "        mkdir(DATA_PATH + '/miniImageNet/images_evaluation')\n",
    "\n",
    "        # Find class identities\n",
    "        classes = []\n",
    "        for root, _, files in os.walk(DATA_PATH + '/miniImageNet/images/'):\n",
    "            for f in files:\n",
    "                if f.endswith('.jpg'):\n",
    "                    classes.append(f[:-12])\n",
    "\n",
    "        classes = list(set(classes))\n",
    "\n",
    "        # Train/test split\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(classes)\n",
    "        background_classes, evaluation_classes = classes[:80], classes[80:]\n",
    "\n",
    "        # Create class folders\n",
    "        for c in background_classes:\n",
    "            mkdir(DATA_PATH + f'/miniImageNet/images_background/{c}/')\n",
    "\n",
    "        for c in evaluation_classes:\n",
    "            mkdir(DATA_PATH + f'/miniImageNet/images_evaluation/{c}/')\n",
    "\n",
    "        # Move images to correct location\n",
    "        for root, _, files in os.walk(DATA_PATH + '/miniImageNet/images'):\n",
    "            for f in files:\n",
    "                if f.endswith('.jpg'):\n",
    "                    class_name = f[:-12]\n",
    "                    image_name = f[-12:]\n",
    "                    # Send to correct folder\n",
    "                    subset_folder = 'images_evaluation' if class_name in evaluation_classes else 'images_background'\n",
    "                    src = f'{root}/{f}'\n",
    "                    dst = DATA_PATH + f'/miniImageNet/{subset_folder}/{class_name}/{image_name}'\n",
    "                    shutil.copy(src, dst)\n",
    "    else:\n",
    "        print(\"Folders already exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class MiniImageNet(Dataset):\n",
    "    def __init__(self, subset):\n",
    "        \"\"\"Dataset class representing miniImageNet dataset\n",
    "        # Arguments:\n",
    "            subset: Whether the dataset represents the background or evaluation set\n",
    "        \"\"\"\n",
    "        if subset not in ('background', 'evaluation'):\n",
    "            raise(ValueError, 'subset must be one of (background, evaluation)')\n",
    "        self.subset = subset\n",
    "\n",
    "        self.df = pd.DataFrame(self.index_subset(self.subset))\n",
    "\n",
    "        # Index of dataframe has direct correspondence to item in dataset\n",
    "        self.df = self.df.assign(id=self.df.index.values)\n",
    "\n",
    "        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n",
    "        self.unique_characters = sorted(self.df['class_name'].unique())\n",
    "        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n",
    "        self.df = self.df.assign(class_id=self.df['class_name'].apply(lambda c: self.class_name_to_id[c]))\n",
    "\n",
    "        # Create dicts\n",
    "        self.datasetid_to_filepath = self.df.to_dict()['filepath']\n",
    "        self.datasetid_to_class_id = self.df.to_dict()['class_id']\n",
    "\n",
    "        # Setup transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Resize(84),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        instance = Image.open(self.datasetid_to_filepath[item])\n",
    "        instance = self.transform(instance)\n",
    "        label = self.datasetid_to_class_id[item]\n",
    "        return instance, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.df['class_name'].unique())\n",
    "\n",
    "    @staticmethod\n",
    "    def index_subset(subset):\n",
    "        \"\"\"Index a subset by looping through all of its files and recording relevant information.\n",
    "        # Arguments\n",
    "            subset: Name of the subset\n",
    "        # Returns\n",
    "            A list of dicts containing information about all the image files in a particular subset of the\n",
    "            miniImageNet dataset\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        print('Indexing {}...'.format(subset))\n",
    "        subset_len = 0\n",
    "        for root, folders, files in os.walk(DATA_PATH + '/miniImageNet/images_{}/'.format(subset)):\n",
    "            subset_len += len([f for f in files if f.endswith('.png')])\n",
    "\n",
    "        for root, folders, files in os.walk(DATA_PATH + '/miniImageNet/images_{}/'.format(subset)):\n",
    "            if len(files) == 0:\n",
    "                continue\n",
    "\n",
    "            class_name = root.split('/')[-1]\n",
    "\n",
    "            for f in files:\n",
    "                images.append({\n",
    "                    'subset': subset,\n",
    "                    'class_name': class_name,\n",
    "                    'filepath': os.path.join(root, f)\n",
    "                })\n",
    "\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing background...\n"
     ]
    }
   ],
   "source": [
    "minimagenet = MiniImageNet(subset ='background')\n",
    "minimagenet = l2l.data.MetaDataset(minimagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import learn2learn as l2l\n",
    "\n",
    "eval_generator = l2l.data.TaskGenerator(dataset=minimagenet, ways=5)\n",
    "support_t = eval_generator.sample(shots=8)\n",
    "query_t = eval_generator.sample(shots=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "36_fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
