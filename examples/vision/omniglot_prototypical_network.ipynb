{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/debo/learn2learn/')\n",
    "sys.path.append('../../learn2learn/')\n",
    "from torch.autograd import grad\n",
    "from utils import clone_module\n",
    "from torch import nn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import learn2learn as l2l\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways=5\n",
    "shots=1\n",
    "meta_lr=0.003\n",
    "fast_lr=0.5\n",
    "meta_batch_size=32\n",
    "adaptation_steps=1\n",
    "num_iterations=60000\n",
    "cuda=True\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL.Image import LANCZOS\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "device = th.device('cpu')\n",
    "if cuda:\n",
    "    th.cuda.manual_seed(seed)\n",
    "    device = th.device('cuda')\n",
    "\n",
    "# Create Dataset\n",
    "# TODO: Create l2l.data.vision.FullOmniglot, which merges background and evaluation sets.\n",
    "omni_background = Omniglot(root='./data',\n",
    "                           background=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(28, interpolation=LANCZOS),\n",
    "                               transforms.ToTensor(),\n",
    "                               # TODO: Add DiscreteRotations([0, 90, 180, 270])\n",
    "                               lambda x: 1.0 - x,\n",
    "                           ]),\n",
    "                           download=True)\n",
    "#    max_y = 1 + max([y for X, y in omni_background])\n",
    "max_y = 964\n",
    "omni_evaluation = Omniglot(root='./data',\n",
    "                           background=False,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(28, interpolation=LANCZOS),\n",
    "                               transforms.ToTensor(),\n",
    "                               # TODO: Add DiscreteRotations([0, 90, 180, 270])\n",
    "                               lambda x: 1.0 - x,\n",
    "                           ]),\n",
    "                           target_transform=transforms.Compose([\n",
    "                               lambda x: max_y + x,\n",
    "                           ]),\n",
    "                           download=True)\n",
    "omniglot = ConcatDataset((omni_background, omni_evaluation))\n",
    "train_generator = l2l.data.TaskGenerator(dataset=omniglot, ways=ways)\n",
    "valid_generator = l2l.data.TaskGenerator(dataset=omniglot, ways=ways)\n",
    "test_generator = l2l.data.TaskGenerator(dataset=omniglot, ways=ways)\n",
    "# TODO: Implement an easy way to split one dataset into splits, based on classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (4): Flatten()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_input_channels = 1\n",
    "def conv_block(in_channels: int, out_channels: int) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    )\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "model = nn.Sequential(\n",
    "        conv_block(num_input_channels, 64),\n",
    "        conv_block(64, 64),\n",
    "        conv_block(64, 64),\n",
    "        conv_block(64, 64),\n",
    "        Flatten(),\n",
    "    )\n",
    "model.to(device, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn import Module\n",
    "from typing import Callable\n",
    "import torch\n",
    "\n",
    "\n",
    "def pairwise_distances(x: torch.Tensor,\n",
    "                       y: torch.Tensor,\n",
    "                       matching_fn: str) -> torch.Tensor:\n",
    "    \"\"\"Efficiently calculate pairwise distances (or other similarity scores) between\n",
    "    two sets of samples.\n",
    "\n",
    "    # Arguments\n",
    "        x: Query samples. A tensor of shape (n_x, d) where d is the embedding dimension\n",
    "        y: Class prototypes. A tensor of shape (n_y, d) where d is the embedding dimension\n",
    "        matching_fn: Distance metric/similarity score to compute between samples\n",
    "    \"\"\"\n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "\n",
    "    if matching_fn == 'l2':\n",
    "        distances = (\n",
    "                x.unsqueeze(1).expand(n_x, n_y, -1) -\n",
    "                y.unsqueeze(0).expand(n_x, n_y, -1)\n",
    "        ).pow(2).sum(dim=2)\n",
    "        return distances\n",
    "    elif matching_fn == 'cosine':\n",
    "        normalised_x = x / (x.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "        normalised_y = y / (y.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "\n",
    "        expanded_x = normalised_x.unsqueeze(1).expand(n_x, n_y, -1)\n",
    "        expanded_y = normalised_y.unsqueeze(0).expand(n_x, n_y, -1)\n",
    "\n",
    "        cosine_similarities = (expanded_x * expanded_y).sum(dim=2)\n",
    "        return 1 - cosine_similarities\n",
    "    elif matching_fn == 'dot':\n",
    "        expanded_x = x.unsqueeze(1).expand(n_x, n_y, -1)\n",
    "        expanded_y = y.unsqueeze(0).expand(n_x, n_y, -1)\n",
    "\n",
    "        return -(expanded_x * expanded_y).sum(dim=2)\n",
    "    else:\n",
    "        raise(ValueError('Unsupported similarity function'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def proto_net_episode(model: Module,\n",
    "                      optimiser: Optimizer,\n",
    "                      loss_fn: Callable,\n",
    "                      x: torch.Tensor,\n",
    "                      y: torch.Tensor,\n",
    "                      n_shot: int,\n",
    "                      k_way: int,\n",
    "                      q_queries: int,\n",
    "                      distance: str,\n",
    "                      train: bool):\n",
    "    \"\"\"Performs a single training episode for a Prototypical Network.\n",
    "\n",
    "    # Arguments\n",
    "        model: Prototypical Network to be trained.\n",
    "        optimiser: Optimiser to calculate gradient step\n",
    "        loss_fn: Loss function to calculate between predictions and outputs. Should be cross-entropy\n",
    "        x: Input samples of few shot classification task\n",
    "        y: Input labels of few shot classification task\n",
    "        n_shot: Number of examples per class in the support set\n",
    "        k_way: Number of classes in the few shot classification task\n",
    "        q_queries: Number of examples per class in the query set\n",
    "        distance: Distance metric to use when calculating distance between class prototypes and queries\n",
    "        train: Whether (True) or not (False) to perform a parameter update\n",
    "\n",
    "    # Returns\n",
    "        loss: Loss of the Prototypical Network on this task\n",
    "        y_pred: Predicted class probabilities for the query set on this task\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        # Zero gradients\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    # Embed all samples\n",
    "    embeddings = model(x)\n",
    "\n",
    "    # Samples are ordered by the NShotWrapper class as follows:\n",
    "    # k lots of n support samples from a particular class\n",
    "    # k lots of q query samples from those classes\n",
    "    support = embeddings[:n_shot*k_way]\n",
    "    queries = embeddings[n_shot*k_way:]\n",
    "    prototypes = compute_prototypes(support, k_way, n_shot)\n",
    "\n",
    "    # Calculate squared distances between all queries and all prototypes\n",
    "    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n",
    "    distances = pairwise_distances(queries, prototypes, distance)\n",
    "\n",
    "    # Calculate log p_{phi} (y = k | x)\n",
    "    log_p_y = (-distances).log_softmax(dim=1)\n",
    "\n",
    "    print('size is ', log_p_y)\n",
    "    print('size y is ', y)\n",
    "    loss = loss_fn(log_p_y, y)\n",
    "\n",
    "    # Prediction probabilities are softmax over distances\n",
    "    y_pred = (-distances).softmax(dim=1)\n",
    "\n",
    "    if train:\n",
    "        # Take gradient step\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return loss, y_pred\n",
    "\n",
    "\n",
    "def compute_prototypes(support: torch.Tensor, k: int, n: int) -> torch.Tensor:\n",
    "    \"\"\"Compute class prototypes from support samples.\n",
    "\n",
    "    # Arguments\n",
    "        support: torch.Tensor. Tensor of shape (n * k, d) where d is the embedding\n",
    "            dimension.\n",
    "        k: int. \"k-way\" i.e. number of classes in the classification task\n",
    "        n: int. \"n-shot\" of the classification task\n",
    "\n",
    "    # Returns\n",
    "        class_prototypes: Prototypes aka mean embeddings for each class\n",
    "    \"\"\"\n",
    "    # Reshape so the first dimension indexes by class then take the mean\n",
    "    # along that dimension to generate the \"prototypes\" for each class\n",
    "    class_prototypes = support.reshape(k, n, -1).mean(dim=1)\n",
    "    return class_prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from PIL.Image import LANCZOS\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "device = th.device('cpu')\n",
    "if cuda:\n",
    "    th.cuda.manual_seed(seed)\n",
    "    device = th.device('cuda')\n",
    "\n",
    "# Create Dataset\n",
    "# TODO: Create l2l.data.vision.FullOmniglot, which merges background and evaluation sets.\n",
    "omni_background = Omniglot(root='./data',\n",
    "                           background=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(28, interpolation=LANCZOS),\n",
    "                               transforms.ToTensor(),\n",
    "                               # TODO: Add DiscreteRotations([0, 90, 180, 270])\n",
    "                               lambda x: 1.0 - x,\n",
    "                           ]),\n",
    "                           download=True)\n",
    "#    max_y = 1 + max([y for X, y in omni_background])\n",
    "max_y = 964\n",
    "omni_evaluation = Omniglot(root='./data',\n",
    "                           background=False,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(28, interpolation=LANCZOS),\n",
    "                               transforms.ToTensor(),\n",
    "                               # TODO: Add DiscreteRotations([0, 90, 180, 270])\n",
    "                               lambda x: 1.0 - x,\n",
    "                           ]),\n",
    "                           target_transform=transforms.Compose([\n",
    "                               lambda x: max_y + x,\n",
    "                           ]),\n",
    "                           download=True)\n",
    "omniglot = ConcatDataset((omni_background, omni_evaluation))\n",
    "train_generator = l2l.data.TaskGenerator(dataset=omniglot, ways=ways)\n",
    "valid_generator = l2l.data.TaskGenerator(dataset=omniglot, ways=ways)\n",
    "test_generator = l2l.data.TaskGenerator(dataset=omniglot, ways=ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_data = train_generator.sample(shots=shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = torch.stack(adaptation_data.data).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28, 28])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28, 28])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptation_data.data[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 4, 2, 1]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptation_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptation_data = train_generator.sample(shots=shots)\n",
    "evaluation_data = train_generator.sample(shots=shots, classes_to_sample=adaptation_data.sampled_classes)\n",
    "support_x = torch.stack(adaptation_data.data).double().to(device)\n",
    "evaluation_x = torch.stack(evaluation_data.data).double().to(device)\n",
    "y = torch.DoubleTensor(adaptation_data.label)\n",
    "n_shot = shots\n",
    "k_way = ways\n",
    "distance='l2'\n",
    "train=True\n",
    "if train:\n",
    "    # Zero gradients\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "else:\n",
    "    model.eval()\n",
    "\n",
    "# Embed all samples\n",
    "embeddings = model(x)\n",
    "\n",
    "# Samples are ordered by the NShotWrapper class as follows:\n",
    "# k lots of n support samples from a particular class\n",
    "# k lots of q query samples from those classes\n",
    "support = model(support_x)\n",
    "queries = model(evaluation_x)\n",
    "prototypes = compute_prototypes(support, k_way, n_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size is  tensor([[-1.2249e-03, -1.4098e+01, -3.9641e+01, -1.2086e+01, -6.7107e+00],\n",
      "        [-2.7582e+00, -7.9253e-02, -4.3593e+00, -1.5438e+01, -1.9832e+01],\n",
      "        [-2.2861e+01, -1.3791e+01, -1.0247e-06, -3.4640e+01, -5.5083e+01],\n",
      "        [-4.7291e+00, -8.1878e+00, -1.6292e+01, -9.1559e-03, -1.3419e+01],\n",
      "        [-3.1144e-01, -9.3519e+00, -2.9496e+01, -7.3150e+00, -1.3210e+00]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<LogSoftmaxBackward>)\n",
      "size y is  tensor([4, 1, 3, 2, 0], device='cuda:0')\n",
      "this should work tensor(11.6066, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<NllLossBackward>) tensor([[9.9878e-01, 7.5359e-07, 6.0819e-18, 5.6387e-06, 1.2178e-03],\n",
      "        [6.3406e-02, 9.2381e-01, 1.2788e-02, 1.9733e-07, 2.4375e-09],\n",
      "        [1.1791e-10, 1.0246e-06, 1.0000e+00, 9.0403e-16, 1.1955e-24],\n",
      "        [8.8345e-03, 2.7802e-04, 8.4033e-08, 9.9089e-01, 1.4866e-06],\n",
      "        [7.3239e-01, 8.6798e-05, 1.5491e-13, 6.6548e-04, 2.6686e-01]],\n",
      "       device='cuda:0', dtype=torch.float64, grad_fn=<SoftmaxBackward>)\n",
      "size is  tensor([], device='cuda:0', size=(0, 5), dtype=torch.float64,\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "size y is  tensor([4., 1., 3., 2., 0.], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (0) to match target batch_size (5).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-726b5248d4f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                       \u001b[0mq_queries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                       \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                       train=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-119-dc48b7ccf9b4>\u001b[0m in \u001b[0;36mproto_net_episode\u001b[0;34m(model, optimiser, loss_fn, x, y, n_shot, k_way, q_queries, distance, train)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_p_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'size y is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Prediction probabilities are softmax over distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1822\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (0) to match target batch_size (5)."
     ]
    }
   ],
   "source": [
    "# Calculate squared distances between all queries and all prototypes\n",
    "# Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n",
    "distances = pairwise_distances(queries, prototypes, distance)\n",
    "\n",
    "# Calculate log p_{phi} (y = k | x)\n",
    "log_p_y = (-distances).log_softmax(dim=1).to(device)\n",
    "y = y.long()\n",
    "\n",
    "print('size is ', log_p_y)\n",
    "print('size y is ', y)\n",
    "loss = loss_fn(log_p_y, y)\n",
    "\n",
    "# Prediction probabilities are softmax over distances\n",
    "y_pred = (-distances).softmax(dim=1)\n",
    "\n",
    "if train:\n",
    "    # Take gradient step\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print('this should work', loss, y_pred)\n",
    "\n",
    "optimiser = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.NLLLoss().cuda()\n",
    "batch_support = torch.stack(adaptation_data.data).double().to(device)\n",
    "batch_label = torch.DoubleTensor(adaptation_data.label)\n",
    "proto_net_episode(model=model,optimiser=optimiser,loss_fn=loss_fn,x=batch_support,\n",
    "                  y=batch_label,\n",
    "                n_shot=shots,\n",
    "                      k_way=ways,\n",
    "                      q_queries=shots,\n",
    "                      distance='l2',\n",
    "                      train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.Tensor([[1.,2., 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.reshape(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_adapt(adaptation_data, evaluation_data, learner, loss, adaptation_steps, device):\n",
    "    for step in range(adaptation_steps):\n",
    "        data = [d for d in adaptation_data]\n",
    "        X = th.cat([d[0] for d in data], dim=0).to(device)\n",
    "        y = th.cat([th.tensor(d[1]).view(-1) for d in data], dim=0).to(device)\n",
    "        train_error = loss(learner(X), y)\n",
    "        train_error /= len(adaptation_data)\n",
    "        learner.adapt(train_error)\n",
    "    data = [d for d in evaluation_data]\n",
    "    X = th.cat([d[0] for d in data], dim=0).to(device)\n",
    "    y = th.cat([th.tensor(d[1]).view(-1) for d in data], dim=0).to(device)\n",
    "    predictions = learner(X)\n",
    "    valid_error = loss(predictions, y)\n",
    "    valid_error /= len(evaluation_data)\n",
    "    valid_accuracy = accuracy(predictions, y)\n",
    "    return valid_error, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 64 1 3, but got 3-dimensional input of size [5, 28, 28] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8664485da715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                            \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                                            \u001b[0madaptation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                                            device)\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mevaluation_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmeta_train_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mevaluation_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-6a9c4026f6e5>\u001b[0m in \u001b[0;36mfast_adapt\u001b[0;34m(adaptation_data, evaluation_data, learner, loss, adaptation_steps, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_error\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaptation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-cc98555573b7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/fewshot/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 64 1 3, but got 3-dimensional input of size [5, 28, 28] instead"
     ]
    }
   ],
   "source": [
    "model.to(device, dtype=torch.double)\n",
    "ptnet = ProtoNet(model, lr=fast_lr, first_order=False)\n",
    "opt = optim.Adam(model.parameters(), meta_lr)\n",
    "loss = nn.CrossEntropyLoss(size_average=True, reduction='mean')\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    opt.zero_grad()\n",
    "    meta_train_error = 0.0\n",
    "    meta_train_accuracy = 0.0\n",
    "    meta_valid_error = 0.0\n",
    "    meta_valid_accuracy = 0.0\n",
    "    meta_test_error = 0.0\n",
    "    meta_test_accuracy = 0.0\n",
    "    for task in range(meta_batch_size):\n",
    "        # Compute meta-training loss\n",
    "        learner = ptnet.clone()\n",
    "        adaptation_data = train_generator.sample(shots=shots)\n",
    "        evaluation_data = train_generator.sample(shots=shots,\n",
    "                                                 classes_to_sample=adaptation_data.sampled_classes)\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(adaptation_data,\n",
    "                                                           evaluation_data,\n",
    "                                                           learner,\n",
    "                                                           loss,\n",
    "                                                           adaptation_steps,\n",
    "                                                           device)\n",
    "        evaluation_error.backward()\n",
    "        meta_train_error += evaluation_error.item()\n",
    "        meta_train_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "        # Compute meta-validation loss\n",
    "        learner = ptnet.clone()\n",
    "        adaptation_data = valid_generator.sample(shots=shots)\n",
    "        evaluation_data = valid_generator.sample(shots=shots,\n",
    "                                                 classes_to_sample=adaptation_data.sampled_classes)\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(adaptation_data,\n",
    "                                                           evaluation_data,\n",
    "                                                           learner,\n",
    "                                                           loss,\n",
    "                                                           adaptation_steps,\n",
    "                                                           device)\n",
    "        meta_valid_error += evaluation_error.item()\n",
    "        meta_valid_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "        # Compute meta-testing loss\n",
    "        learner = ptnet.clone()\n",
    "        adaptation_data = test_generator.sample(shots=shots)\n",
    "        evaluation_data = test_generator.sample(shots=shots,\n",
    "                                                classes_to_sample=adaptation_data.sampled_classes)\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(adaptation_data,\n",
    "                                                           evaluation_data,\n",
    "                                                           learner,\n",
    "                                                           loss,\n",
    "                                                           adaptation_steps,\n",
    "                                                           device)\n",
    "        meta_test_error += evaluation_error.item()\n",
    "        meta_test_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "    # Print some metrics\n",
    "    print('\\n')\n",
    "    print('Iteration', iteration)\n",
    "    print('Meta Train Error', meta_train_error / meta_batch_size)\n",
    "    print('Meta Train Accuracy', meta_train_accuracy / meta_batch_size)\n",
    "    print('Meta Valid Error', meta_valid_error / meta_batch_size)\n",
    "    print('Meta Valid Accuracy', meta_valid_accuracy / meta_batch_size)\n",
    "    print('Meta Test Error', meta_test_error / meta_batch_size)\n",
    "    print('Meta Test Accuracy', meta_test_accuracy / meta_batch_size)\n",
    "\n",
    "    # Average the accumulated gradients and optimize\n",
    "    for p in ptnet.parameters():\n",
    "        p.grad.data.mul_(1.0 / meta_batch_size)\n",
    "    opt.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "36_fewshot",
   "language": "python",
   "name": "fewshot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
