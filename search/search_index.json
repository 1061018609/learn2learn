{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"learn2learn is a software library for meta-learning research. learn2learn builds on top of PyTorch to accelerate two aspects of the meta-learning research cycle: fast prototyping , essential in letting researchers quickly try new ideas, and correct reproducibility , ensuring that these ideas are evaluated fairly. learn2learn provides low-level utilities and unified interface to create new algorithms and domains, together with high-quality implementations of existing algorithms and standardized benchmarks. It retains compatibility with torchvision , torchaudio , torchtext , cherry , and any other PyTorch-based library you might be using. Overview learn2learn.data : TaskDataset and transforms to create few-shot tasks from any PyTorch dataset. learn2learn.vision : Models, datasets, and benchmarks for computer vision and few-shot learning. learn2learn.gym : Environment and utilities for meta-reinforcement learning. learn2learn.algorithms : High-level wrappers for existing meta-learning algorithms. learn2learn.optim : Utilities and algorithms for differentiable optimization and meta-descent. Resources Website: http://learn2learn.net/ Documentation: http://learn2learn.net/docs/ Tutorials: http://learn2learn.net/tutorials/getting_started/ Examples: https://github.com/learnables/learn2learn/tree/master/examples GitHub: https://github.com/learnables/learn2learn/ Slack: http://slack.learn2learn.net/ Installation \u00b6 1 pip install learn2learn Snippets & Examples \u00b6 The following snippets provide a sneak peek at the functionalities of learn2learn. High-level Wrappers \u00b6 Few-Shot Learning with MAML For more algorithms (ProtoNets, ANIL, Meta-SGD, Reptile, Meta-Curvature, KFO) refer to the examples folder. Most of them can be implemented with with the GBML wrapper. ( documentation ). 1 2 3 4 5 6 7 8 9 10 maml = l2l . algorithms . MAML ( model , lr = 0.1 ) opt = torch . optim . SGD ( maml . parameters (), lr = 0.001 ) for iteration in range ( 10 ): opt . zero_grad () task_model = maml . clone () # torch.clone() for nn.Modules adaptation_loss = compute_loss ( task_model ) task_model . adapt ( adaptation_loss ) # computes gradient, update task_model in-place evaluation_loss = compute_loss ( task_model ) evaluation_loss . backward () # gradients w.r.t. maml.parameters() opt . step () Meta-Descent with Hypergradient Learn any kind of optimization algorithm with the LearnableOptimizer . ( example and documentation ) 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( l2l . nn . Scale ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) # metaopt has .step() opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) # metaopt also has .parameters() metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear Learning Domains \u00b6 Custom Few-Shot Dataset Many standardized datasets (Omniglot, mini-/tiered-ImageNet, FC100, CIFAR-FS) are readily available in learn2learn.vision.datasets . ( documentation ) 1 2 3 4 5 6 7 8 9 10 dataset = l2l . data . MetaDataset ( MyDataset ()) # any PyTorch dataset transforms = [ # Easy to define your own transform l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task # Meta-train on the task Environments and Utilities for Meta-RL Parallelize your own meta-environments with AsyncVectorEnv , or use the standardized ones. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 def make_env (): env = l2l . gym . HalfCheetahForwardBackwardEnv () env = cherry . envs . ActionSpaceScaler ( env ) return env env = l2l . gym . AsyncVectorEnv ([ make_env for _ in range ( 16 )]) # uses 16 threads for task_config in env . sample_tasks ( 20 ): env . set_task ( task ) # all threads receive the same task state = env . reset () # use standard Gym API action = my_policy ( env ) env . step ( action ) Low-Level Utilities \u00b6 Differentiable Optimization Learn and differentiate through updates of PyTorch Modules. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 12 13 model = MyModel () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) learned_update = l2l . optim . ParameterUpdate ( # learnable update function model . parameters (), transform ) clone = l2l . clone_module ( model ) # torch.clone() for nn.Modules error = loss ( clone ( X ), y ) updates = learned_update ( # similar API as torch.autograd.grad error , clone . parameters (), create_graph = True , ) l2l . update_module ( clone , updates = updates ) loss ( clone ( X ), y ) . backward () # Gradients w.r.t model.parameters() and learned_update.parameters() Changelog \u00b6 A human-readable changelog is available in the CHANGELOG.md file. Citation \u00b6 To cite the learn2learn repository in your academic publications, please use the following reference. Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner. \"learn2learn\" . https://github.com/learnables/learn2learn , 2019. You can also use the following Bibtex entry. 1 2 3 4 5 6 7 @misc { learn2learn2019 , author = {Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner} , title = {learn2learn} , month = sep , year = 2019 , url = {https://github.com/learnables/learn2learn} } Acknowledgements & Friends \u00b6 The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License. TorchMeta is similar library, with a focus on datasets for supervised meta-learning. higher is a PyTorch library that enables differentiating through optimization inner-loops. While they monkey-patch nn.Module to be stateless, learn2learn retains the stateful PyTorch look-and-feel. For more information, refer to their ArXiv paper .","title":"Home"},{"location":"#installation","text":"1 pip install learn2learn","title":"Installation"},{"location":"#snippets-examples","text":"The following snippets provide a sneak peek at the functionalities of learn2learn.","title":"Snippets &amp; Examples"},{"location":"#high-level-wrappers","text":"Few-Shot Learning with MAML For more algorithms (ProtoNets, ANIL, Meta-SGD, Reptile, Meta-Curvature, KFO) refer to the examples folder. Most of them can be implemented with with the GBML wrapper. ( documentation ). 1 2 3 4 5 6 7 8 9 10 maml = l2l . algorithms . MAML ( model , lr = 0.1 ) opt = torch . optim . SGD ( maml . parameters (), lr = 0.001 ) for iteration in range ( 10 ): opt . zero_grad () task_model = maml . clone () # torch.clone() for nn.Modules adaptation_loss = compute_loss ( task_model ) task_model . adapt ( adaptation_loss ) # computes gradient, update task_model in-place evaluation_loss = compute_loss ( task_model ) evaluation_loss . backward () # gradients w.r.t. maml.parameters() opt . step () Meta-Descent with Hypergradient Learn any kind of optimization algorithm with the LearnableOptimizer . ( example and documentation ) 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( l2l . nn . Scale ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) # metaopt has .step() opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) # metaopt also has .parameters() metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear","title":"High-level Wrappers"},{"location":"#learning-domains","text":"Custom Few-Shot Dataset Many standardized datasets (Omniglot, mini-/tiered-ImageNet, FC100, CIFAR-FS) are readily available in learn2learn.vision.datasets . ( documentation ) 1 2 3 4 5 6 7 8 9 10 dataset = l2l . data . MetaDataset ( MyDataset ()) # any PyTorch dataset transforms = [ # Easy to define your own transform l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task # Meta-train on the task Environments and Utilities for Meta-RL Parallelize your own meta-environments with AsyncVectorEnv , or use the standardized ones. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 def make_env (): env = l2l . gym . HalfCheetahForwardBackwardEnv () env = cherry . envs . ActionSpaceScaler ( env ) return env env = l2l . gym . AsyncVectorEnv ([ make_env for _ in range ( 16 )]) # uses 16 threads for task_config in env . sample_tasks ( 20 ): env . set_task ( task ) # all threads receive the same task state = env . reset () # use standard Gym API action = my_policy ( env ) env . step ( action )","title":"Learning Domains"},{"location":"#low-level-utilities","text":"Differentiable Optimization Learn and differentiate through updates of PyTorch Modules. ( documentation ) 1 2 3 4 5 6 7 8 9 10 11 12 13 model = MyModel () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) learned_update = l2l . optim . ParameterUpdate ( # learnable update function model . parameters (), transform ) clone = l2l . clone_module ( model ) # torch.clone() for nn.Modules error = loss ( clone ( X ), y ) updates = learned_update ( # similar API as torch.autograd.grad error , clone . parameters (), create_graph = True , ) l2l . update_module ( clone , updates = updates ) loss ( clone ( X ), y ) . backward () # Gradients w.r.t model.parameters() and learned_update.parameters()","title":"Low-Level Utilities"},{"location":"#changelog","text":"A human-readable changelog is available in the CHANGELOG.md file.","title":"Changelog"},{"location":"#citation","text":"To cite the learn2learn repository in your academic publications, please use the following reference. Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner. \"learn2learn\" . https://github.com/learnables/learn2learn , 2019. You can also use the following Bibtex entry. 1 2 3 4 5 6 7 @misc { learn2learn2019 , author = {Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner} , title = {learn2learn} , month = sep , year = 2019 , url = {https://github.com/learnables/learn2learn} }","title":"Citation"},{"location":"#acknowledgements-friends","text":"The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License. TorchMeta is similar library, with a focus on datasets for supervised meta-learning. higher is a PyTorch library that enables differentiating through optimization inner-loops. While they monkey-patch nn.Module to be stateless, learn2learn retains the stateful PyTorch look-and-feel. For more information, refer to their ArXiv paper .","title":"Acknowledgements &amp; Friends"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6 v0.1.2 \u00b6 Added \u00b6 New example: Meta-World example with MAML-TRPO with it's own env wrapper. (@ Kostis-S-Z ) l2l.vision.benchmarks interface. Differentiable optimization utilities in l2l.optim . (including l2l.optim.LearnableOptimizer for meta-descent) General gradient-based meta-learning wrapper in l2l.algorithms.GBML . Various nn.Modules in l2l.nn . l2l.update_module as a more general alternative to l2l.algorithms.maml_update . Changed \u00b6 Fixed \u00b6 clone_module supports non-Module objects. VGG flowers now relies on tarfile.open() instead of tarfile.TarFile(). v0.1.1 \u00b6 Added \u00b6 New tutorial: 'Feature Reuse with ANIL'. (@ewinapun) Changed \u00b6 Mujoco imports optional for docs: the import error is postponed to first method call. Fixed \u00b6 MAML() and clone_module support for RNN modules. v0.1.0.1 \u00b6 Fixed \u00b6 Remove Cython dependency when installing from PyPI and clean up package distribution. v0.1.0 \u00b6 Added \u00b6 A CHANGELOG.md file. New vision datasets: FC100, tiered-Imagenet, FGVCAircraft, VGGFlowers102. New vision examples: Reptile & ANIL. Extensive benchmarks of all vision examples. Changed \u00b6 Re-wrote TaskDataset and task transforms in Cython, for a 20x speed-up. Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5). New Material doc theme with links to changelog and examples. Fixed \u00b6 Support for RandomClassRotation with newer versions of torchvision. Various minor fixes in the examples. Add Dropbox download if GDrive fails for FC100.","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"changelog/#v012","text":"","title":"v0.1.2"},{"location":"changelog/#added_1","text":"New example: Meta-World example with MAML-TRPO with it's own env wrapper. (@ Kostis-S-Z ) l2l.vision.benchmarks interface. Differentiable optimization utilities in l2l.optim . (including l2l.optim.LearnableOptimizer for meta-descent) General gradient-based meta-learning wrapper in l2l.algorithms.GBML . Various nn.Modules in l2l.nn . l2l.update_module as a more general alternative to l2l.algorithms.maml_update .","title":"Added"},{"location":"changelog/#changed_1","text":"","title":"Changed"},{"location":"changelog/#fixed_1","text":"clone_module supports non-Module objects. VGG flowers now relies on tarfile.open() instead of tarfile.TarFile().","title":"Fixed"},{"location":"changelog/#v011","text":"","title":"v0.1.1"},{"location":"changelog/#added_2","text":"New tutorial: 'Feature Reuse with ANIL'. (@ewinapun)","title":"Added"},{"location":"changelog/#changed_2","text":"Mujoco imports optional for docs: the import error is postponed to first method call.","title":"Changed"},{"location":"changelog/#fixed_2","text":"MAML() and clone_module support for RNN modules.","title":"Fixed"},{"location":"changelog/#v0101","text":"","title":"v0.1.0.1"},{"location":"changelog/#fixed_3","text":"Remove Cython dependency when installing from PyPI and clean up package distribution.","title":"Fixed"},{"location":"changelog/#v010","text":"","title":"v0.1.0"},{"location":"changelog/#added_3","text":"A CHANGELOG.md file. New vision datasets: FC100, tiered-Imagenet, FGVCAircraft, VGGFlowers102. New vision examples: Reptile & ANIL. Extensive benchmarks of all vision examples.","title":"Added"},{"location":"changelog/#changed_3","text":"Re-wrote TaskDataset and task transforms in Cython, for a 20x speed-up. Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5). New Material doc theme with links to changelog and examples.","title":"Changed"},{"location":"changelog/#fixed_4","text":"Support for RandomClassRotation with newer versions of torchvision. Various minor fixes in the examples. Add Dropbox download if GDrive fails for FC100.","title":"Fixed"},{"location":"examples.optim/","text":"Meta-Optimization \u00b6 This directory contains examples of using learn2learn for meta-optimization or meta-descent. Hypergradient \u00b6 The script hypergrad_mnist.py demonstrates how to implement a slightly modified version of \" Online Learning Rate Adaptation with Hypergradient Descent \". The implementation departs from the algorithm presented in the paper in two ways. We forgo the analytical formulation of the learning rate's gradient to demonstrate the capability of the LearnableOptimizer class. We adapt per-parameter learning rates instead of updating a single learning rate shared by all parameters. Usage Warning The parameters for this script were not carefully tuned. Manually edit the script and run: 1 python examples/optimization/hypergrad_mnist.py","title":"Optimization"},{"location":"examples.optim/#meta-optimization","text":"This directory contains examples of using learn2learn for meta-optimization or meta-descent.","title":"Meta-Optimization"},{"location":"examples.optim/#hypergradient","text":"The script hypergrad_mnist.py demonstrates how to implement a slightly modified version of \" Online Learning Rate Adaptation with Hypergradient Descent \". The implementation departs from the algorithm presented in the paper in two ways. We forgo the analytical formulation of the learning rate's gradient to demonstrate the capability of the LearnableOptimizer class. We adapt per-parameter learning rates instead of updating a single learning rate shared by all parameters. Usage Warning The parameters for this script were not carefully tuned. Manually edit the script and run: 1 python examples/optimization/hypergrad_mnist.py","title":"Hypergradient"},{"location":"examples.rl/","text":"Meta-Reinforcement Learning \u00b6 Warning Meta-RL results are particularly finicky to compare. Different papers use different environment implementations, which in turn produce different convergence and rewards. The plots below only serve to indicate what kind of performance you can expect with learn2learn. MAML \u00b6 The above results are obtained by running maml_trpo.py on HalfCheetahForwardBackwardEnv and AntForwardBackwardEnv for 300 updates. The figures show the expected sum of rewards over all tasks. The line and shadow are the mean and standard deviation computed over 3 random seeds. Info Those results were obtained in August 2019, and might be outdated.","title":"Reinforcement Learning"},{"location":"examples.rl/#meta-reinforcement-learning","text":"Warning Meta-RL results are particularly finicky to compare. Different papers use different environment implementations, which in turn produce different convergence and rewards. The plots below only serve to indicate what kind of performance you can expect with learn2learn.","title":"Meta-Reinforcement Learning"},{"location":"examples.rl/#maml","text":"The above results are obtained by running maml_trpo.py on HalfCheetahForwardBackwardEnv and AntForwardBackwardEnv for 300 updates. The figures show the expected sum of rewards over all tasks. The line and shadow are the mean and standard deviation computed over 3 random seeds. Info Those results were obtained in August 2019, and might be outdated.","title":"MAML"},{"location":"examples.vision/","text":"Meta-Learning & Computer Vision \u00b6 This directory contains meta-learning examples and reproductions for common computer vision benchmarks. MAML \u00b6 The following files reproduce MAML on the Omniglot and mini -ImageNet datasets. The FOMAML results can be obtained by setting first_order=True in the MAML wrapper. On Omniglot, the CNN results can be obtained by swapping OmniglotFC with OmniglotCNN . maml_omniglot.py - MAML on the Omniglot dataset with a fully-connected network. maml_miniimagenet.py - MAML on the mini -ImageNet dataset with the standard convolutional network. Note that the original MAML paper trains with 5 fast adaptation step, but tests with 10 steps. This implementation only provides the training code. Results When adapting the code to different datasets, we obtained the following results. Only the fast-adaptation learning rate needs a bit of tuning, and good values usually lie in a 0.5-2x range of the original value. Dataset Architecture Ways Shots Original learn2learn Omniglot FC 5 1 89.7% 88.9% Omniglot CNN 5 1 98.7% 99.1% mini-ImageNet CNN 5 1 48.7% 48.3% mini-ImageNet CNN 5 5 63.1% 65.4% CIFAR-FS CNN 5 5 71.5% 73.6% FC100 CNN 5 5 n/a 49.0% Usage Manually edit the respective files and run: 1 python examples/vision/maml_omniglot.py or 1 python examples/vision/maml_miniimagenet.py Prototypical Networks \u00b6 The file protonet_miniimagenet.py reproduces Prototypical Networks on the mini -ImageNet dataset. This implementation provides training and testing code. Results Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 1 49.4% 49.1% mini-ImageNet CNN 5 5 68.2% 66.5% Usage For 1 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py For 5 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py --shot 5 --train-way 20 ANIL \u00b6 The file anil_fc100.py implements ANIL on the FC100 dataset. Results While ANIL only used mini -ImageNet as a benchmark, we provide results for CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% Usage Manually edit the above file and run: 1 python examples/vision/anil_fc100.py Reptile \u00b6 The file reptile_miniimagenet.py reproduces Reptile on the mini -ImageNet dataset. Results The mini -ImageNet file can easily be adapted to obtain results on Omniglot and CIFAR-FS as well. Dataset Architecture Ways Shots Original learn2learn Omniglot CNN 5 5 99.5% 99.5% mini-ImageNet CNN 5 5 66.0% 65.5% CIFAR-FS CNN 10 3 n/a 46.3% Usage Manually edit the above file and run: 1 python examples/vision/reptile_miniimagenet.py","title":"Computer Vision"},{"location":"examples.vision/#meta-learning-computer-vision","text":"This directory contains meta-learning examples and reproductions for common computer vision benchmarks.","title":"Meta-Learning &amp; Computer Vision"},{"location":"examples.vision/#maml","text":"The following files reproduce MAML on the Omniglot and mini -ImageNet datasets. The FOMAML results can be obtained by setting first_order=True in the MAML wrapper. On Omniglot, the CNN results can be obtained by swapping OmniglotFC with OmniglotCNN . maml_omniglot.py - MAML on the Omniglot dataset with a fully-connected network. maml_miniimagenet.py - MAML on the mini -ImageNet dataset with the standard convolutional network. Note that the original MAML paper trains with 5 fast adaptation step, but tests with 10 steps. This implementation only provides the training code. Results When adapting the code to different datasets, we obtained the following results. Only the fast-adaptation learning rate needs a bit of tuning, and good values usually lie in a 0.5-2x range of the original value. Dataset Architecture Ways Shots Original learn2learn Omniglot FC 5 1 89.7% 88.9% Omniglot CNN 5 1 98.7% 99.1% mini-ImageNet CNN 5 1 48.7% 48.3% mini-ImageNet CNN 5 5 63.1% 65.4% CIFAR-FS CNN 5 5 71.5% 73.6% FC100 CNN 5 5 n/a 49.0% Usage Manually edit the respective files and run: 1 python examples/vision/maml_omniglot.py or 1 python examples/vision/maml_miniimagenet.py","title":"MAML"},{"location":"examples.vision/#prototypical-networks","text":"The file protonet_miniimagenet.py reproduces Prototypical Networks on the mini -ImageNet dataset. This implementation provides training and testing code. Results Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 1 49.4% 49.1% mini-ImageNet CNN 5 5 68.2% 66.5% Usage For 1 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py For 5 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py --shot 5 --train-way 20","title":"Prototypical Networks"},{"location":"examples.vision/#anil","text":"The file anil_fc100.py implements ANIL on the FC100 dataset. Results While ANIL only used mini -ImageNet as a benchmark, we provide results for CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% Usage Manually edit the above file and run: 1 python examples/vision/anil_fc100.py","title":"ANIL"},{"location":"examples.vision/#reptile","text":"The file reptile_miniimagenet.py reproduces Reptile on the mini -ImageNet dataset. Results The mini -ImageNet file can easily be adapted to obtain results on Omniglot and CIFAR-FS as well. Dataset Architecture Ways Shots Original learn2learn Omniglot CNN 5 5 99.5% 99.5% mini-ImageNet CNN 5 5 66.0% 65.5% CIFAR-FS CNN 10 3 n/a 46.3% Usage Manually edit the above file and run: 1 python examples/vision/reptile_miniimagenet.py","title":"Reptile"},{"location":"docs/learn2learn.algorithms/","text":"learn2learn.algorithms \u00b6 A set of high-level algorithm implementations, with easy-to-use API. MAML \u00b6 1 MAML ( model , lr , first_order = False , allow_unused = None , allow_nograd = False ) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt() methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the first-order approximation of MAML. (FOMAML) allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" Example 1 2 3 4 5 6 linear = l2l . algorithms . MAML ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward () adapt \u00b6 1 MAML . adapt ( loss , first_order = None , allow_unused = None , allow_nograd = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. clone \u00b6 1 MAML . clone ( first_order = None , allow_unused = None , allow_nograd = None ) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. MetaSGD \u00b6 1 MetaSGD ( model , lr = 1.0 , first_order = False , lrs = None ) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example 1 2 3 4 5 6 linear = l2l . algorithms . MetaSGD ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward () clone \u00b6 1 MetaSGD . clone () Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates. adapt \u00b6 1 MetaSGD . adapt ( loss , first_order = None ) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates. GBML \u00b6 1 2 3 4 5 6 7 8 GBML ( module , transform , lr = 1.0 , adapt_transform = False , first_order = False , allow_unused = False , allow_nograd = False , ** kwargs ) [Source] Description General wrapper for gradient-based meta-learning implementations. A variety of algorithms can simply be implemented by changing the kind of transform used during fast-adaptation. For example, if the transform is Scale we recover Meta-SGD [2] with adapt_transform=False and Alpha MAML [4] with adapt_transform=True . If the transform is a Kronecker-factored module (e.g. neural network, or linear), we recover KFO from [5]. Arguments module (Module) - Module to be wrapped. tranform (Module) - Transform used to update the module. lr (float) - Fast adaptation learning rate. adapt_transform (bool, optional , default=False) - Whether to update the transform's parameters during fast-adaptation. first_order (bool, optional , default=False) - Whether to use the first-order approximation. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d Park & Oliva. 2019. \u201cMeta-Curvature.\u201d Behl et al. 2019. \u201cAlpha MAML: Adaptive Model-Agnostic Meta-Learning.\u201d Arnold et al. 2019. \u201cWhen MAML Can Adapt Fast and How to Assist When It Cannot.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 model = SmallCNN () transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) gbml = l2l . algorithms . GBML ( module = model , transform = transform , lr = 0.01 , adapt_transform = True , ) gbml . to ( device ) opt = torch . optim . SGD ( gbml . parameters (), lr = 0.001 ) __Training with 1 adaptation step__ for iteration in range ( 10 ): opt . zero_grad () task_model = gbml . clone () loss = compute_loss ( task_model ) task_model . adapt ( loss ) loss . backward () opt . step () clone \u00b6 1 2 3 4 GBML . clone ( first_order = None , allow_unused = None , allow_nograd = None , adapt_transform = None ) Description Similar to MAML.clone() . Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. adapt \u00b6 1 GBML . adapt ( loss , first_order = None , allow_nograd = None , allow_unused = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. The parameters of the transform are only adapted if self.adapt_update is True . Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"learn2learn.algorithms"},{"location":"docs/learn2learn.algorithms/#learn2learnalgorithms","text":"A set of high-level algorithm implementations, with easy-to-use API.","title":"learn2learn.algorithms"},{"location":"docs/learn2learn.algorithms/#maml","text":"1 MAML ( model , lr , first_order = False , allow_unused = None , allow_nograd = False ) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt() methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the first-order approximation of MAML. (FOMAML) allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" Example 1 2 3 4 5 6 linear = l2l . algorithms . MAML ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward ()","title":"MAML"},{"location":"docs/learn2learn.algorithms/#adapt","text":"1 MAML . adapt ( loss , first_order = None , allow_unused = None , allow_nograd = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#clone","text":"1 MAML . clone ( first_order = None , allow_unused = None , allow_nograd = None ) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"clone"},{"location":"docs/learn2learn.algorithms/#metasgd","text":"1 MetaSGD ( model , lr = 1.0 , first_order = False , lrs = None ) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example 1 2 3 4 5 6 linear = l2l . algorithms . MetaSGD ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward ()","title":"MetaSGD"},{"location":"docs/learn2learn.algorithms/#clone_1","text":"1 MetaSGD . clone () Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates.","title":"clone"},{"location":"docs/learn2learn.algorithms/#adapt_1","text":"1 MetaSGD . adapt ( loss , first_order = None ) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#gbml","text":"1 2 3 4 5 6 7 8 GBML ( module , transform , lr = 1.0 , adapt_transform = False , first_order = False , allow_unused = False , allow_nograd = False , ** kwargs ) [Source] Description General wrapper for gradient-based meta-learning implementations. A variety of algorithms can simply be implemented by changing the kind of transform used during fast-adaptation. For example, if the transform is Scale we recover Meta-SGD [2] with adapt_transform=False and Alpha MAML [4] with adapt_transform=True . If the transform is a Kronecker-factored module (e.g. neural network, or linear), we recover KFO from [5]. Arguments module (Module) - Module to be wrapped. tranform (Module) - Transform used to update the module. lr (float) - Fast adaptation learning rate. adapt_transform (bool, optional , default=False) - Whether to update the transform's parameters during fast-adaptation. first_order (bool, optional , default=False) - Whether to use the first-order approximation. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d Park & Oliva. 2019. \u201cMeta-Curvature.\u201d Behl et al. 2019. \u201cAlpha MAML: Adaptive Model-Agnostic Meta-Learning.\u201d Arnold et al. 2019. \u201cWhen MAML Can Adapt Fast and How to Assist When It Cannot.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 model = SmallCNN () transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) gbml = l2l . algorithms . GBML ( module = model , transform = transform , lr = 0.01 , adapt_transform = True , ) gbml . to ( device ) opt = torch . optim . SGD ( gbml . parameters (), lr = 0.001 ) __Training with 1 adaptation step__ for iteration in range ( 10 ): opt . zero_grad () task_model = gbml . clone () loss = compute_loss ( task_model ) task_model . adapt ( loss ) loss . backward () opt . step ()","title":"GBML"},{"location":"docs/learn2learn.algorithms/#clone_2","text":"1 2 3 4 GBML . clone ( first_order = None , allow_unused = None , allow_nograd = None , adapt_transform = None ) Description Similar to MAML.clone() . Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"clone"},{"location":"docs/learn2learn.algorithms/#adapt_2","text":"1 GBML . adapt ( loss , first_order = None , allow_nograd = None , allow_unused = None ) Description Takes a gradient step on the loss and updates the cloned parameters in place. The parameters of the transform are only adapted if self.adapt_update is True . Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"adapt"},{"location":"docs/learn2learn.data/","text":"learn2learn.data \u00b6 A set of utilities for data & tasks loading, preprocessing, and sampling. MetaDataset \u00b6 1 MetaDataset ( dataset ) Description It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. labels_to_indices (Dict) - A dictionary mapping label to their indices. If not specified then we loop through all the datapoints to understand the mapping. (default: None) Example 1 2 mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist ) TaskDataset \u00b6 1 TaskDataset ( dataset , task_transforms = None , num_tasks =- 1 , task_collate = None ) [Source] Description Creates a set of tasks from a given Dataset. In addition to the Dataset, TaskDataset accepts a list of task transformations ( task_transforms ) which define the kind of tasks sampled from the dataset. The tasks are lazily sampled upon indexing (or calling the .sample() method), and their descriptions cached for later use. If num_tasks is -1, the TaskDataset will not cache task descriptions and instead continuously resample new ones. In this case, the length of the TaskDataset is set to 1. For more information on tasks and task descriptions, please refer to the documentation of task transforms. Arguments dataset (Dataset) - Dataset of data to compute tasks. task_transforms (list, optional , default=None) - List of task transformations. num_tasks (int, optional , default=-1) - Number of tasks to generate. Example 1 2 3 4 5 6 7 8 9 dataset = l2l . data . MetaDataset ( MyDataset ()) transforms = [ l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task learn2learn.data.transforms \u00b6 Description Collection of general task transformations. A task transformation is an object that implements the callable interface. (Either a function or an object that implements the __call__ special method.) Each transformation is called on a task description, which consists of a list of DataDescription with attributes index and transforms , where index corresponds to the index of single data sample inthe dataset, and transforms is a list of transformations that will be applied to the sample. Each transformation must return a new task description. At first, the task description contains all samples from the dataset. A task transform takes this task description list and modifies it such that a particular task is created. For example, the NWays task transform filters data samples from the task description such that remaining ones belong to a random subset of all classes available. (The size of the subset is controlled via the class's n argument.) On the other hand, the LoadData task transform simply appends a call to load the actual data from the dataset to the list of transformations of each sample. To create a task from a task description, the TaskDataset applies each sample's list of transform s in order. Then, all samples are collated via the TaskDataset 's collate function. LoadData \u00b6 1 LoadData ( dataset ) [Source] Description Loads a sample from the dataset given its index. Arguments dataset (Dataset) - The dataset from which to load the sample. NWays \u00b6 1 NWays ( dataset , n = 2 ) [Source] Description Keeps samples from N random labels present in the task description. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. KShots \u00b6 1 KShots ( dataset , k = 1 , replacement = False ) [Source] Description Keeps K samples for each present labels. Arguments dataset (Dataset) - The dataset from which to load the sample. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample with replacement. FilterLabels \u00b6 1 FilterLabels ( dataset , labels ) [Source] Description Removes samples that do not belong to the given set of labels. Arguments dataset (Dataset) - The dataset from which to load the sample. labels (list) - The list of labels to include. FusedNWaysKShots \u00b6 1 FusedNWaysKShots ( dataset , n = 2 , k = 1 , replacement = False , filter_labels = None ) [Source] Description Efficient implementation of FilterLabels, NWays, and KShots. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample shots with replacement. filter_labels (list, optional , default=None) - The list of labels to include. Defaults to all labels in the dataset. RemapLabels \u00b6 1 RemapLabels ( dataset , shuffle = True ) [Source] Description Given samples from K classes, maps the labels to 0, ..., K. Arguments dataset (Dataset) - The dataset from which to load the sample. ConsecutiveLabels \u00b6 1 ConsecutiveLabels ( dataset ) [Source] Description Re-orders the samples in the task description such that they are sorted in consecutive order. Note: when used before RemapLabels , the labels will be homogeneously clustered, but in no specific order. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"learn2learn.data"},{"location":"docs/learn2learn.data/#learn2learndata","text":"A set of utilities for data & tasks loading, preprocessing, and sampling.","title":"learn2learn.data"},{"location":"docs/learn2learn.data/#metadataset","text":"1 MetaDataset ( dataset ) Description It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. labels_to_indices (Dict) - A dictionary mapping label to their indices. If not specified then we loop through all the datapoints to understand the mapping. (default: None) Example 1 2 mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist )","title":"MetaDataset"},{"location":"docs/learn2learn.data/#taskdataset","text":"1 TaskDataset ( dataset , task_transforms = None , num_tasks =- 1 , task_collate = None ) [Source] Description Creates a set of tasks from a given Dataset. In addition to the Dataset, TaskDataset accepts a list of task transformations ( task_transforms ) which define the kind of tasks sampled from the dataset. The tasks are lazily sampled upon indexing (or calling the .sample() method), and their descriptions cached for later use. If num_tasks is -1, the TaskDataset will not cache task descriptions and instead continuously resample new ones. In this case, the length of the TaskDataset is set to 1. For more information on tasks and task descriptions, please refer to the documentation of task transforms. Arguments dataset (Dataset) - Dataset of data to compute tasks. task_transforms (list, optional , default=None) - List of task transformations. num_tasks (int, optional , default=-1) - Number of tasks to generate. Example 1 2 3 4 5 6 7 8 9 dataset = l2l . data . MetaDataset ( MyDataset ()) transforms = [ l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task","title":"TaskDataset"},{"location":"docs/learn2learn.data/#learn2learndatatransforms","text":"Description Collection of general task transformations. A task transformation is an object that implements the callable interface. (Either a function or an object that implements the __call__ special method.) Each transformation is called on a task description, which consists of a list of DataDescription with attributes index and transforms , where index corresponds to the index of single data sample inthe dataset, and transforms is a list of transformations that will be applied to the sample. Each transformation must return a new task description. At first, the task description contains all samples from the dataset. A task transform takes this task description list and modifies it such that a particular task is created. For example, the NWays task transform filters data samples from the task description such that remaining ones belong to a random subset of all classes available. (The size of the subset is controlled via the class's n argument.) On the other hand, the LoadData task transform simply appends a call to load the actual data from the dataset to the list of transformations of each sample. To create a task from a task description, the TaskDataset applies each sample's list of transform s in order. Then, all samples are collated via the TaskDataset 's collate function.","title":"learn2learn.data.transforms"},{"location":"docs/learn2learn.data/#loaddata","text":"1 LoadData ( dataset ) [Source] Description Loads a sample from the dataset given its index. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"LoadData"},{"location":"docs/learn2learn.data/#nways","text":"1 NWays ( dataset , n = 2 ) [Source] Description Keeps samples from N random labels present in the task description. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels.","title":"NWays"},{"location":"docs/learn2learn.data/#kshots","text":"1 KShots ( dataset , k = 1 , replacement = False ) [Source] Description Keeps K samples for each present labels. Arguments dataset (Dataset) - The dataset from which to load the sample. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample with replacement.","title":"KShots"},{"location":"docs/learn2learn.data/#filterlabels","text":"1 FilterLabels ( dataset , labels ) [Source] Description Removes samples that do not belong to the given set of labels. Arguments dataset (Dataset) - The dataset from which to load the sample. labels (list) - The list of labels to include.","title":"FilterLabels"},{"location":"docs/learn2learn.data/#fusednwayskshots","text":"1 FusedNWaysKShots ( dataset , n = 2 , k = 1 , replacement = False , filter_labels = None ) [Source] Description Efficient implementation of FilterLabels, NWays, and KShots. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample shots with replacement. filter_labels (list, optional , default=None) - The list of labels to include. Defaults to all labels in the dataset.","title":"FusedNWaysKShots"},{"location":"docs/learn2learn.data/#remaplabels","text":"1 RemapLabels ( dataset , shuffle = True ) [Source] Description Given samples from K classes, maps the labels to 0, ..., K. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"RemapLabels"},{"location":"docs/learn2learn.data/#consecutivelabels","text":"1 ConsecutiveLabels ( dataset ) [Source] Description Re-orders the samples in the task description such that they are sorted in consecutive order. Note: when used before RemapLabels , the labels will be homogeneously clustered, but in no specific order. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"ConsecutiveLabels"},{"location":"docs/learn2learn.gym/","text":"learn2learn.gym \u00b6 Environment, models, and other utilities related to reinforcement learning and OpenAI Gym. MetaEnv \u00b6 1 MetaEnv ( task = None ) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then 1 2 3 4 5 6 env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations. AsyncVectorEnv \u00b6 1 AsyncVectorEnv ( env_fns , env = None ) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations. learn2learn.gym.envs.mujoco \u00b6 HalfCheetahForwardBackwardEnv \u00b6 1 HalfCheetahForwardBackwardEnv ( task = None ) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntForwardBackwardEnv \u00b6 1 AntForwardBackwardEnv ( task = None ) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntDirectionEnv \u00b6 1 AntDirectionEnv ( task = None ) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidForwardBackwardEnv \u00b6 1 HumanoidForwardBackwardEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidDirectionEnv \u00b6 1 HumanoidDirectionEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. learn2learn.gym.envs.particles \u00b6 Particles2DEnv \u00b6 1 Particles2DEnv ( task = None ) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation. learn2learn.gym.envs.metaworld \u00b6 MetaWorldML1 \u00b6 1 MetaWorldML1 ( task_name , env_type = 'train' , n_goals = 50 , sample_all = False ) [Source] Description The ML1 Benchmark of Meta-World is focused on solving just one task on different object / goal configurations.This task can be either one of the following: 'reach', 'push' and 'pick-and-place'. The meta-training is performed on a set of 50 randomly chosen once initial object and goal positions. The meta-testing is performed on a held-out set of 10 new different configurations. The starting state of the robot arm is always fixed. The goal positions are not provided in the observation space, forcing the Sawyer robot arm to explore and adapt to the new goal through trial-and-error. This is considered a relatively easy problem for a meta-learning algorithm to solve and acts as a sanity check to a working implementation. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019). MetaWorldML10 \u00b6 1 MetaWorldML10 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description The ML10 Benchmark of Meta-World consists of 10 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. The meta-training tasks have been intentionally selected to have a structural similarity to the test tasks. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This is a much harder problem than ML1 which probably requires more samples to train. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019). MetaWorldML45 \u00b6 1 MetaWorldML45 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description Similarly to ML10, this Benchmark has a variety of 45 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This benchmark is significantly difficult to solve due to the diversity across tasks. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#learn2learngym","text":"Environment, models, and other utilities related to reinforcement learning and OpenAI Gym.","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#metaenv","text":"1 MetaEnv ( task = None ) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then 1 2 3 4 5 6 env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations.","title":"MetaEnv"},{"location":"docs/learn2learn.gym/#asyncvectorenv","text":"1 AsyncVectorEnv ( env_fns , env = None ) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations.","title":"AsyncVectorEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsmujoco","text":"","title":"learn2learn.gym.envs.mujoco"},{"location":"docs/learn2learn.gym/#halfcheetahforwardbackwardenv","text":"1 HalfCheetahForwardBackwardEnv ( task = None ) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HalfCheetahForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antforwardbackwardenv","text":"1 AntForwardBackwardEnv ( task = None ) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antdirectionenv","text":"1 AntDirectionEnv ( task = None ) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntDirectionEnv"},{"location":"docs/learn2learn.gym/#humanoidforwardbackwardenv","text":"1 HumanoidForwardBackwardEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#humanoiddirectionenv","text":"1 HumanoidDirectionEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidDirectionEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsparticles","text":"","title":"learn2learn.gym.envs.particles"},{"location":"docs/learn2learn.gym/#particles2denv","text":"1 Particles2DEnv ( task = None ) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation.","title":"Particles2DEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsmetaworld","text":"","title":"learn2learn.gym.envs.metaworld"},{"location":"docs/learn2learn.gym/#metaworldml1","text":"1 MetaWorldML1 ( task_name , env_type = 'train' , n_goals = 50 , sample_all = False ) [Source] Description The ML1 Benchmark of Meta-World is focused on solving just one task on different object / goal configurations.This task can be either one of the following: 'reach', 'push' and 'pick-and-place'. The meta-training is performed on a set of 50 randomly chosen once initial object and goal positions. The meta-testing is performed on a held-out set of 10 new different configurations. The starting state of the robot arm is always fixed. The goal positions are not provided in the observation space, forcing the Sawyer robot arm to explore and adapt to the new goal through trial-and-error. This is considered a relatively easy problem for a meta-learning algorithm to solve and acts as a sanity check to a working implementation. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"MetaWorldML1"},{"location":"docs/learn2learn.gym/#metaworldml10","text":"1 MetaWorldML10 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description The ML10 Benchmark of Meta-World consists of 10 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. The meta-training tasks have been intentionally selected to have a structural similarity to the test tasks. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This is a much harder problem than ML1 which probably requires more samples to train. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"MetaWorldML10"},{"location":"docs/learn2learn.gym/#metaworldml45","text":"1 MetaWorldML45 ( env_type = 'train' , sample_all = False , task_name = None ) [Source] Description Similarly to ML10, this Benchmark has a variety of 45 different tasks for meta-training and 5 new tasks for meta-testing. For each task there is only one goal that is randomly chosen once. The starting state and object position is random. No task ID is provided in the observation space, meaning the meta-learning algorithm will need to identify each task from experience. This benchmark is significantly difficult to solve due to the diversity across tasks. For more information regarding this benchmark, please consult [1]. Credit Original implementation found in https://github.com/rlworkgroup/metaworld. References Yu, Tianhe, et al. \"Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\" arXiv preprint arXiv:1910.10897 (2019).","title":"MetaWorldML45"},{"location":"docs/learn2learn/","text":"learn2learn \u00b6 clone_module \u00b6 1 clone_module ( module ) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example 1 2 3 4 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate all the way to net. detach_module \u00b6 1 detach_module ( module ) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example 1 2 3 4 5 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) detach_module ( clone ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate on clone, not net. update_module \u00b6 1 update_module ( module , updates = None ) [Source] Description Updates the parameters of a module in-place, in a way that preserves differentiability. The parameters of the module are swapped with their update values, according to: p \\gets p + u, where p is the parameter, and u is its corresponding update. Arguments module (Module) - The module to update. updates (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the tensors in .update attributes. Example 1 2 3 4 5 6 7 8 error = loss ( model ( X ), y ) grads = torch . autograd . grad ( error , model . parameters (), create_graph = True , ) updates = [ - lr * g for g in grads ] l2l . update_module ( model , updates = updates ) magic_box \u00b6 1 magic_box ( x ) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example 1 2 loss = ( magic_box ( cum_log_probs ) * advantages ) . mean () # loss is the mean advantage loss . backward ()","title":"learn2learn"},{"location":"docs/learn2learn/#learn2learn","text":"","title":"learn2learn"},{"location":"docs/learn2learn/#clone_module","text":"1 clone_module ( module ) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example 1 2 3 4 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate all the way to net.","title":"clone_module"},{"location":"docs/learn2learn/#detach_module","text":"1 detach_module ( module ) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example 1 2 3 4 5 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) detach_module ( clone ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate on clone, not net.","title":"detach_module"},{"location":"docs/learn2learn/#update_module","text":"1 update_module ( module , updates = None ) [Source] Description Updates the parameters of a module in-place, in a way that preserves differentiability. The parameters of the module are swapped with their update values, according to: p \\gets p + u, where p is the parameter, and u is its corresponding update. Arguments module (Module) - The module to update. updates (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the tensors in .update attributes. Example 1 2 3 4 5 6 7 8 error = loss ( model ( X ), y ) grads = torch . autograd . grad ( error , model . parameters (), create_graph = True , ) updates = [ - lr * g for g in grads ] l2l . update_module ( model , updates = updates )","title":"update_module"},{"location":"docs/learn2learn/#magic_box","text":"1 magic_box ( x ) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example 1 2 loss = ( magic_box ( cum_log_probs ) * advantages ) . mean () # loss is the mean advantage loss . backward ()","title":"magic_box"},{"location":"docs/learn2learn.nn/","text":"learn2learn.nn \u00b6 Additional torch.nn.Module s frequently used for meta-learning. Lambda \u00b6 1 Lambda ( lmb ) [Source] Description Utility class to create a wrapper based on a lambda function. Arguments lmb (callable) - The function to call in the forward pass. Example 1 2 3 4 mean23 = Lambda ( lambda x : x . mean ( dim = [ 2 , 3 ])) # mean23 is a Module x = features ( img ) x = mean23 ( x ) x = x . flatten () Flatten \u00b6 1 Flatten () [Source] Description Utility Module to flatten inputs to (batch_size, -1) shape. Example 1 2 3 4 flatten = Flatten () x = torch . randn ( 5 , 3 , 32 , 32 ) x = flatten ( x ) print ( x . shape ) # (5, 3072) Scale \u00b6 1 Scale ( shape , alpha = 1.0 ) [Source] Description A per-parameter scaling factor with learnable parameter. Arguments shape (int or tuple) - The shape of the scaling matrix. alpha (float, optional , default=1.0) - Initial value for the scaling factor. Example 1 2 3 x = torch . ones ( 3 ) scale = Scale ( x . shape , alpha = 0.5 ) print ( scale ( x )) # [.5, .5, .5] KroneckerLinear \u00b6 1 KroneckerLinear ( n , m , bias = True , psd = False , device = None ) [Source] Description A linear transformation whose parameters are expressed as a Kronecker product. This Module maps an input vector x \\in \\mathbb{R}^{nm} to y = Ax + b such that: A = R^\\top \\otimes L, where L \\in \\mathbb{R}^{n \\times n} and R \\in \\mathbb{R}^{m \\times m} are the learnable Kronecker factors. This implementation can reduce the memory requirement for large linear mapping from \\mathcal{O}(n^2 \\cdot m^2) to \\mathcal{O}(n^2 + m^2) , but forces y \\in \\mathbb{R}^{nm} . The matrix A is initialized as the identity, and the bias as a zero vector. Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. psd (bool, optional , default=False) - Forces the matrix A to be positive semi-definite if True. device (device, optional , default=None) - The device on which to instantiate the Module. References Jose et al. 2018. \"Kronecker recurrent units\". Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 m , n = 2 , 3 x = torch . randn ( 6 ) kronecker = KroneckerLinear ( n , m ) y = kronecker ( x ) y . shape # (6, ) KroneckerRNN \u00b6 1 KroneckerRNN ( n , m , bias = True , sigma = None ) [Source] Description Implements a recurrent neural network whose matrices are parameterized via their Kronecker factors. (See KroneckerLinear for details.) Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerRNN ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, ) KroneckerLSTM \u00b6 1 KroneckerLSTM ( n , m , bias = True , sigma = None ) [Source] Description Implements an LSTM using a factorization similar to the one of KroneckerLinear . Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerLSTM ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, )","title":"learn2learn.nn"},{"location":"docs/learn2learn.nn/#learn2learnnn","text":"Additional torch.nn.Module s frequently used for meta-learning.","title":"learn2learn.nn"},{"location":"docs/learn2learn.nn/#lambda","text":"1 Lambda ( lmb ) [Source] Description Utility class to create a wrapper based on a lambda function. Arguments lmb (callable) - The function to call in the forward pass. Example 1 2 3 4 mean23 = Lambda ( lambda x : x . mean ( dim = [ 2 , 3 ])) # mean23 is a Module x = features ( img ) x = mean23 ( x ) x = x . flatten ()","title":"Lambda"},{"location":"docs/learn2learn.nn/#flatten","text":"1 Flatten () [Source] Description Utility Module to flatten inputs to (batch_size, -1) shape. Example 1 2 3 4 flatten = Flatten () x = torch . randn ( 5 , 3 , 32 , 32 ) x = flatten ( x ) print ( x . shape ) # (5, 3072)","title":"Flatten"},{"location":"docs/learn2learn.nn/#scale","text":"1 Scale ( shape , alpha = 1.0 ) [Source] Description A per-parameter scaling factor with learnable parameter. Arguments shape (int or tuple) - The shape of the scaling matrix. alpha (float, optional , default=1.0) - Initial value for the scaling factor. Example 1 2 3 x = torch . ones ( 3 ) scale = Scale ( x . shape , alpha = 0.5 ) print ( scale ( x )) # [.5, .5, .5]","title":"Scale"},{"location":"docs/learn2learn.nn/#kroneckerlinear","text":"1 KroneckerLinear ( n , m , bias = True , psd = False , device = None ) [Source] Description A linear transformation whose parameters are expressed as a Kronecker product. This Module maps an input vector x \\in \\mathbb{R}^{nm} to y = Ax + b such that: A = R^\\top \\otimes L, where L \\in \\mathbb{R}^{n \\times n} and R \\in \\mathbb{R}^{m \\times m} are the learnable Kronecker factors. This implementation can reduce the memory requirement for large linear mapping from \\mathcal{O}(n^2 \\cdot m^2) to \\mathcal{O}(n^2 + m^2) , but forces y \\in \\mathbb{R}^{nm} . The matrix A is initialized as the identity, and the bias as a zero vector. Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. psd (bool, optional , default=False) - Forces the matrix A to be positive semi-definite if True. device (device, optional , default=None) - The device on which to instantiate the Module. References Jose et al. 2018. \"Kronecker recurrent units\". Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 m , n = 2 , 3 x = torch . randn ( 6 ) kronecker = KroneckerLinear ( n , m ) y = kronecker ( x ) y . shape # (6, )","title":"KroneckerLinear"},{"location":"docs/learn2learn.nn/#kroneckerrnn","text":"1 KroneckerRNN ( n , m , bias = True , sigma = None ) [Source] Description Implements a recurrent neural network whose matrices are parameterized via their Kronecker factors. (See KroneckerLinear for details.) Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerRNN ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, )","title":"KroneckerRNN"},{"location":"docs/learn2learn.nn/#kroneckerlstm","text":"1 KroneckerLSTM ( n , m , bias = True , sigma = None ) [Source] Description Implements an LSTM using a factorization similar to the one of KroneckerLinear . Arguments n (int) - Dimensionality of the left Kronecker factor. m (int) - Dimensionality of the right Kronecker factor. bias (bool, optional , default=True) - Whether to include the bias term. sigma (callable, optional , default=None) - The activation function. References Jose et al. 2018. \"Kronecker recurrent units\". Example 1 2 3 4 5 6 m , n = 2 , 3 x = torch . randn ( 6 ) h = torch . randn ( 6 ) kronecker = KroneckerLSTM ( n , m ) y , new_h = kronecker ( x , h ) y . shape # (6, )","title":"KroneckerLSTM"},{"location":"docs/learn2learn.optim/","text":"learn2learn.optim \u00b6 A set of utilities to write differentiable optimization algorithms. LearnableOptimizer \u00b6 1 LearnableOptimizer ( model , transform , lr = 1.0 ) [Source] Description A PyTorch Optimizer with learnable transform, enabling the implementation of meta-descent / hyper-gradient algorithms. This optimizer takes a Module and a gradient transform. At each step, the gradient of the module is passed through the transforms, and the module differentiably update -- i.e. when the next backward is called, gradients of both the module and the transform are computed. In turn, the transform can be updated via your favorite optmizer. Arguments model (Module) - Module to be updated. transform (Module) - Transform used to compute updates of the model. lr (float) - Learning rate. References Sutton. 1992. \u201cGain Adaptation Beats Least Squares.\u201d Schraudolph. 1999. \u201cLocal Gain Adaptation in Stochastic Gradient Descent.\u201d Baydin et al. 2017. \u201cOnline Learning Rate Adaptation with Hypergradient Descent.\u201d Majumder et al. 2019. \u201cLearning the Learning Rate for Gradient Descent by Gradient Descent.\u201d Jacobsen et al. 2019. \u201cMeta-Descent for Online, Continual Prediction.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear zero_grad \u00b6 1 LearnableOptimizer . zero_grad () Only reset target parameters. ParameterUpdate \u00b6 1 ParameterUpdate ( parameters , transform ) [Source] Description Convenience class to implement custom update functions. Objects instantiated from this class behave similarly to torch.autograd.grad , but return parameter updates as opposed to gradients. Concretely, the gradients are first computed, then fed to their respective transform whose output is finally returned to the user. Additionally, this class supports parameters that might not require updates by setting the allow_nograd flag to True. In this case, the returned update is None . Arguments parameters (list) - Parameters of the model to update. transform (callable) - A callable that returns an instantiated transform given a parameter. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 model = torch . nn . Linear () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) get_update = ParameterUpdate ( model , transform ) opt = torch . optim . SGD ( model . parameters () + get_update . parameters ()) for iteration in range ( 10 ): opt . zero_grad () error = loss ( model ( X ), y ) updates = get_update ( error , model . parameters (), create_graph = True , ) l2l . update_module ( model , updates ) opt . step () forward \u00b6 1 2 3 4 5 6 ParameterUpdate . forward ( loss , parameters , create_graph = False , retain_graph = False , allow_unused = False , allow_nograd = False ) Description Similar to torch.autograd.grad, but passes the gradients through the provided transform. Arguments loss (Tensor) - The loss to differentiate. parameters (iterable) - Parameters w.r.t. which we want to compute the update. create_graph (bool, optional , default=False) - Same as torch.autograd.grad . retain_graph (bool, optional , default=False) - Same as torch.autograd.grad . allow_unused (bool, optional , default=False) - Same as torch.autograd.grad . allow_nograd (bool, optional , default=False) - Properly handles parameters that do not require gradients. (Their update will be None .) DifferentiableSGD \u00b6 1 DifferentiableSGD ( lr ) [Source] Description A callable object that applies a list of updates to the parameters of a torch.nn.Module in a differentiable manner. For each parameter p and corresponding gradient g , calling an instance of this class results in updating parameters: p \\gets p - \\alpha g, where \\alpha is the learning rate. Note: The module is updated in-place. Arguments lr (float) - The learning rate used to update the model. Example 1 2 3 4 5 6 sgd = DifferentiableSGD ( 0.1 ) gradients = torch . autograd . grad ( loss , model . parameters (), create_gaph = True ) sgd ( model , gradients ) # model is updated in-place forward \u00b6 1 DifferentiableSGD . forward ( module , gradients = None ) Arguments module (Module) - The module to update. gradients (list, optional , default=None) - A list of gradients for each parameter of the module. If None, will use the gradients in .grad attributes. learn2learn.optim.transforms \u00b6 Optimization transforms are special modules that take gradients as inputs and output model updates. Transforms are usually parameterized, and those parameters can be learned by gradient descent, allow you to learn optimization functions from data. ModuleTransform \u00b6 1 ModuleTransform ( module_cls ) [Source] Description The ModuleTransform creates a an optimization transform based on any nn.Module. ModuleTransform automatically instanciates a module from its class, based on a given parameter. The input and output shapes are of the module are set to (1, param.numel()) . When optimizing large layers, this type of transform can quickly run out of memory. See KroneckerTransform for a scalable alternative. Arguments module_cls (callable) - A callable that instantiates the module used to transform gradients. Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) linear_transform = ModuleTransform ( torch . nn . Linear ) linear_update = linear_transform ( classifier . weight ) # maps gradients to updates, both of shape (1, 7840) loss ( classifier ( X ), y ) . backward () update = linear_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD. KroneckerTransform \u00b6 1 KroneckerTransform ( kronecker_cls , bias = False , psd = True ) [Source] Description The KroneckerTransform creates a an optimization transform based on nn.Module's that admit a Kronecker factorization. (see l2l.nn.Kronecker* ) Akin to the ModuleTransform, this class of transform instanciates a module from its class, based on a given parameter. But, instead of reshaping the gradients to shape (1, param.numel()) , this class assumes a Kronecker factorization of the weights for memory and computational efficiency. The specific dimension of the Kronecker factorization depends on the the parameter's shape. For a weight of shape (n, m), a KroneckerLinear transform consists of two weights with shapes (n, n) and (m, m) rather than a single weight of shape (nm, nm). Refer to Arnold et al., 2019 for more details. Arguments kronecker_cls (callable) - A callable that instantiates the Kronecker module used to transform gradients. References Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) kronecker_transform = KroneckerTransform ( l2l . nn . KroneckerLinear ) kronecker_update = kronecker_transform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = kronecker_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD. MetaCurvatureTransform \u00b6 1 MetaCurvatureTransform ( param , lr = 1.0 ) [Source] Description Implements the Meta-Curvature transform of Park and Oliva, 2019. Unlike ModuleTranform and KroneckerTransform , this class does not wrap other Modules but is directly called on a weight to instantiate the transform. Arguments param (Tensor) - The weight whose gradients will be transformed. lr (float, optional , default=1.0) - Scaling factor of the udpate. (non-learnable) References Park & Oliva. 2019. Meta-curvature. Example 1 2 3 4 5 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) metacurvature_update = MetaCurvatureTransform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = metacurvature_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"learn2learn.optim"},{"location":"docs/learn2learn.optim/#learn2learnoptim","text":"A set of utilities to write differentiable optimization algorithms.","title":"learn2learn.optim"},{"location":"docs/learn2learn.optim/#learnableoptimizer","text":"1 LearnableOptimizer ( model , transform , lr = 1.0 ) [Source] Description A PyTorch Optimizer with learnable transform, enabling the implementation of meta-descent / hyper-gradient algorithms. This optimizer takes a Module and a gradient transform. At each step, the gradient of the module is passed through the transforms, and the module differentiably update -- i.e. when the next backward is called, gradients of both the module and the transform are computed. In turn, the transform can be updated via your favorite optmizer. Arguments model (Module) - Module to be updated. transform (Module) - Transform used to compute updates of the model. lr (float) - Learning rate. References Sutton. 1992. \u201cGain Adaptation Beats Least Squares.\u201d Schraudolph. 1999. \u201cLocal Gain Adaptation in Stochastic Gradient Descent.\u201d Baydin et al. 2017. \u201cOnline Learning Rate Adaptation with Hypergradient Descent.\u201d Majumder et al. 2019. \u201cLearning the Learning Rate for Gradient Descent by Gradient Descent.\u201d Jacobsen et al. 2019. \u201cMeta-Descent for Online, Continual Prediction.\u201d Example 1 2 3 4 5 6 7 8 9 10 11 linear = nn . Linear ( 784 , 10 ) transform = l2l . optim . ModuleTransform ( torch . nn . Linear ) metaopt = l2l . optim . LearnableOptimizer ( linear , transform , lr = 0.01 ) opt = torch . optim . SGD ( metaopt . parameters (), lr = 0.001 ) metaopt . zero_grad () opt . zero_grad () error = loss ( linear ( X ), y ) error . backward () opt . step () # update metaopt metaopt . step () # update linear","title":"LearnableOptimizer"},{"location":"docs/learn2learn.optim/#zero_grad","text":"1 LearnableOptimizer . zero_grad () Only reset target parameters.","title":"zero_grad"},{"location":"docs/learn2learn.optim/#parameterupdate","text":"1 ParameterUpdate ( parameters , transform ) [Source] Description Convenience class to implement custom update functions. Objects instantiated from this class behave similarly to torch.autograd.grad , but return parameter updates as opposed to gradients. Concretely, the gradients are first computed, then fed to their respective transform whose output is finally returned to the user. Additionally, this class supports parameters that might not require updates by setting the allow_nograd flag to True. In this case, the returned update is None . Arguments parameters (list) - Parameters of the model to update. transform (callable) - A callable that returns an instantiated transform given a parameter. Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 model = torch . nn . Linear () transform = l2l . optim . KroneckerTransform ( l2l . nn . KroneckerLinear ) get_update = ParameterUpdate ( model , transform ) opt = torch . optim . SGD ( model . parameters () + get_update . parameters ()) for iteration in range ( 10 ): opt . zero_grad () error = loss ( model ( X ), y ) updates = get_update ( error , model . parameters (), create_graph = True , ) l2l . update_module ( model , updates ) opt . step ()","title":"ParameterUpdate"},{"location":"docs/learn2learn.optim/#forward","text":"1 2 3 4 5 6 ParameterUpdate . forward ( loss , parameters , create_graph = False , retain_graph = False , allow_unused = False , allow_nograd = False ) Description Similar to torch.autograd.grad, but passes the gradients through the provided transform. Arguments loss (Tensor) - The loss to differentiate. parameters (iterable) - Parameters w.r.t. which we want to compute the update. create_graph (bool, optional , default=False) - Same as torch.autograd.grad . retain_graph (bool, optional , default=False) - Same as torch.autograd.grad . allow_unused (bool, optional , default=False) - Same as torch.autograd.grad . allow_nograd (bool, optional , default=False) - Properly handles parameters that do not require gradients. (Their update will be None .)","title":"forward"},{"location":"docs/learn2learn.optim/#differentiablesgd","text":"1 DifferentiableSGD ( lr ) [Source] Description A callable object that applies a list of updates to the parameters of a torch.nn.Module in a differentiable manner. For each parameter p and corresponding gradient g , calling an instance of this class results in updating parameters: p \\gets p - \\alpha g, where \\alpha is the learning rate. Note: The module is updated in-place. Arguments lr (float) - The learning rate used to update the model. Example 1 2 3 4 5 6 sgd = DifferentiableSGD ( 0.1 ) gradients = torch . autograd . grad ( loss , model . parameters (), create_gaph = True ) sgd ( model , gradients ) # model is updated in-place","title":"DifferentiableSGD"},{"location":"docs/learn2learn.optim/#forward_1","text":"1 DifferentiableSGD . forward ( module , gradients = None ) Arguments module (Module) - The module to update. gradients (list, optional , default=None) - A list of gradients for each parameter of the module. If None, will use the gradients in .grad attributes.","title":"forward"},{"location":"docs/learn2learn.optim/#learn2learnoptimtransforms","text":"Optimization transforms are special modules that take gradients as inputs and output model updates. Transforms are usually parameterized, and those parameters can be learned by gradient descent, allow you to learn optimization functions from data.","title":"learn2learn.optim.transforms"},{"location":"docs/learn2learn.optim/#moduletransform","text":"1 ModuleTransform ( module_cls ) [Source] Description The ModuleTransform creates a an optimization transform based on any nn.Module. ModuleTransform automatically instanciates a module from its class, based on a given parameter. The input and output shapes are of the module are set to (1, param.numel()) . When optimizing large layers, this type of transform can quickly run out of memory. See KroneckerTransform for a scalable alternative. Arguments module_cls (callable) - A callable that instantiates the module used to transform gradients. Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) linear_transform = ModuleTransform ( torch . nn . Linear ) linear_update = linear_transform ( classifier . weight ) # maps gradients to updates, both of shape (1, 7840) loss ( classifier ( X ), y ) . backward () update = linear_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"ModuleTransform"},{"location":"docs/learn2learn.optim/#kroneckertransform","text":"1 KroneckerTransform ( kronecker_cls , bias = False , psd = True ) [Source] Description The KroneckerTransform creates a an optimization transform based on nn.Module's that admit a Kronecker factorization. (see l2l.nn.Kronecker* ) Akin to the ModuleTransform, this class of transform instanciates a module from its class, based on a given parameter. But, instead of reshaping the gradients to shape (1, param.numel()) , this class assumes a Kronecker factorization of the weights for memory and computational efficiency. The specific dimension of the Kronecker factorization depends on the the parameter's shape. For a weight of shape (n, m), a KroneckerLinear transform consists of two weights with shapes (n, n) and (m, m) rather than a single weight of shape (nm, nm). Refer to Arnold et al., 2019 for more details. Arguments kronecker_cls (callable) - A callable that instantiates the Kronecker module used to transform gradients. References Arnold et al. 2019. \"When MAML can adapt fast and how to assist when it cannot\". Example 1 2 3 4 5 6 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) kronecker_transform = KroneckerTransform ( l2l . nn . KroneckerLinear ) kronecker_update = kronecker_transform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = kronecker_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"KroneckerTransform"},{"location":"docs/learn2learn.optim/#metacurvaturetransform","text":"1 MetaCurvatureTransform ( param , lr = 1.0 ) [Source] Description Implements the Meta-Curvature transform of Park and Oliva, 2019. Unlike ModuleTranform and KroneckerTransform , this class does not wrap other Modules but is directly called on a weight to instantiate the transform. Arguments param (Tensor) - The weight whose gradients will be transformed. lr (float, optional , default=1.0) - Scaling factor of the udpate. (non-learnable) References Park & Oliva. 2019. Meta-curvature. Example 1 2 3 4 5 classifier = torch . nn . Linear ( 784 , 10 , bias = False ) metacurvature_update = MetaCurvatureTransform ( classifier . weight ) loss ( classifier ( X ), y ) . backward () update = metacurvature_update ( classifier . weight . grad ) classifier . weight . data . add_ ( - lr , update ) # Not a differentiable update. See l2l.optim.DifferentiableSGD.","title":"MetaCurvatureTransform"},{"location":"docs/learn2learn.text/","text":"NewsClassification \u00b6 1 NewsClassification ( root , train = True , transform = None , download = False ) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.text/#newsclassification","text":"1 NewsClassification ( root , train = True , transform = None , download = False ) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.vision/","text":"learn2learn.vision \u00b6 Datasets, models, and other utilities related to computer vision. learn2learn.vision.models \u00b6 Description A set of commonly used models for meta-learning vision tasks. OmniglotFC \u00b6 1 OmniglotFC ( input_size , output_size , sizes = None ) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example 1 2 3 net = OmniglotFC ( input_size = 28 ** 2 , output_size = 10 , sizes = [ 64 , 64 , 64 ]) OmniglotCNN \u00b6 1 OmniglotCNN ( output_size = 5 , hidden_size = 64 , layers = 4 ) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = OmniglotCNN ( output_size = 20 , hidden_size = 128 , layers = 3 ) MiniImagenetCNN \u00b6 1 MiniImagenetCNN ( output_size , hidden_size = 32 , layers = 4 ) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = MiniImagenetCNN ( output_size = 20 , hidden_size = 128 , layers = 3 ) learn2learn.vision.datasets \u00b6 Description Some datasets commonly used in meta-learning vision tasks. FullOmniglot \u00b6 1 FullOmniglot ( root , transform = None , target_transform = None , download = False ) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 4 5 6 7 8 omniglot = l2l . vision . datasets . FullOmniglot ( root = './data' , transform = transforms . Compose ([ transforms . Resize ( 28 , interpolation = LANCZOS ), transforms . ToTensor (), lambda x : 1.0 - x , ]), download = True ) omniglot = l2l . data . MetaDataset ( omniglot ) MiniImagenet \u00b6 1 2 3 4 5 MiniImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . MiniImagenet ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways ) TieredImagenet \u00b6 1 2 3 4 5 TieredImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The tiered -ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository. Like mini -ImageNet, tiered -ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100. The train-validation-test split is made such that classes from similar categories are in the same splits. There are 34 categories each containing between 10 and 30 classes. Of these categories, 20 (351 classes; 448,695 images) are used for training, 6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing. References Ren et al, 2018. \"Meta-Learning for Semi-Supervised Few-Shot Classification.\" ICLR '18. Ren Mengye. 2018. \"few-shot-ssl-public\". https://github.com/renmengye/few-shot-ssl-public Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . TieredImagenet ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) FC100 \u00b6 1 2 3 4 5 FC100 ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The FC100 dataset was originally introduced by Oreshkin et al., 2018. It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are split so as to minimize the information overlap between splits. The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training, 4 (20 classes) for validation, and 4 (20 classes) for testing. Each class contains 600 images. The specific splits are provided in the Supplementary Material of the paper. Our data is downloaded from the link provided by [2]. References Oreshkin et al. 2018. \"TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning.\" NeurIPS. Kwoonjoon Lee. 2019. \"MetaOptNet.\" https://github.com/kjunelee/MetaOptNet Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . FC100 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) CIFARFS \u00b6 1 2 3 4 5 CIFARFS ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019. It consists of 60'000 colour images of sizes 32x32 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019. References Bertinetto et al. 2019. \"Meta-learning with differentiable closed-form solvers\". ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . CIFARFS ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways ) VGGFlower102 \u00b6 1 2 3 4 5 VGGFlower102 ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images. We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al. References Nilsback, M. and A. Zisserman. 2006. \"A Visual Vocabulary for Flower Classification.\" CVPR '06. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. https://www.robots.ox.ac.uk/~vgg/data/flowers/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . VGGFlower102 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) FGVCAircraft \u00b6 1 2 3 4 5 FGVCAircraft ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 10,200 images of aircraft (102 classes, each 100 images). We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al. References Maji et al. 2013. \"Fine-Grained Visual Classification of Aircraft.\" arXiv [cs.CV]. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . FGVCAircraft ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) learn2learn.vision.transforms \u00b6 Description A set of transformations commonly used in meta-learning vision tasks. RandomClassRotation \u00b6 1 RandomClassRotation ( dataset , degrees ) [Source] Description Samples rotations from a given list uniformly at random, and applies it to all images from a given class. Arguments degrees (list) - The rotations to be sampled. Example 1 transform = RandomClassRotation ([ 0 , 90 , 180 , 270 ]) learn2learn.vision.benchmarks \u00b6 The benchmark modules provides a convenient interface to standardized benchmarks in the literature. It provides train/validation/test TaskDatasets and TaskTransforms for pre-defined datasets. This utility is useful for researchers to compare new algorithms against existing benchmarks. For a more fine-grained control over tasks and data, we recommend directly using l2l.data.TaskDataset and l2l.data.TaskTransforms . list_tasksets \u00b6 1 list_tasksets () [Source] Description Returns a list of all available benchmarks. Example 1 2 3 for name in l2l . vision . benchmarks . list_tasksets (): print ( name ) tasksets = l2l . vision . benchmarks . get_tasksets ( name ) get_tasksets \u00b6 1 2 3 4 5 6 7 8 9 get_tasksets ( name , train_ways = 5 , train_samples = 10 , test_ways = 5 , test_samples = 10 , num_tasks =- 1 , root = '~/data' , device = None , ** kwargs ) [Source] Description Returns the tasksets for a particular benchmark, using literature standard data and task transformations. The returned object is a namedtuple with attributes train , validation , test which correspond to their respective TaskDatasets. See examples/vision/maml_miniimagenet.py for an example. Arguments name (str) - The name of the benchmark. Full list in list_tasksets() . train_ways (int, optional , default=5) - The number of classes per train tasks. train_samples (int, optional , default=10) - The number of samples per train tasks. test_ways (int, optional , default=5) - The number of classes per test tasks. Also used for validation tasks. test_samples (int, optional , default=10) - The number of samples per test tasks. Also used for validation tasks. num_tasks (int, optional , default=-1) - The number of tasks in each TaskDataset. root (str, optional , default='~/data') - Where the data is stored. Example 1 2 3 4 5 6 7 train_tasks , validation_tasks , test_tasks = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = train_tasks . sample () or : tasksets = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = tasksets . train . sample ()","title":"learn2learn.vision"},{"location":"docs/learn2learn.vision/#learn2learnvision","text":"Datasets, models, and other utilities related to computer vision.","title":"learn2learn.vision"},{"location":"docs/learn2learn.vision/#learn2learnvisionmodels","text":"Description A set of commonly used models for meta-learning vision tasks.","title":"learn2learn.vision.models"},{"location":"docs/learn2learn.vision/#omniglotfc","text":"1 OmniglotFC ( input_size , output_size , sizes = None ) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example 1 2 3 net = OmniglotFC ( input_size = 28 ** 2 , output_size = 10 , sizes = [ 64 , 64 , 64 ])","title":"OmniglotFC"},{"location":"docs/learn2learn.vision/#omniglotcnn","text":"1 OmniglotCNN ( output_size = 5 , hidden_size = 64 , layers = 4 ) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = OmniglotCNN ( output_size = 20 , hidden_size = 128 , layers = 3 )","title":"OmniglotCNN"},{"location":"docs/learn2learn.vision/#miniimagenetcnn","text":"1 MiniImagenetCNN ( output_size , hidden_size = 32 , layers = 4 ) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = MiniImagenetCNN ( output_size = 20 , hidden_size = 128 , layers = 3 )","title":"MiniImagenetCNN"},{"location":"docs/learn2learn.vision/#learn2learnvisiondatasets","text":"Description Some datasets commonly used in meta-learning vision tasks.","title":"learn2learn.vision.datasets"},{"location":"docs/learn2learn.vision/#fullomniglot","text":"1 FullOmniglot ( root , transform = None , target_transform = None , download = False ) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 4 5 6 7 8 omniglot = l2l . vision . datasets . FullOmniglot ( root = './data' , transform = transforms . Compose ([ transforms . Resize ( 28 , interpolation = LANCZOS ), transforms . ToTensor (), lambda x : 1.0 - x , ]), download = True ) omniglot = l2l . data . MetaDataset ( omniglot )","title":"FullOmniglot"},{"location":"docs/learn2learn.vision/#miniimagenet","text":"1 2 3 4 5 MiniImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . MiniImagenet ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways )","title":"MiniImagenet"},{"location":"docs/learn2learn.vision/#tieredimagenet","text":"1 2 3 4 5 TieredImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The tiered -ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository. Like mini -ImageNet, tiered -ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100. The train-validation-test split is made such that classes from similar categories are in the same splits. There are 34 categories each containing between 10 and 30 classes. Of these categories, 20 (351 classes; 448,695 images) are used for training, 6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing. References Ren et al, 2018. \"Meta-Learning for Semi-Supervised Few-Shot Classification.\" ICLR '18. Ren Mengye. 2018. \"few-shot-ssl-public\". https://github.com/renmengye/few-shot-ssl-public Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . TieredImagenet ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"TieredImagenet"},{"location":"docs/learn2learn.vision/#fc100","text":"1 2 3 4 5 FC100 ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The FC100 dataset was originally introduced by Oreshkin et al., 2018. It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are split so as to minimize the information overlap between splits. The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training, 4 (20 classes) for validation, and 4 (20 classes) for testing. Each class contains 600 images. The specific splits are provided in the Supplementary Material of the paper. Our data is downloaded from the link provided by [2]. References Oreshkin et al. 2018. \"TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning.\" NeurIPS. Kwoonjoon Lee. 2019. \"MetaOptNet.\" https://github.com/kjunelee/MetaOptNet Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . FC100 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"FC100"},{"location":"docs/learn2learn.vision/#cifarfs","text":"1 2 3 4 5 CIFARFS ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019. It consists of 60'000 colour images of sizes 32x32 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019. References Bertinetto et al. 2019. \"Meta-learning with differentiable closed-form solvers\". ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . CIFARFS ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways )","title":"CIFARFS"},{"location":"docs/learn2learn.vision/#vggflower102","text":"1 2 3 4 5 VGGFlower102 ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images. We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al. References Nilsback, M. and A. Zisserman. 2006. \"A Visual Vocabulary for Flower Classification.\" CVPR '06. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. https://www.robots.ox.ac.uk/~vgg/data/flowers/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . VGGFlower102 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"VGGFlower102"},{"location":"docs/learn2learn.vision/#fgvcaircraft","text":"1 2 3 4 5 FGVCAircraft ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 10,200 images of aircraft (102 classes, each 100 images). We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al. References Maji et al. 2013. \"Fine-Grained Visual Classification of Aircraft.\" arXiv [cs.CV]. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . FGVCAircraft ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"FGVCAircraft"},{"location":"docs/learn2learn.vision/#learn2learnvisiontransforms","text":"Description A set of transformations commonly used in meta-learning vision tasks.","title":"learn2learn.vision.transforms"},{"location":"docs/learn2learn.vision/#randomclassrotation","text":"1 RandomClassRotation ( dataset , degrees ) [Source] Description Samples rotations from a given list uniformly at random, and applies it to all images from a given class. Arguments degrees (list) - The rotations to be sampled. Example 1 transform = RandomClassRotation ([ 0 , 90 , 180 , 270 ])","title":"RandomClassRotation"},{"location":"docs/learn2learn.vision/#learn2learnvisionbenchmarks","text":"The benchmark modules provides a convenient interface to standardized benchmarks in the literature. It provides train/validation/test TaskDatasets and TaskTransforms for pre-defined datasets. This utility is useful for researchers to compare new algorithms against existing benchmarks. For a more fine-grained control over tasks and data, we recommend directly using l2l.data.TaskDataset and l2l.data.TaskTransforms .","title":"learn2learn.vision.benchmarks"},{"location":"docs/learn2learn.vision/#list_tasksets","text":"1 list_tasksets () [Source] Description Returns a list of all available benchmarks. Example 1 2 3 for name in l2l . vision . benchmarks . list_tasksets (): print ( name ) tasksets = l2l . vision . benchmarks . get_tasksets ( name )","title":"list_tasksets"},{"location":"docs/learn2learn.vision/#get_tasksets","text":"1 2 3 4 5 6 7 8 9 get_tasksets ( name , train_ways = 5 , train_samples = 10 , test_ways = 5 , test_samples = 10 , num_tasks =- 1 , root = '~/data' , device = None , ** kwargs ) [Source] Description Returns the tasksets for a particular benchmark, using literature standard data and task transformations. The returned object is a namedtuple with attributes train , validation , test which correspond to their respective TaskDatasets. See examples/vision/maml_miniimagenet.py for an example. Arguments name (str) - The name of the benchmark. Full list in list_tasksets() . train_ways (int, optional , default=5) - The number of classes per train tasks. train_samples (int, optional , default=10) - The number of samples per train tasks. test_ways (int, optional , default=5) - The number of classes per test tasks. Also used for validation tasks. test_samples (int, optional , default=10) - The number of samples per test tasks. Also used for validation tasks. num_tasks (int, optional , default=-1) - The number of tasks in each TaskDataset. root (str, optional , default='~/data') - Where the data is stored. Example 1 2 3 4 5 6 7 train_tasks , validation_tasks , test_tasks = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = train_tasks . sample () or : tasksets = l2l . vision . benchmarks . get_tasksets ( 'omniglot' ) batch = tasksets . train . sample ()","title":"get_tasksets"},{"location":"tutorials/getting_started/","text":"Getting Started \u00b6 learn2learn is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules. Installing \u00b6 A pip package is available, updated periodically. Use the command: pip install -U learn2learn For the most update-to-date version clone the repository and use: pip install -e . When installing from sources, make sure that Cython is installed: pip install cython . Info While learn2learn is actively used in current research projects, it is still in development. Breaking changes might occur. Development \u00b6 To simplify the development process, the following commands can be executed from the cloned sources: make build - Builds learn2learn in place. make clean - Cleans previous installation. make lint - Runs linting on the codebase. make lint-examples - Runs linting on the examples. make tests - Runs a light testing suite. (i.e. the Travis one) make alltests - Runs an extensive testing suite. (much longer) make docs - Builds the documentation and serves the website locally. Tip If you encounter a problem, feel free to an open an issue and we'll look into it.","title":"Getting Started"},{"location":"tutorials/getting_started/#getting-started","text":"learn2learn is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules.","title":"Getting Started"},{"location":"tutorials/getting_started/#installing","text":"A pip package is available, updated periodically. Use the command: pip install -U learn2learn For the most update-to-date version clone the repository and use: pip install -e . When installing from sources, make sure that Cython is installed: pip install cython . Info While learn2learn is actively used in current research projects, it is still in development. Breaking changes might occur.","title":"Installing"},{"location":"tutorials/getting_started/#development","text":"To simplify the development process, the following commands can be executed from the cloned sources: make build - Builds learn2learn in place. make clean - Cleans previous installation. make lint - Runs linting on the codebase. make lint-examples - Runs linting on the examples. make tests - Runs a light testing suite. (i.e. the Travis one) make alltests - Runs an extensive testing suite. (much longer) make docs - Builds the documentation and serves the website locally. Tip If you encounter a problem, feel free to an open an issue and we'll look into it.","title":"Development"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/","text":"Feature Reuse with ANIL \u00b6 Written by Ewina Pun on 3/30/2020. In this article, we will dive into a meta-learning algorithm called ANIL (Almost No Inner Loop) presented by Raghu et al., 2019 , and explain how to implement it with learn2learn. Note This tutorial is written for experienced PyTorch users who are getting started with meta-learning. Overview \u00b6 We look into how ANIL takes advantage of feature reuse for few-shot learning. ANIL simplifies MAML by removing the inner loop for all but the task-specific head of the underlying neural network. ANIL performs as well as MAML on benchmark few-shot classification and reinforcement learning tasks, and is computationally more efficient than MAML. We implement ANIL with learn2learn and provide additional results of how ANIL performs on other datasets. Lastly, we explain the implementation code step-be-step, making it easy for users to try ANIL on other datasets. ANIL algorithm \u00b6 Among various meta-learning algorithms for few-shot learning, MAML (model-agnostic meta-learning) (Finn et al. 2017) has been highly popular due to its substantial performance on several benchmarks. Its idea is to establish a meta-learner that seeks an initialization useful for fast learning of different tasks, then adapt to specific tasks quickly (within a few steps) and efficiently (with only a few examples). There are two types of parameter updates: the outer loop and the inner loop. The outer loop updates the meta-initialization of the neural network parameters to a setting that enables fast adaptation to new tasks. The inner loop takes the outer loop initialization and performs task-specific adaptation over a few labeled samples. To read more about meta-learning and MAML, you can read the summary article written by Finn on learning to learn and Lilian Weng's review on meta-learning . In 2019, Raghu et al. conjectured that we can obtain the same rapid learning performance of MAML solely through feature reuse. To test this hypothesis, they introduced ANIL (almost no inner loop), a simplified algorithm of MAML that is equally effective but computationally faster. Rapid learning vs. feature reuse Visualizations of rapid learning and feature reuse. Diagram from Raghu et al., 2019. Before we describe ANIL, we have to understand the difference between rapid learning and feature reuse. In rapid learning , the meta-initialization in the outer loop results in a parameter setting that is favorable for fast learning, thus significant adaptation to new tasks can rapidly take place in the inner loop. In feature reuse , the meta-initialization already contains useful features that can be reused, so little adaptation on the parameters is required in the inner loop. To prove feature reuse is a competitive alternative to rapid learning in MAML, the authors proposed a simplified algorithm, ANIL, where the inner loop is removed for all but the task-specific head of the underlying neural network during training and testing. ANIL vs. MAML Now, let us illustrate the difference mathematically. Let \\theta be the set of meta-initialization parameters for the feature extractable layers of the network and w be the meta-initialization parameters for the head. We obtain the label prediction \\hat{y} = w^T\\phi_\\theta(x) , where x is the input data and \\phi is a feature extractor parametrized by \\theta . Given \\theta_i and w_i at iteration step i , the outer loop updates both parameters via gradient descent: \\theta_{i+1} = \\theta_i - \\alpha\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) w_{i+1} = w_i - \\alpha\\nabla_{w_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) where \\mathcal{L}_\\tau is the loss computed for task \\tau , and \\alpha is the meta learning rate in the outer loop. Notice how the gradient is taken with respect to the initialization parameters w_i and \\theta_i , but the loss is computed on the adapted parameters \\theta_i^\\prime and w_i^\\prime . For one adaptation step in the inner loop, ANIL computes those adapted parameters as: \\theta_{i}^\\prime = \\theta_i w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) where \\beta is the learning rate of the inner loop. Concretely, ANIL keeps the feature extractor constant and only adapts the head with gradient descent. In contrast, MAML updates both the head and the feature extractor: \\theta_{i}^\\prime = \\theta_i - \\beta\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y). Unsurprisingly, ANIL is much more computationally efficient since it requires fewer updates in the inner loop. What might be surprising, is that this efficiency comes at almost no cost in terms of performance. Results ANIL provides fast adaptation in the absence of almost all inner loop parameter updates, while still matching the performance of MAML on few-shot image classification with Mini-ImageNet and Omniglot and standard reinforcement learning tasks. Method Omniglot-20way-1shot Omniglot-20way-5shot Mini-ImageNet-5way-1shot Mini-ImageNet-5way-5shot MAML 93.7 \u00b1 0.7 96.4 \u00b1 0.1 46.9 \u00b1 0.2 63.1 \u00b1 0.4 ANIL 96.2 \u00b1 0.5 98.0 \u00b1 0.3 46.7 \u00b1 0.4 61.5 \u00b1 0.5 Using ANIL with learn2learn \u00b6 With our understanding of how ANIL works, we are ready to implement the algorithm. An example implementation on the FC100 dataset is available at: anil_fc100.py . Using this implementation, we are able to obtain the following results on datasets such as Mini-ImageNet, CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn Mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% ANIL Implementation \u00b6 This section breaks down step-by-step the ANIL implementation with our example code. Creating dataset 1 2 3 4 train_dataset = l2l . vision . datasets . FC100 ( root = '~/data' , transform = tv . transforms . ToTensor (), mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) First, data are obtained and separated into train, validation and test dataset with l2l.vision.datasets.FC100 . tv.transforms.ToTensor() converts Python Imaging Library (PIL) images to PyTorch tensors. l2l.data.MetaDataset is a thin wrapper around torch datasets that automatically generates bookkeeping information to create new tasks. 1 2 3 4 5 6 7 8 9 train_transforms = [ FusedNWaysKShots ( train_dataset , n = ways , k = 2 * shots ), LoadData ( train_dataset ), RemapLabels ( train_dataset ), ConsecutiveLabels ( train_dataset ), ] train_tasks = l2l . data . TaskDataset ( train_dataset , task_transforms = train_transforms , num_tasks = 20000 ) l2l.data.TaskDataset creates a set of tasks from the MetaDataset using a list of task transformations: FusedNWaysKShots(dataset, n=ways, k=2*shots) : efficient implementation to keep k data samples from n randomly sampled labels. LoadData(dataset) : loads a sample from the dataset given its index. RemapLabels(dataset) : given samples from n classes, maps the labels to 0, \\dots, n . ConsecutiveLabels(dataset) : re-orders the samples in the task description such that they are sorted in consecutive order. Question Why k = 2*shots ? The number of samples k is twice the number of shots because one half of the samples are for adaption and the other half are for evaluation in the inner loop. Info For more details, please refer to the documentation of learn2learn.data . Creating model 1 2 3 4 5 6 features = l2l . vision . models . ConvBase ( output_size = 64 , channels = 3 , max_pool = True ) features = torch . nn . Sequential ( features , Lambda ( lambda x : x . view ( - 1 , 256 ))) features . to ( device ) head = torch . nn . Linear ( 256 , ways ) head = l2l . algorithms . MAML ( head , lr = fast_lr ) head . to ( device ) We then instantiate two modules, one for features and one for the head. ConvBase instantiates a four-layer CNN, and the head is a fully connected layer. Because we are not updating the feature extractor parameters, we only need to wrap the head with the l2l.algorithms.MAML() wrapper, which takes in the fast adaptation learning rate fast_lr used for the inner loop later. Info For more details on the MAML wrapper, please refer to the documentation of l2l.algorithms . Optimization setup 1 2 3 all_parameters = list ( features . parameters ()) + list ( head . parameters ()) optimizer = torch . optim . Adam ( all_parameters , lr = meta_lr ) loss = nn . CrossEntropyLoss ( reduction = 'mean' ) Next, we set up the optimizer with mini-batch SGD using torch.optim.Adam , which takes in both feature and head parameters, and learning rate meta_lr used for the outer loop. Outer loop 1 2 3 4 5 6 for iteration in range ( iters ): ... for task in range ( meta_bsz ): learner = head . clone () batch = train_tasks . sample () ... For training, validation and testing, we first sample a task, then copy the head with head.clone() , which is a method exposed by the MAML wrapper for PyTorch modules, akin to tensor.clone() for PyTorch tensors. Calling clone() allows us to update the parameters of the clone while maintaining ability to back-propagate to the parameters in head . There's no need for feature.clone() as we are only adapting the head. Inner loop 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def fast_adapt ( batch , learner , features , loss , adaptation_steps , shots , ways , device = None ): data , labels = batch data , labels = data . to ( device ), labels . to ( device ) data = features ( data ) # Separate data into adaptation/evaluation sets adaptation_indices = np . zeros ( data . size ( 0 ), dtype = bool ) adaptation_indices [ np . arange ( shots * ways ) * 2 ] = True evaluation_indices = torch . from_numpy ( ~ adaptation_indices ) adaptation_indices = torch . from_numpy ( adaptation_indices ) adaptation_data , adaptation_labels = data [ adaptation_indices ], labels [ adaptation_indices ] evaluation_data , evaluation_labels = data [ evaluation_indices ], labels [ evaluation_indices ] for step in range ( adaptation_steps ): train_error = loss ( learner ( adaptation_data ), adaptation_labels ) learner . adapt ( train_error ) predictions = learner ( evaluation_data ) valid_error = loss ( predictions , evaluation_labels ) valid_accuracy = accuracy ( predictions , evaluation_labels ) return valid_error , valid_accuracy In fast_adapt() , we separate data into adaptation and evaluation sets with k shot samples each. In each adaptation step, learner.adapt() takes a gradient step on the loss and updates the cloned parameter, learner , such that we can back-propagate through the adaptation step. Under the hood, this is achieved by calling torch.autograd.grad() and setting create_graph=True . fast_adapt() then returns the evaluation loss and accuracy based on the predicted and true labels. Question Why is the number of adaptation steps so small? To demonstrate fast adaptation, we want the algorithm to adapt to each specific task quickly within a few steps. Since the number of samples is so small in few-shot learning, increasing number of adaptation steps would not help raising the performance. Closing the outer loop 1 2 3 4 5 6 7 8 evaluation_error . backward () meta_train_error += evaluation_error . item () meta_train_accuracy += evaluation_accuracy . item () ... # Average the accumulated gradients and optimize for p in all_parameters : p . grad . data . mul_ ( 1.0 / meta_bsz ) optimizer . step () We compute the gradients with evaluation_error.backward() right after the inner loop updates to free activation and adaptation buffers from memory as early as possible. Lastly, after collecting the gradients, we average the accumulated gradients and updates the parameter at the end of each iteration with optimizer.step() . Conclusion \u00b6 Having explained the inner-workings of ANIL and its code implementation with learn2learn, I hope this tutorial will be helpful to those who are interested in using ANIL for their own research and applications. References \u00b6 Raghu, A., Raghu, M., Bengio, S., & Vinyals, O. (2019). Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1909.09157 Finn, C., Abbeel, P., & Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1703.03400","title":"Feature Reuse with ANIL"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#feature-reuse-with-anil","text":"Written by Ewina Pun on 3/30/2020. In this article, we will dive into a meta-learning algorithm called ANIL (Almost No Inner Loop) presented by Raghu et al., 2019 , and explain how to implement it with learn2learn. Note This tutorial is written for experienced PyTorch users who are getting started with meta-learning.","title":"Feature Reuse with ANIL"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#overview","text":"We look into how ANIL takes advantage of feature reuse for few-shot learning. ANIL simplifies MAML by removing the inner loop for all but the task-specific head of the underlying neural network. ANIL performs as well as MAML on benchmark few-shot classification and reinforcement learning tasks, and is computationally more efficient than MAML. We implement ANIL with learn2learn and provide additional results of how ANIL performs on other datasets. Lastly, we explain the implementation code step-be-step, making it easy for users to try ANIL on other datasets.","title":"Overview"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#anil-algorithm","text":"Among various meta-learning algorithms for few-shot learning, MAML (model-agnostic meta-learning) (Finn et al. 2017) has been highly popular due to its substantial performance on several benchmarks. Its idea is to establish a meta-learner that seeks an initialization useful for fast learning of different tasks, then adapt to specific tasks quickly (within a few steps) and efficiently (with only a few examples). There are two types of parameter updates: the outer loop and the inner loop. The outer loop updates the meta-initialization of the neural network parameters to a setting that enables fast adaptation to new tasks. The inner loop takes the outer loop initialization and performs task-specific adaptation over a few labeled samples. To read more about meta-learning and MAML, you can read the summary article written by Finn on learning to learn and Lilian Weng's review on meta-learning . In 2019, Raghu et al. conjectured that we can obtain the same rapid learning performance of MAML solely through feature reuse. To test this hypothesis, they introduced ANIL (almost no inner loop), a simplified algorithm of MAML that is equally effective but computationally faster. Rapid learning vs. feature reuse Visualizations of rapid learning and feature reuse. Diagram from Raghu et al., 2019. Before we describe ANIL, we have to understand the difference between rapid learning and feature reuse. In rapid learning , the meta-initialization in the outer loop results in a parameter setting that is favorable for fast learning, thus significant adaptation to new tasks can rapidly take place in the inner loop. In feature reuse , the meta-initialization already contains useful features that can be reused, so little adaptation on the parameters is required in the inner loop. To prove feature reuse is a competitive alternative to rapid learning in MAML, the authors proposed a simplified algorithm, ANIL, where the inner loop is removed for all but the task-specific head of the underlying neural network during training and testing. ANIL vs. MAML Now, let us illustrate the difference mathematically. Let \\theta be the set of meta-initialization parameters for the feature extractable layers of the network and w be the meta-initialization parameters for the head. We obtain the label prediction \\hat{y} = w^T\\phi_\\theta(x) , where x is the input data and \\phi is a feature extractor parametrized by \\theta . Given \\theta_i and w_i at iteration step i , the outer loop updates both parameters via gradient descent: \\theta_{i+1} = \\theta_i - \\alpha\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) w_{i+1} = w_i - \\alpha\\nabla_{w_i}\\mathcal{L}_{\\tau}(w^{\\prime \\top}_i\\phi_{\\theta^\\prime_i}(x), y) where \\mathcal{L}_\\tau is the loss computed for task \\tau , and \\alpha is the meta learning rate in the outer loop. Notice how the gradient is taken with respect to the initialization parameters w_i and \\theta_i , but the loss is computed on the adapted parameters \\theta_i^\\prime and w_i^\\prime . For one adaptation step in the inner loop, ANIL computes those adapted parameters as: \\theta_{i}^\\prime = \\theta_i w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) where \\beta is the learning rate of the inner loop. Concretely, ANIL keeps the feature extractor constant and only adapts the head with gradient descent. In contrast, MAML updates both the head and the feature extractor: \\theta_{i}^\\prime = \\theta_i - \\beta\\nabla_{\\theta_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y) w_{i}^\\prime = w_i - \\beta\\nabla_{w_i}\\mathcal{L}_{\\tau}(w_i^T\\phi_{\\theta_i}(x), y). Unsurprisingly, ANIL is much more computationally efficient since it requires fewer updates in the inner loop. What might be surprising, is that this efficiency comes at almost no cost in terms of performance. Results ANIL provides fast adaptation in the absence of almost all inner loop parameter updates, while still matching the performance of MAML on few-shot image classification with Mini-ImageNet and Omniglot and standard reinforcement learning tasks. Method Omniglot-20way-1shot Omniglot-20way-5shot Mini-ImageNet-5way-1shot Mini-ImageNet-5way-5shot MAML 93.7 \u00b1 0.7 96.4 \u00b1 0.1 46.9 \u00b1 0.2 63.1 \u00b1 0.4 ANIL 96.2 \u00b1 0.5 98.0 \u00b1 0.3 46.7 \u00b1 0.4 61.5 \u00b1 0.5","title":"ANIL algorithm"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#using-anil-with-learn2learn","text":"With our understanding of how ANIL works, we are ready to implement the algorithm. An example implementation on the FC100 dataset is available at: anil_fc100.py . Using this implementation, we are able to obtain the following results on datasets such as Mini-ImageNet, CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn Mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6%","title":"Using ANIL with learn2learn"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#anil-implementation","text":"This section breaks down step-by-step the ANIL implementation with our example code. Creating dataset 1 2 3 4 train_dataset = l2l . vision . datasets . FC100 ( root = '~/data' , transform = tv . transforms . ToTensor (), mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) First, data are obtained and separated into train, validation and test dataset with l2l.vision.datasets.FC100 . tv.transforms.ToTensor() converts Python Imaging Library (PIL) images to PyTorch tensors. l2l.data.MetaDataset is a thin wrapper around torch datasets that automatically generates bookkeeping information to create new tasks. 1 2 3 4 5 6 7 8 9 train_transforms = [ FusedNWaysKShots ( train_dataset , n = ways , k = 2 * shots ), LoadData ( train_dataset ), RemapLabels ( train_dataset ), ConsecutiveLabels ( train_dataset ), ] train_tasks = l2l . data . TaskDataset ( train_dataset , task_transforms = train_transforms , num_tasks = 20000 ) l2l.data.TaskDataset creates a set of tasks from the MetaDataset using a list of task transformations: FusedNWaysKShots(dataset, n=ways, k=2*shots) : efficient implementation to keep k data samples from n randomly sampled labels. LoadData(dataset) : loads a sample from the dataset given its index. RemapLabels(dataset) : given samples from n classes, maps the labels to 0, \\dots, n . ConsecutiveLabels(dataset) : re-orders the samples in the task description such that they are sorted in consecutive order. Question Why k = 2*shots ? The number of samples k is twice the number of shots because one half of the samples are for adaption and the other half are for evaluation in the inner loop. Info For more details, please refer to the documentation of learn2learn.data . Creating model 1 2 3 4 5 6 features = l2l . vision . models . ConvBase ( output_size = 64 , channels = 3 , max_pool = True ) features = torch . nn . Sequential ( features , Lambda ( lambda x : x . view ( - 1 , 256 ))) features . to ( device ) head = torch . nn . Linear ( 256 , ways ) head = l2l . algorithms . MAML ( head , lr = fast_lr ) head . to ( device ) We then instantiate two modules, one for features and one for the head. ConvBase instantiates a four-layer CNN, and the head is a fully connected layer. Because we are not updating the feature extractor parameters, we only need to wrap the head with the l2l.algorithms.MAML() wrapper, which takes in the fast adaptation learning rate fast_lr used for the inner loop later. Info For more details on the MAML wrapper, please refer to the documentation of l2l.algorithms . Optimization setup 1 2 3 all_parameters = list ( features . parameters ()) + list ( head . parameters ()) optimizer = torch . optim . Adam ( all_parameters , lr = meta_lr ) loss = nn . CrossEntropyLoss ( reduction = 'mean' ) Next, we set up the optimizer with mini-batch SGD using torch.optim.Adam , which takes in both feature and head parameters, and learning rate meta_lr used for the outer loop. Outer loop 1 2 3 4 5 6 for iteration in range ( iters ): ... for task in range ( meta_bsz ): learner = head . clone () batch = train_tasks . sample () ... For training, validation and testing, we first sample a task, then copy the head with head.clone() , which is a method exposed by the MAML wrapper for PyTorch modules, akin to tensor.clone() for PyTorch tensors. Calling clone() allows us to update the parameters of the clone while maintaining ability to back-propagate to the parameters in head . There's no need for feature.clone() as we are only adapting the head. Inner loop 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def fast_adapt ( batch , learner , features , loss , adaptation_steps , shots , ways , device = None ): data , labels = batch data , labels = data . to ( device ), labels . to ( device ) data = features ( data ) # Separate data into adaptation/evaluation sets adaptation_indices = np . zeros ( data . size ( 0 ), dtype = bool ) adaptation_indices [ np . arange ( shots * ways ) * 2 ] = True evaluation_indices = torch . from_numpy ( ~ adaptation_indices ) adaptation_indices = torch . from_numpy ( adaptation_indices ) adaptation_data , adaptation_labels = data [ adaptation_indices ], labels [ adaptation_indices ] evaluation_data , evaluation_labels = data [ evaluation_indices ], labels [ evaluation_indices ] for step in range ( adaptation_steps ): train_error = loss ( learner ( adaptation_data ), adaptation_labels ) learner . adapt ( train_error ) predictions = learner ( evaluation_data ) valid_error = loss ( predictions , evaluation_labels ) valid_accuracy = accuracy ( predictions , evaluation_labels ) return valid_error , valid_accuracy In fast_adapt() , we separate data into adaptation and evaluation sets with k shot samples each. In each adaptation step, learner.adapt() takes a gradient step on the loss and updates the cloned parameter, learner , such that we can back-propagate through the adaptation step. Under the hood, this is achieved by calling torch.autograd.grad() and setting create_graph=True . fast_adapt() then returns the evaluation loss and accuracy based on the predicted and true labels. Question Why is the number of adaptation steps so small? To demonstrate fast adaptation, we want the algorithm to adapt to each specific task quickly within a few steps. Since the number of samples is so small in few-shot learning, increasing number of adaptation steps would not help raising the performance. Closing the outer loop 1 2 3 4 5 6 7 8 evaluation_error . backward () meta_train_error += evaluation_error . item () meta_train_accuracy += evaluation_accuracy . item () ... # Average the accumulated gradients and optimize for p in all_parameters : p . grad . data . mul_ ( 1.0 / meta_bsz ) optimizer . step () We compute the gradients with evaluation_error.backward() right after the inner loop updates to free activation and adaptation buffers from memory as early as possible. Lastly, after collecting the gradients, we average the accumulated gradients and updates the parameter at the end of each iteration with optimizer.step() .","title":"ANIL Implementation"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#conclusion","text":"Having explained the inner-workings of ANIL and its code implementation with learn2learn, I hope this tutorial will be helpful to those who are interested in using ANIL for their own research and applications.","title":"Conclusion"},{"location":"tutorials/anil_tutorial/ANIL_tutorial/#references","text":"Raghu, A., Raghu, M., Bengio, S., & Vinyals, O. (2019). Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1909.09157 Finn, C., Abbeel, P., & Levine, S. (2017). Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1703.03400","title":"References"}]}