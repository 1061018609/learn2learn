{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"learn2learn is a PyTorch library for meta-learning implementations. The goal of meta-learning is to enable agents to learn how to learn . That is, we would like our agents to become better learners as they solve more and more tasks. For example, the animation below shows an agent that learns to run after a only one parameter update. Features learn2learn provides high- and low-level utilities for meta-learning. The high-level utilities allow arbitrary users to take advantage of exisiting meta-learning algorithms. The low-level utilities enable researchers to develop new and better meta-learning algorithms. Some features of learn2learn include: Modular API: implement your own training loops with our low-level utilities. Provides various meta-learning algorithms (e.g. MAML, FOMAML, MetaSGD, ProtoNets, DiCE) Task generator with unified API, compatible with torchvision, torchtext, torchaudio, and cherry. Provides standardized meta-learning tasks for vision (Omniglot, mini-ImageNet), reinforcement learning (Particles, Mujoco), and even text (news classification). 100% compatible with PyTorch -- use your own modules, datasets, or libraries! Installation pip install learn2learn API Demo The following is an example of using the high-level MAML implementation on MNIST. For more algorithms and lower-level utilities, please refer to the documentation or the examples . import learn2learn as l2l mnist = torchvision.datasets.MNIST(root=\"/tmp/mnist\", train=True) mnist = l2l.data.MetaDataset(mnist) task_generator = l2l.data.TaskGenerator(mnist, ways=3, classes=[0, 1, 4, 6, 8, 9], tasks=10) model = Net() maml = l2l.algorithms.MAML(model, lr=1e-3, first_order=False) opt = optim.Adam(maml.parameters(), lr=4e-3) for iteration in range(num_iterations): learner = maml.clone() # Creates a clone of model adaptation_task = task_generator.sample(shots=1) # Fast adapt for step in range(adaptation_steps): error = compute_loss(adaptation_task) learner.adapt(error) # Compute evaluation loss evaluation_task = task_generator.sample(shots=1, task=adaptation_task.sampled_task) evaluation_error = compute_loss(evaluation_task) # Meta-update the model parameters opt.zero_grad() evaluation_error.backward() opt.step() Acknowledgements The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License.","title":"Home"},{"location":"#installation","text":"pip install learn2learn","title":"Installation"},{"location":"#api-demo","text":"The following is an example of using the high-level MAML implementation on MNIST. For more algorithms and lower-level utilities, please refer to the documentation or the examples . import learn2learn as l2l mnist = torchvision.datasets.MNIST(root=\"/tmp/mnist\", train=True) mnist = l2l.data.MetaDataset(mnist) task_generator = l2l.data.TaskGenerator(mnist, ways=3, classes=[0, 1, 4, 6, 8, 9], tasks=10) model = Net() maml = l2l.algorithms.MAML(model, lr=1e-3, first_order=False) opt = optim.Adam(maml.parameters(), lr=4e-3) for iteration in range(num_iterations): learner = maml.clone() # Creates a clone of model adaptation_task = task_generator.sample(shots=1) # Fast adapt for step in range(adaptation_steps): error = compute_loss(adaptation_task) learner.adapt(error) # Compute evaluation loss evaluation_task = task_generator.sample(shots=1, task=adaptation_task.sampled_task) evaluation_error = compute_loss(evaluation_task) # Meta-update the model parameters opt.zero_grad() evaluation_error.backward() opt.step()","title":"API Demo"},{"location":"#acknowledgements","text":"The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License.","title":"Acknowledgements"},{"location":"docs/learn2learn.algorithms/","text":"MAML MAML(model, lr, first_order=False) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d Example linear = l2l.algorithms.MAML(nn.Linear(20, 10), lr=0.01) clone = linear.clone() error = loss(clone(X), y) clone.adapt(error) error = loss(clone(X), y) error.backward() adapt MAML.adapt(loss, first_order=None) Description Updates the clone parameters in place using the MAML update. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. clone MAML.clone(first_order=None) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. maml_update maml_update(model, lr, grads=None) Description Performs a MAML update on model using grads and lr. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lr (float) - The learning rate used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example maml = l2l.algorithms.MAML(Model(), lr=0.1) model = maml.clone() # The next two lines essentially implement model.adapt(loss) grads = autograd.grad(loss, model.parameters(), create_graph=True) maml_update(model, lr=0.1, grads) MetaSGD MetaSGD(model, lr=1.0, first_order=False, lrs=None) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example linear = l2l.algorithms.MetaSGD(nn.Linear(20, 10), lr=0.01) clone = linear.clone() error = loss(clone(X), y) clone.adapt(error) error = loss(clone(X), y) error.backward() clone MetaSGD.clone() Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates. adapt MetaSGD.adapt(loss, first_order=None) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates. meta_sgd_update meta_sgd_update(model, lrs=None, grads=None) Description Performs a MetaSGD update on model using grads and lrs. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lrs (list) - The meta-learned learning rates used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example meta = l2l.algorithms.MetaSGD(Model(), lr=1.0) lrs = [th.ones_like(p) for p in meta.model.parameters()] model = meta.clone() # The next two lines essentially implement model.adapt(loss) grads = autograd.grad(loss, model.parameters(), create_graph=True) meta_sgd_update(model, lrs=lrs, grads)","title":"learn2learn.algorithms"},{"location":"docs/learn2learn.algorithms/#maml","text":"MAML(model, lr, first_order=False) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d Example linear = l2l.algorithms.MAML(nn.Linear(20, 10), lr=0.01) clone = linear.clone() error = loss(clone(X), y) clone.adapt(error) error = loss(clone(X), y) error.backward()","title":"MAML"},{"location":"docs/learn2learn.algorithms/#adapt","text":"MAML.adapt(loss, first_order=None) Description Updates the clone parameters in place using the MAML update. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#clone","text":"MAML.clone(first_order=None) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order.","title":"clone"},{"location":"docs/learn2learn.algorithms/#maml_update","text":"maml_update(model, lr, grads=None) Description Performs a MAML update on model using grads and lr. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lr (float) - The learning rate used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example maml = l2l.algorithms.MAML(Model(), lr=0.1) model = maml.clone() # The next two lines essentially implement model.adapt(loss) grads = autograd.grad(loss, model.parameters(), create_graph=True) maml_update(model, lr=0.1, grads)","title":"maml_update"},{"location":"docs/learn2learn.algorithms/#metasgd","text":"MetaSGD(model, lr=1.0, first_order=False, lrs=None) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example linear = l2l.algorithms.MetaSGD(nn.Linear(20, 10), lr=0.01) clone = linear.clone() error = loss(clone(X), y) clone.adapt(error) error = loss(clone(X), y) error.backward()","title":"MetaSGD"},{"location":"docs/learn2learn.algorithms/#clone_1","text":"MetaSGD.clone() Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates.","title":"clone"},{"location":"docs/learn2learn.algorithms/#adapt_1","text":"MetaSGD.adapt(loss, first_order=None) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#meta_sgd_update","text":"meta_sgd_update(model, lrs=None, grads=None) Description Performs a MetaSGD update on model using grads and lrs. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lrs (list) - The meta-learned learning rates used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example meta = l2l.algorithms.MetaSGD(Model(), lr=1.0) lrs = [th.ones_like(p) for p in meta.model.parameters()] model = meta.clone() # The next two lines essentially implement model.adapt(loss) grads = autograd.grad(loss, model.parameters(), create_graph=True) meta_sgd_update(model, lrs=lrs, grads)","title":"meta_sgd_update"},{"location":"docs/learn2learn.data/","text":"MetaDataset MetaDataset(dataset) Descritpion It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. Example mnist = torchvision.datasets.MNIST(root=\"/tmp/mnist\", train=True) mnist = l2l.data.MetaDataset(mnist) TaskGenerator TaskGenerator(dataset, classes=None, ways=2, tasks=1, shots=1) [Source] Description A wrapper to generate few-shot classification tasks. tasks can both indicate predefined tasks, or just the number of tasks to sample. If specified as an int, a list of size task would be generated from which we'll sample. If specified as a list, then that list of tasks would be used to sample always. The acceptable shape of list would be n * w , with n the number of tasks to sample and w the number of ways. Each of the task should have w distinct elements all of which are required to be a subset of ways. Arguments dataset (MetaDataset or Dataset) - The (meta-) dataset to wrap. classes (list, optional , default=None) - List of classes to sample from, if none then sample from all available classes in dataset. (default: None) ways (int, optional , default=2) - Number of labels to sample from. shots (int, optional , default=1) - Number of data points per task to sample. tasks (int or list, optional , default=1) - Tasks to be generated. sample TaskGenerator.sample(shots=None, task=None) Description Returns a dataset and the labels that we have sampled. The dataset is of length shots * ways . The length of labels we have sampled is the same as shots . Arguments shots (int, optional , default=None) - Number of data points to return per class, if None gets self.shots. task (list, optional , default=None) - List of labels you want to sample from. Returns Dataset - Containing the sampled task.","title":"learn2learn.data"},{"location":"docs/learn2learn.data/#metadataset","text":"MetaDataset(dataset) Descritpion It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. Example mnist = torchvision.datasets.MNIST(root=\"/tmp/mnist\", train=True) mnist = l2l.data.MetaDataset(mnist)","title":"MetaDataset"},{"location":"docs/learn2learn.data/#taskgenerator","text":"TaskGenerator(dataset, classes=None, ways=2, tasks=1, shots=1) [Source] Description A wrapper to generate few-shot classification tasks. tasks can both indicate predefined tasks, or just the number of tasks to sample. If specified as an int, a list of size task would be generated from which we'll sample. If specified as a list, then that list of tasks would be used to sample always. The acceptable shape of list would be n * w , with n the number of tasks to sample and w the number of ways. Each of the task should have w distinct elements all of which are required to be a subset of ways. Arguments dataset (MetaDataset or Dataset) - The (meta-) dataset to wrap. classes (list, optional , default=None) - List of classes to sample from, if none then sample from all available classes in dataset. (default: None) ways (int, optional , default=2) - Number of labels to sample from. shots (int, optional , default=1) - Number of data points per task to sample. tasks (int or list, optional , default=1) - Tasks to be generated.","title":"TaskGenerator"},{"location":"docs/learn2learn.data/#sample","text":"TaskGenerator.sample(shots=None, task=None) Description Returns a dataset and the labels that we have sampled. The dataset is of length shots * ways . The length of labels we have sampled is the same as shots . Arguments shots (int, optional , default=None) - Number of data points to return per class, if None gets self.shots. task (list, optional , default=None) - List of labels you want to sample from. Returns Dataset - Containing the sampled task.","title":"sample"},{"location":"docs/learn2learn.gym/","text":"learn2learn.gym MetaEnv MetaEnv(task=None) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations. AsyncVectorEnv AsyncVectorEnv(env_fns, env=None) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations. learn2learn.gym.envs.mujoco HalfCheetahForwardBackwardEnv HalfCheetahForwardBackwardEnv(task=None) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntForwardBackwardEnv AntForwardBackwardEnv(task=None) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntDirectionEnv AntDirectionEnv(task=None) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidForwardBackwardEnv HumanoidForwardBackwardEnv(task=None) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidDirectionEnv HumanoidDirectionEnv(task=None) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. learn2learn.gym.envs.particles Particles2DEnv Particles2DEnv(task=None) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation.","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#learn2learngym","text":"","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#metaenv","text":"MetaEnv(task=None) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations.","title":"MetaEnv"},{"location":"docs/learn2learn.gym/#asyncvectorenv","text":"AsyncVectorEnv(env_fns, env=None) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations.","title":"AsyncVectorEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsmujoco","text":"","title":"learn2learn.gym.envs.mujoco"},{"location":"docs/learn2learn.gym/#halfcheetahforwardbackwardenv","text":"HalfCheetahForwardBackwardEnv(task=None) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HalfCheetahForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antforwardbackwardenv","text":"AntForwardBackwardEnv(task=None) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antdirectionenv","text":"AntDirectionEnv(task=None) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntDirectionEnv"},{"location":"docs/learn2learn.gym/#humanoidforwardbackwardenv","text":"HumanoidForwardBackwardEnv(task=None) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#humanoiddirectionenv","text":"HumanoidDirectionEnv(task=None) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidDirectionEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsparticles","text":"","title":"learn2learn.gym.envs.particles"},{"location":"docs/learn2learn.gym/#particles2denv","text":"Particles2DEnv(task=None) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation.","title":"Particles2DEnv"},{"location":"docs/learn2learn/","text":"clone_module clone_module(module) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example net = nn.Sequential(Linear(20, 10), nn.ReLU(), nn.Linear(10, 2)) clone = clone_module(net) error = loss(clone(X), y) error.backward() # Gradients are back-propagate all the way to net. detach_module detach_module(module) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example net = nn.Sequential(Linear(20, 10), nn.ReLU(), nn.Linear(10, 2)) clone = clone_module(net) detach_module(clone) error = loss(clone(X), y) error.backward() # Gradients are back-propagate on clone, not net. magic_box magic_box(x) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \u201cDiCE: The Infinitely Differentiable Monte-Carlo Estimator.\u201d arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example loss = (magic_box(cum_log_probs) * advantages).mean() # loss is the mean advantage loss.backward()","title":"learn2learn"},{"location":"docs/learn2learn/#clone_module","text":"clone_module(module) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example net = nn.Sequential(Linear(20, 10), nn.ReLU(), nn.Linear(10, 2)) clone = clone_module(net) error = loss(clone(X), y) error.backward() # Gradients are back-propagate all the way to net.","title":"clone_module"},{"location":"docs/learn2learn/#detach_module","text":"detach_module(module) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example net = nn.Sequential(Linear(20, 10), nn.ReLU(), nn.Linear(10, 2)) clone = clone_module(net) detach_module(clone) error = loss(clone(X), y) error.backward() # Gradients are back-propagate on clone, not net.","title":"detach_module"},{"location":"docs/learn2learn/#magic_box","text":"magic_box(x) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \u201cDiCE: The Infinitely Differentiable Monte-Carlo Estimator.\u201d arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example loss = (magic_box(cum_log_probs) * advantages).mean() # loss is the mean advantage loss.backward()","title":"magic_box"},{"location":"docs/learn2learn.text/","text":"NewsClassification NewsClassification(root, train=True, transform=None, download=False) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.text/#newsclassification","text":"NewsClassification(root, train=True, transform=None, download=False) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.vision/","text":"learn2learn.vision.models Description A set of commonly used models for meta-learning vision tasks. OmniglotFC OmniglotFC(input_size, output_size, sizes=None) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example net = OmniglotFC(input_size=28**2, output_size=10, sizes=[64, 64, 64]) OmniglotCNN OmniglotCNN(output_size=5, hidden_size=64, layers=4) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example model = OmniglotCNN(output_size=20, hidden_size=128, layers=3) MiniImagenetCNN MiniImagenetCNN(output_size, hidden_size=32, layers=4) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example model = MiniImagenet(output_size=20, hidden_size=128, layers=3) learn2learn.vision.datasets Description Some datasets commonly used in meta-learning vision tasks. FullOmniglot FullOmniglot(root, transform=None, target_transform=None, download=False) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example omniglot = l2l.vision.datasets.FullOmniglot(root='./data', transform=transforms.Compose([ l2l.vision.transforms.RandomDiscreteRotation( [0.0, 90.0, 180.0, 270.0]), transforms.Resize(28, interpolation=LANCZOS), transforms.ToTensor(), lambda x: 1.0 - x, ]), download=True) omniglot = l2l.data.MetaDataset(omniglot) MiniImagenet MiniImagenet(root, mode='train', transform=None, target_transform=None) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example train_dataset = l2l.vision.datasets.MiniImagenet(root='./data', mode='train') train_dataset = l2l.data.MetaDataset(train_dataset) train_generator = l2l.data.TaskGenerator(dataset=train_dataset, ways=ways) learn2learn.vision.transforms Description A set of transformations commonly used in meta-learning vision tasks. RandomDiscreteRotation RandomDiscreteRotation(degrees, *args, **kwargs) [Source] Description Samples rotations from a given list, uniformly at random. Arguments degrees (list) - The rotations to be sampled. Example transform = RandomDiscreteRotation([0, 90, 180, 270])","title":"learn2learn.vision"},{"location":"docs/learn2learn.vision/#learn2learnvisionmodels","text":"Description A set of commonly used models for meta-learning vision tasks.","title":"learn2learn.vision.models"},{"location":"docs/learn2learn.vision/#omniglotfc","text":"OmniglotFC(input_size, output_size, sizes=None) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example net = OmniglotFC(input_size=28**2, output_size=10, sizes=[64, 64, 64])","title":"OmniglotFC"},{"location":"docs/learn2learn.vision/#omniglotcnn","text":"OmniglotCNN(output_size=5, hidden_size=64, layers=4) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example model = OmniglotCNN(output_size=20, hidden_size=128, layers=3)","title":"OmniglotCNN"},{"location":"docs/learn2learn.vision/#miniimagenetcnn","text":"MiniImagenetCNN(output_size, hidden_size=32, layers=4) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example model = MiniImagenet(output_size=20, hidden_size=128, layers=3)","title":"MiniImagenetCNN"},{"location":"docs/learn2learn.vision/#learn2learnvisiondatasets","text":"Description Some datasets commonly used in meta-learning vision tasks.","title":"learn2learn.vision.datasets"},{"location":"docs/learn2learn.vision/#fullomniglot","text":"FullOmniglot(root, transform=None, target_transform=None, download=False) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example omniglot = l2l.vision.datasets.FullOmniglot(root='./data', transform=transforms.Compose([ l2l.vision.transforms.RandomDiscreteRotation( [0.0, 90.0, 180.0, 270.0]), transforms.Resize(28, interpolation=LANCZOS), transforms.ToTensor(), lambda x: 1.0 - x, ]), download=True) omniglot = l2l.data.MetaDataset(omniglot)","title":"FullOmniglot"},{"location":"docs/learn2learn.vision/#miniimagenet","text":"MiniImagenet(root, mode='train', transform=None, target_transform=None) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example train_dataset = l2l.vision.datasets.MiniImagenet(root='./data', mode='train') train_dataset = l2l.data.MetaDataset(train_dataset) train_generator = l2l.data.TaskGenerator(dataset=train_dataset, ways=ways)","title":"MiniImagenet"},{"location":"docs/learn2learn.vision/#learn2learnvisiontransforms","text":"Description A set of transformations commonly used in meta-learning vision tasks.","title":"learn2learn.vision.transforms"},{"location":"docs/learn2learn.vision/#randomdiscreterotation","text":"RandomDiscreteRotation(degrees, *args, **kwargs) [Source] Description Samples rotations from a given list, uniformly at random. Arguments degrees (list) - The rotations to be sampled. Example transform = RandomDiscreteRotation([0, 90, 180, 270])","title":"RandomDiscreteRotation"},{"location":"tutorials/getting_started/","text":"Getting Started L2L is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules. What is meta-learning? Machine learning is typically concerned with the process of adapting an agent or model to perform well on a given task \\mathcal{T} . If any aspect of \\mathcal{T} changes then we must begin training anew; however, it is easy to imagine several situations where we may want to teach an agent or train a model to perform several tasks that are very similar in nature or skill required. In this case, we would like to extract \"general\" knowledge from training on an individual task to reduce the amount of time and data needed to train on a subsequent later task. To formalize this notion we assume that the tasks trained on are i.i.d. samples \\{\\mathcal{T}_{1} \\dotsb \\mathcal{T}_{m}\\} , and we have a loss function \\mathcal{L} defined for all \\mathcal{T} . We can then phrase the problem of meta-learning a few ways. One way is as k-shot learning, where we aim to find a model or policy M that minimizes E_{\\mathcal{T} }[\\mathcal{L}(M_{k}(\\mathcal{T}))] , where M_{k} denotes the model M after training on \\mathcal{T} for k episodes. For more information about specific meta-learning algorithms, please refer to the appropriate tutorial. How to Use L2L Installing A pip package is available, updated periodically. Use the command: pip install learn2learn For the most update-to-date version clone the repository and use: pip install -e . A list of dependencies is maintained and periodically updated in requirements-dev.txt. To install them all use pip install -r requirements-dev.txt Important: As learn2learn is still in the developmental stage, breaking changes are likely to occur. If you encounter a problem, feel free to an open an issue and we'll look into it. Source Files Examples of learn2learn in action can be found here . The source code for algorithm implementations is also available here .","title":"Getting Started"},{"location":"tutorials/getting_started/#getting-started","text":"L2L is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules.","title":"Getting Started"},{"location":"tutorials/getting_started/#what-is-meta-learning","text":"Machine learning is typically concerned with the process of adapting an agent or model to perform well on a given task \\mathcal{T} . If any aspect of \\mathcal{T} changes then we must begin training anew; however, it is easy to imagine several situations where we may want to teach an agent or train a model to perform several tasks that are very similar in nature or skill required. In this case, we would like to extract \"general\" knowledge from training on an individual task to reduce the amount of time and data needed to train on a subsequent later task. To formalize this notion we assume that the tasks trained on are i.i.d. samples \\{\\mathcal{T}_{1} \\dotsb \\mathcal{T}_{m}\\} , and we have a loss function \\mathcal{L} defined for all \\mathcal{T} . We can then phrase the problem of meta-learning a few ways. One way is as k-shot learning, where we aim to find a model or policy M that minimizes E_{\\mathcal{T} }[\\mathcal{L}(M_{k}(\\mathcal{T}))] , where M_{k} denotes the model M after training on \\mathcal{T} for k episodes. For more information about specific meta-learning algorithms, please refer to the appropriate tutorial.","title":"What is meta-learning?"},{"location":"tutorials/getting_started/#how-to-use-l2l","text":"","title":"How to Use L2L"},{"location":"tutorials/getting_started/#installing","text":"A pip package is available, updated periodically. Use the command: pip install learn2learn For the most update-to-date version clone the repository and use: pip install -e . A list of dependencies is maintained and periodically updated in requirements-dev.txt. To install them all use pip install -r requirements-dev.txt Important: As learn2learn is still in the developmental stage, breaking changes are likely to occur. If you encounter a problem, feel free to an open an issue and we'll look into it.","title":"Installing"},{"location":"tutorials/getting_started/#source-files","text":"Examples of learn2learn in action can be found here . The source code for algorithm implementations is also available here .","title":"Source Files"}]}