{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"learn2learn is a PyTorch library for meta-learning implementations. The goal of meta-learning is to enable agents to learn how to learn . That is, we would like our agents to become better learners as they solve more and more tasks. For example, the animation below shows an agent that learns to run after a only one parameter update. Features learn2learn provides high- and low-level utilities for meta-learning. The high-level utilities allow arbitrary users to take advantage of exisiting meta-learning algorithms. The low-level utilities enable researchers to develop new and better meta-learning algorithms. Some features of learn2learn include: Modular API: implement your own training loops with our low-level utilities. Provides various meta-learning algorithms (e.g. MAML, FOMAML, MetaSGD, ProtoNets, DiCE) Task generator with unified API, compatible with torchvision, torchtext, torchaudio, and cherry. Provides standardized meta-learning tasks for vision (Omniglot, mini-ImageNet), reinforcement learning (Particles, Mujoco), and even text (news classification). 100% compatible with PyTorch -- use your own modules, datasets, or libraries! Installation \u00b6 1 pip install learn2learn API Demo \u00b6 The following is an example of using the high-level MAML implementation on MNIST. For more algorithms and lower-level utilities, please refer to the documentation or the examples . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import learn2learn as l2l mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist ) train_tasks = l2l . data . TaskDataset ( mnist , task_transforms = [ NWays ( mnist , n = 3 ), KShots ( mnist , k = 1 ), LoadData ( mnist ), ], num_tasks = 10 ) model = Net () maml = l2l . algorithms . MAML ( model , lr = 1e-3 , first_order = False ) opt = optim . Adam ( maml . parameters (), lr = 4e-3 ) for iteration in range ( num_iterations ): learner = maml . clone () # Creates a clone of model for task in train_tasks : # Split task in adaptation_task and evalutation_task # Fast adapt for step in range ( adaptation_steps ): error = compute_loss ( adaptation_task ) learner . adapt ( error ) # Compute evaluation loss evaluation_error = compute_loss ( evaluation_task ) # Meta-update the model parameters opt . zero_grad () evaluation_error . backward () opt . step () Changelog \u00b6 A human-readable changelog is available in the CHANGELOG.md file. Documentation \u00b6 Documentation and tutorials are available on learn2learn\u2019s website: http://learn2learn.net . Citation \u00b6 To cite the learn2learn repository in your academic publications, please use the following reference. Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner. \"learn2learn\" . https://github.com/learnables/learn2learn , 2019. You can also use the following Bibtex entry. 1 2 3 4 5 6 7 @misc { learn2learn2019 , author = {Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner} , title = {learn2learn} , month = sep , year = 2019 , url = {https://github.com/learnables/learn2learn} } Acknowledgements & Friends \u00b6 The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License. TorchMeta is similar library, with a focus on supervised meta-learning. If learn2learn were missing a particular functionality, we would go check if TorchMeta has it. But we would also open an issue ;) higher is a PyTorch library that also enables differentiating through optimization inner-loops. Their approach is different from learn2learn in that they monkey-patch nn.Module to be stateless. For more information, refer to their ArXiv paper .","title":"Home"},{"location":"#installation","text":"1 pip install learn2learn","title":"Installation"},{"location":"#api-demo","text":"The following is an example of using the high-level MAML implementation on MNIST. For more algorithms and lower-level utilities, please refer to the documentation or the examples . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import learn2learn as l2l mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist ) train_tasks = l2l . data . TaskDataset ( mnist , task_transforms = [ NWays ( mnist , n = 3 ), KShots ( mnist , k = 1 ), LoadData ( mnist ), ], num_tasks = 10 ) model = Net () maml = l2l . algorithms . MAML ( model , lr = 1e-3 , first_order = False ) opt = optim . Adam ( maml . parameters (), lr = 4e-3 ) for iteration in range ( num_iterations ): learner = maml . clone () # Creates a clone of model for task in train_tasks : # Split task in adaptation_task and evalutation_task # Fast adapt for step in range ( adaptation_steps ): error = compute_loss ( adaptation_task ) learner . adapt ( error ) # Compute evaluation loss evaluation_error = compute_loss ( evaluation_task ) # Meta-update the model parameters opt . zero_grad () evaluation_error . backward () opt . step ()","title":"API Demo"},{"location":"#changelog","text":"A human-readable changelog is available in the CHANGELOG.md file.","title":"Changelog"},{"location":"#documentation","text":"Documentation and tutorials are available on learn2learn\u2019s website: http://learn2learn.net .","title":"Documentation"},{"location":"#citation","text":"To cite the learn2learn repository in your academic publications, please use the following reference. Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner. \"learn2learn\" . https://github.com/learnables/learn2learn , 2019. You can also use the following Bibtex entry. 1 2 3 4 5 6 7 @misc { learn2learn2019 , author = {Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner} , title = {learn2learn} , month = sep , year = 2019 , url = {https://github.com/learnables/learn2learn} }","title":"Citation"},{"location":"#acknowledgements-friends","text":"The RL environments are adapted from Tristan Deleu's implementations and from the ProMP repository . Both shared with permission, under the MIT License. TorchMeta is similar library, with a focus on supervised meta-learning. If learn2learn were missing a particular functionality, we would go check if TorchMeta has it. But we would also open an issue ;) higher is a PyTorch library that also enables differentiating through optimization inner-loops. Their approach is different from learn2learn in that they monkey-patch nn.Module to be stateless. For more information, refer to their ArXiv paper .","title":"Acknowledgements &amp; Friends"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 Added \u00b6 Changed \u00b6 Fixed \u00b6 v0.1.0.1 \u00b6 Fixed \u00b6 Remove Cython dependency when installing from PyPI and clean up package distribution. v0.1.0 \u00b6 Added \u00b6 A CHANGELOG.md file. New vision datasets: FC100, tiered-Imagenet, FGVCAircraft, VGGFlowers102. New vision examples: Reptile & ANIL. Extensive benchmarks of all vision examples. Changed \u00b6 Re-wrote TaskDataset and task transforms in Cython, for a 20x speed-up. Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5). New Material doc theme with links to changelog and examples. Fixed \u00b6 Support for RandomClassRotation with newer versions of torchvision. Various minor fixes in the examples. Add Dropbox download if GDrive fails for FC100.","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#added","text":"","title":"Added"},{"location":"changelog/#changed","text":"","title":"Changed"},{"location":"changelog/#fixed","text":"","title":"Fixed"},{"location":"changelog/#v0101","text":"","title":"v0.1.0.1"},{"location":"changelog/#fixed_1","text":"Remove Cython dependency when installing from PyPI and clean up package distribution.","title":"Fixed"},{"location":"changelog/#v010","text":"","title":"v0.1.0"},{"location":"changelog/#added_1","text":"A CHANGELOG.md file. New vision datasets: FC100, tiered-Imagenet, FGVCAircraft, VGGFlowers102. New vision examples: Reptile & ANIL. Extensive benchmarks of all vision examples.","title":"Added"},{"location":"changelog/#changed_1","text":"Re-wrote TaskDataset and task transforms in Cython, for a 20x speed-up. Travis testing with different versions of Python (3.6, 3.7), torch (1.1, 1.2, 1.3, 1.4), and torchvision (0.3, 0.4, 0.5). New Material doc theme with links to changelog and examples.","title":"Changed"},{"location":"changelog/#fixed_2","text":"Support for RandomClassRotation with newer versions of torchvision. Various minor fixes in the examples. Add Dropbox download if GDrive fails for FC100.","title":"Fixed"},{"location":"examples.rl/","text":"Meta-Reinforcement Learning \u00b6 Warning Meta-RL results are particularly finicky to compare. Different papers use different environment implementations, which in turn produce different convergence and rewards. The plots below only serve to indicate what kind of performance you can expect with learn2learn. MAML \u00b6 The above results are obtained by running maml_trpo.py on HalfCheetahForwardBackwardEnv and AntForwardBackwardEnv for 300 updates. The figures show the expected sum of rewards over all tasks. The line and shadow are the mean and standard deviation computed over 3 random seeds. Info Those results were obtained in August 2019, and might be outdated.","title":"Reinforcement Learning"},{"location":"examples.rl/#meta-reinforcement-learning","text":"Warning Meta-RL results are particularly finicky to compare. Different papers use different environment implementations, which in turn produce different convergence and rewards. The plots below only serve to indicate what kind of performance you can expect with learn2learn.","title":"Meta-Reinforcement Learning"},{"location":"examples.rl/#maml","text":"The above results are obtained by running maml_trpo.py on HalfCheetahForwardBackwardEnv and AntForwardBackwardEnv for 300 updates. The figures show the expected sum of rewards over all tasks. The line and shadow are the mean and standard deviation computed over 3 random seeds. Info Those results were obtained in August 2019, and might be outdated.","title":"MAML"},{"location":"examples.vision/","text":"Meta-Learning & Computer Vision \u00b6 This directory contains meta-learning examples and reproductions for common computer vision benchmarks. MAML \u00b6 The following files reproduce MAML on the Omniglot and mini -ImageNet datasets. The FOMAML results can be obtained by setting first_order=True in the MAML wrapper. On Omniglot, the CNN results can be obtained by swapping OmniglotFC with OmniglotCNN . maml_omniglot.py - MAML on the Omniglot dataset with a fully-connected network. maml_miniimagenet.py - MAML on the mini -ImageNet dataset with the standard convolutional network. Note that the original MAML paper trains with 5 fast adaptation step, but tests with 10 steps. This implementation only provides the training code. Results When adapting the code to different datasets, we obtained the following results. Only the fast-adaptation learning rate needs a bit of tuning, and good values usually lie in a 0.5-2x range of the original value. Dataset Architecture Ways Shots Original learn2learn Omniglot FC 5 1 89.7% 88.9% Omniglot CNN 5 1 98.7% 99.1% mini-ImageNet CNN 5 1 48.7% 48.3% mini-ImageNet CNN 5 5 63.1% 65.4% CIFAR-FS CNN 5 5 71.5% 73.6% FC100 CNN 5 5 n/a 49.0% Usage Manually edit the respective files and run: 1 python examples/vision/maml_omniglot.py or 1 python examples/vision/maml_miniimagenet.py Prototypical Networks \u00b6 The file protonet_miniimagenet.py reproduces Prototypical Networks on the mini -ImageNet dataset. This implementation provides training and testing code. Results Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 1 49.4% 49.1% mini-ImageNet CNN 5 5 68.2% 66.5% Usage For 1 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py For 5 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py --shot 5 --train-way 20 ANIL \u00b6 The file anil_fc100.py implements ANIL on the FC100 dataset. Results While ANIL only used mini -ImageNet as a benchmark, we provide results for CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% Usage Manually edit the above file and run: 1 python examples/vision/anil_fc100.py Reptile \u00b6 The file reptile_miniimagenet.py reproduces Reptile on the mini -ImageNet dataset. Results The mini -ImageNet file can easily be adapted to obtain results on Omniglot and CIFAR-FS as well. Dataset Architecture Ways Shots Original learn2learn Omniglot CNN 5 5 99.5% 99.5% mini-ImageNet CNN 5 5 66.0% 65.5% CIFAR-FS CNN 10 3 n/a 46.3% Usage Manually edit the above file and run: 1 python examples/vision/reptile_miniimagenet.py","title":"Computer Vision"},{"location":"examples.vision/#meta-learning-computer-vision","text":"This directory contains meta-learning examples and reproductions for common computer vision benchmarks.","title":"Meta-Learning &amp; Computer Vision"},{"location":"examples.vision/#maml","text":"The following files reproduce MAML on the Omniglot and mini -ImageNet datasets. The FOMAML results can be obtained by setting first_order=True in the MAML wrapper. On Omniglot, the CNN results can be obtained by swapping OmniglotFC with OmniglotCNN . maml_omniglot.py - MAML on the Omniglot dataset with a fully-connected network. maml_miniimagenet.py - MAML on the mini -ImageNet dataset with the standard convolutional network. Note that the original MAML paper trains with 5 fast adaptation step, but tests with 10 steps. This implementation only provides the training code. Results When adapting the code to different datasets, we obtained the following results. Only the fast-adaptation learning rate needs a bit of tuning, and good values usually lie in a 0.5-2x range of the original value. Dataset Architecture Ways Shots Original learn2learn Omniglot FC 5 1 89.7% 88.9% Omniglot CNN 5 1 98.7% 99.1% mini-ImageNet CNN 5 1 48.7% 48.3% mini-ImageNet CNN 5 5 63.1% 65.4% CIFAR-FS CNN 5 5 71.5% 73.6% FC100 CNN 5 5 n/a 49.0% Usage Manually edit the respective files and run: 1 python examples/vision/maml_omniglot.py or 1 python examples/vision/maml_miniimagenet.py","title":"MAML"},{"location":"examples.vision/#prototypical-networks","text":"The file protonet_miniimagenet.py reproduces Prototypical Networks on the mini -ImageNet dataset. This implementation provides training and testing code. Results Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 1 49.4% 49.1% mini-ImageNet CNN 5 5 68.2% 66.5% Usage For 1 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py For 5 shot 5 ways: 1 python examples/vision/protonet_miniimagenet.py --shot 5 --train-way 20","title":"Prototypical Networks"},{"location":"examples.vision/#anil","text":"The file anil_fc100.py implements ANIL on the FC100 dataset. Results While ANIL only used mini -ImageNet as a benchmark, we provide results for CIFAR-FS and FC100 as well. Dataset Architecture Ways Shots Original learn2learn mini-ImageNet CNN 5 5 61.5% 63.2% CIFAR-FS CNN 5 5 n/a 68.3% FC100 CNN 5 5 n/a 47.6% Usage Manually edit the above file and run: 1 python examples/vision/anil_fc100.py","title":"ANIL"},{"location":"examples.vision/#reptile","text":"The file reptile_miniimagenet.py reproduces Reptile on the mini -ImageNet dataset. Results The mini -ImageNet file can easily be adapted to obtain results on Omniglot and CIFAR-FS as well. Dataset Architecture Ways Shots Original learn2learn Omniglot CNN 5 5 99.5% 99.5% mini-ImageNet CNN 5 5 66.0% 65.5% CIFAR-FS CNN 10 3 n/a 46.3% Usage Manually edit the above file and run: 1 python examples/vision/reptile_miniimagenet.py","title":"Reptile"},{"location":"docs/learn2learn.algorithms/","text":"learn2learn.algorithms \u00b6 MAML \u00b6 1 MAML ( model , lr , first_order = False , allow_unused = None , allow_nograd = False ) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the first-order approximation of MAML. (FOMAML) allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" Example 1 2 3 4 5 6 linear = l2l . algorithms . MAML ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward () adapt \u00b6 1 MAML . adapt ( loss , first_order = None , allow_unused = None , allow_nograd = None ) Description Updates the clone parameters in place using the MAML update. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. clone \u00b6 1 MAML . clone ( first_order = None , allow_unused = None , allow_nograd = None ) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd. maml_update \u00b6 1 maml_update ( model , lr , grads = None ) [Source] Description Performs a MAML update on model using grads and lr. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lr (float) - The learning rate used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example 1 2 3 4 maml = l2l . algorithms . MAML ( Model (), lr = 0.1 ) model = maml . clone () # The next two lines essentially implement model.adapt(loss) grads = autograd . grad ( loss , model . parameters (), create_graph = True ) maml_update ( model , lr = 0.1 , grads ) MetaSGD \u00b6 1 MetaSGD ( model , lr = 1.0 , first_order = False , lrs = None ) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example 1 2 3 4 5 6 linear = l2l . algorithms . MetaSGD ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward () clone \u00b6 1 MetaSGD . clone () Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates. adapt \u00b6 1 MetaSGD . adapt ( loss , first_order = None ) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates. meta_sgd_update \u00b6 1 meta_sgd_update ( model , lrs = None , grads = None ) Description Performs a MetaSGD update on model using grads and lrs. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lrs (list) - The meta-learned learning rates used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example 1 2 3 4 5 meta = l2l . algorithms . MetaSGD ( Model (), lr = 1.0 ) lrs = [ th . ones_like ( p ) for p in meta . model . parameters ()] model = meta . clone () # The next two lines essentially implement model.adapt(loss) grads = autograd . grad ( loss , model . parameters (), create_graph = True ) meta_sgd_update ( model , lrs = lrs , grads )","title":"learn2learn.algorithms"},{"location":"docs/learn2learn.algorithms/#learn2learnalgorithms","text":"","title":"learn2learn.algorithms"},{"location":"docs/learn2learn.algorithms/#maml","text":"1 MAML ( model , lr , first_order = False , allow_unused = None , allow_nograd = False ) [Source] Description High-level implementation of Model-Agnostic Meta-Learning . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. For the first-order version of MAML (i.e. FOMAML), set the first_order flag to True upon initialization. Arguments model (Module) - Module to be wrapped. lr (float) - Fast adaptation learning rate. first_order (bool, optional , default=False) - Whether to use the first-order approximation of MAML. (FOMAML) allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to allow_nograd . allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" Example 1 2 3 4 5 6 linear = l2l . algorithms . MAML ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward ()","title":"MAML"},{"location":"docs/learn2learn.algorithms/#adapt","text":"1 MAML . adapt ( loss , first_order = None , allow_unused = None , allow_nograd = None ) Description Updates the clone parameters in place using the MAML update. Arguments loss (Tensor) - Loss to minimize upon update. first_order (bool, optional , default=None) - Whether to use first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=None) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#clone","text":"1 MAML . clone ( first_order = None , allow_unused = None , allow_nograd = None ) Description Returns a MAML -wrapped copy of the module whose parameters and buffers are torch.clone d from the original module. This implies that back-propagating losses on the cloned module will populate the buffers of the original module. For more information, refer to learn2learn.clone_module(). Arguments first_order (bool, optional , default=None) - Whether the clone uses first- or second-order updates. Defaults to self.first_order. allow_unused (bool, optional , default=None) - Whether to allow differentiation of unused parameters. Defaults to self.allow_unused. allow_nograd (bool, optional , default=False) - Whether to allow adaptation with parameters that have requires_grad = False . Defaults to self.allow_nograd.","title":"clone"},{"location":"docs/learn2learn.algorithms/#maml_update","text":"1 maml_update ( model , lr , grads = None ) [Source] Description Performs a MAML update on model using grads and lr. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lr (float) - The learning rate used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example 1 2 3 4 maml = l2l . algorithms . MAML ( Model (), lr = 0.1 ) model = maml . clone () # The next two lines essentially implement model.adapt(loss) grads = autograd . grad ( loss , model . parameters (), create_graph = True ) maml_update ( model , lr = 0.1 , grads )","title":"maml_update"},{"location":"docs/learn2learn.algorithms/#metasgd","text":"1 MetaSGD ( model , lr = 1.0 , first_order = False , lrs = None ) [Source] Description High-level implementation of Meta-SGD . This class wraps an arbitrary nn.Module and augments it with clone() and adapt methods. It behaves similarly to MAML , but in addition a set of per-parameters learning rates are learned for fast-adaptation. Arguments model (Module) - Module to be wrapped. lr (float) - Initialization value of the per-parameter fast adaptation learning rates. first_order (bool, optional , default=False) - Whether to use the first-order version. lrs (list of Parameters, optional , default=None) - If not None, overrides lr , and uses the list as learning rates for fast-adaptation. References Li et al. 2017. \u201cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\u201d arXiv. Example 1 2 3 4 5 6 linear = l2l . algorithms . MetaSGD ( nn . Linear ( 20 , 10 ), lr = 0.01 ) clone = linear . clone () error = loss ( clone ( X ), y ) clone . adapt ( error ) error = loss ( clone ( X ), y ) error . backward ()","title":"MetaSGD"},{"location":"docs/learn2learn.algorithms/#clone_1","text":"1 MetaSGD . clone () Descritpion Akin to MAML.clone() but for MetaSGD: it includes a set of learnable fast-adaptation learning rates.","title":"clone"},{"location":"docs/learn2learn.algorithms/#adapt_1","text":"1 MetaSGD . adapt ( loss , first_order = None ) Descritpion Akin to MAML.adapt() but for MetaSGD: it updates the model with the learnable per-parameter learning rates.","title":"adapt"},{"location":"docs/learn2learn.algorithms/#meta_sgd_update","text":"1 meta_sgd_update ( model , lrs = None , grads = None ) Description Performs a MetaSGD update on model using grads and lrs. The function re-routes the Python object, thus avoiding in-place operations. NOTE: The model itself is updated in-place (no deepcopy), but the parameters' tensors are not. Arguments model (Module) - The model to update. lrs (list) - The meta-learned learning rates used to update the model. grads (list, optional , default=None) - A list of gradients for each parameter of the model. If None, will use the gradients in .grad attributes. Example 1 2 3 4 5 meta = l2l . algorithms . MetaSGD ( Model (), lr = 1.0 ) lrs = [ th . ones_like ( p ) for p in meta . model . parameters ()] model = meta . clone () # The next two lines essentially implement model.adapt(loss) grads = autograd . grad ( loss , model . parameters (), create_graph = True ) meta_sgd_update ( model , lrs = lrs , grads )","title":"meta_sgd_update"},{"location":"docs/learn2learn.data/","text":"learn2learn.data \u00b6 MetaDataset \u00b6 1 MetaDataset ( dataset ) Description It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. labels_to_indices (Dict) - A dictionary mapping label to their indices. If not specified then we loop through all the datapoints to understand the mapping. (default: None) Example 1 2 mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist ) TaskDataset \u00b6 1 TaskDataset ( dataset , task_transforms = None , num_tasks =- 1 , task_collate = None ) [Source] Description Creates a set of tasks from a given Dataset. In addition to the Dataset, TaskDataset accepts a list of task transformations ( task_transforms ) which define the kind of tasks sampled from the dataset. The tasks are lazily sampled upon indexing (or calling the .sample() method), and their descriptions cached for later use. If num_tasks is -1, the TaskDataset will not cache task descriptions and instead continuously resample new ones. In this case, the length of the TaskDataset is set to 1. For more information on tasks and task descriptions, please refer to the documentation of task transforms. Arguments dataset (Dataset) - Dataset of data to compute tasks. task_transforms (list, optional , default=None) - List of task transformations. num_tasks (int, optional , default=-1) - Number of tasks to generate. Example 1 2 3 4 5 6 7 8 9 dataset = l2l . data . MetaDataset ( MyDataset ()) transforms = [ l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task learn2learn.data.transforms \u00b6 Description Collection of general task transformations. A task transformation is an object that implements the callable interface. (Either a function or an object that implements the __call__ special method.) Each transformation is called on a task description, which consists of a list of DataDescription with attributes index and transforms , where index corresponds to the index of single data sample inthe dataset, and transforms is a list of transformations that will be applied to the sample. Each transformation must return a new task description. At first, the task description contains all samples from the dataset. A task transform takes this task description list and modifies it such that a particular task is created. For example, the NWays task transform filters data samples from the task description such that remaining ones belong to a random subset of all classes available. (The size of the subset is controlled via the class's n argument.) On the other hand, the LoadData task transform simply appends a call to load the actual data from the dataset to the list of transformations of each sample. To create a task from a task description, the TaskDataset applies each sample's list of transform s in order. Then, all samples are collated via the TaskDataset 's collate function. LoadData \u00b6 1 LoadData ( dataset ) [Source] Description Loads a sample from the dataset given its index. Arguments dataset (Dataset) - The dataset from which to load the sample. NWays \u00b6 1 NWays ( dataset , n = 2 ) [Source] Description Keeps samples from N random labels present in the task description. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. KShots \u00b6 1 KShots ( dataset , k = 1 , replacement = False ) [Source] Description Keeps K samples for each present labels. Arguments dataset (Dataset) - The dataset from which to load the sample. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample with replacement. FilterLabels \u00b6 1 FilterLabels ( dataset , labels ) [Source] Description Removes samples that do not belong to the given set of labels. Arguments dataset (Dataset) - The dataset from which to load the sample. labels (list) - The list of labels to include. FusedNWaysKShots \u00b6 1 FusedNWaysKShots ( dataset , n = 2 , k = 1 , replacement = False , filter_labels = None ) [Source] Description Efficient implementation of FilterLabels, NWays, and KShots. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample shots with replacement. filter_labels (list, optional , default=None) - The list of labels to include. Defaults to all labels in the dataset. RemapLabels \u00b6 1 RemapLabels ( dataset , shuffle = True ) [Source] Description Given samples from K classes, maps the labels to 0, ..., K. Arguments dataset (Dataset) - The dataset from which to load the sample. ConsecutiveLabels \u00b6 1 ConsecutiveLabels ( dataset ) [Source] Description Re-orders the samples in the task description such that they are sorted in consecutive order. Note: when used before RemapLabels , the labels will be homogeneously clustered, but in no specific order. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"learn2learn.data"},{"location":"docs/learn2learn.data/#learn2learndata","text":"","title":"learn2learn.data"},{"location":"docs/learn2learn.data/#metadataset","text":"1 MetaDataset ( dataset ) Description It wraps a torch dataset by creating a map of target to indices. This comes in handy when we want to sample elements randomly for a particular label. Notes: For l2l to work its important that the dataset returns a (data, target) tuple. If your dataset doesn't return that, it should be trivial to wrap your dataset with another class to do that. TODO : Add example for wrapping a non standard l2l dataset Arguments dataset (Dataset) - A torch dataset. labels_to_indices (Dict) - A dictionary mapping label to their indices. If not specified then we loop through all the datapoints to understand the mapping. (default: None) Example 1 2 mnist = torchvision . datasets . MNIST ( root = \"/tmp/mnist\" , train = True ) mnist = l2l . data . MetaDataset ( mnist )","title":"MetaDataset"},{"location":"docs/learn2learn.data/#taskdataset","text":"1 TaskDataset ( dataset , task_transforms = None , num_tasks =- 1 , task_collate = None ) [Source] Description Creates a set of tasks from a given Dataset. In addition to the Dataset, TaskDataset accepts a list of task transformations ( task_transforms ) which define the kind of tasks sampled from the dataset. The tasks are lazily sampled upon indexing (or calling the .sample() method), and their descriptions cached for later use. If num_tasks is -1, the TaskDataset will not cache task descriptions and instead continuously resample new ones. In this case, the length of the TaskDataset is set to 1. For more information on tasks and task descriptions, please refer to the documentation of task transforms. Arguments dataset (Dataset) - Dataset of data to compute tasks. task_transforms (list, optional , default=None) - List of task transformations. num_tasks (int, optional , default=-1) - Number of tasks to generate. Example 1 2 3 4 5 6 7 8 9 dataset = l2l . data . MetaDataset ( MyDataset ()) transforms = [ l2l . data . transforms . NWays ( dataset , n = 5 ), l2l . data . transforms . KShots ( dataset , k = 1 ), l2l . data . transforms . LoadData ( dataset ), ] taskset = TaskDataset ( dataset , transforms , num_tasks = 20000 ) for task in taskset : X , y = task","title":"TaskDataset"},{"location":"docs/learn2learn.data/#learn2learndatatransforms","text":"Description Collection of general task transformations. A task transformation is an object that implements the callable interface. (Either a function or an object that implements the __call__ special method.) Each transformation is called on a task description, which consists of a list of DataDescription with attributes index and transforms , where index corresponds to the index of single data sample inthe dataset, and transforms is a list of transformations that will be applied to the sample. Each transformation must return a new task description. At first, the task description contains all samples from the dataset. A task transform takes this task description list and modifies it such that a particular task is created. For example, the NWays task transform filters data samples from the task description such that remaining ones belong to a random subset of all classes available. (The size of the subset is controlled via the class's n argument.) On the other hand, the LoadData task transform simply appends a call to load the actual data from the dataset to the list of transformations of each sample. To create a task from a task description, the TaskDataset applies each sample's list of transform s in order. Then, all samples are collated via the TaskDataset 's collate function.","title":"learn2learn.data.transforms"},{"location":"docs/learn2learn.data/#loaddata","text":"1 LoadData ( dataset ) [Source] Description Loads a sample from the dataset given its index. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"LoadData"},{"location":"docs/learn2learn.data/#nways","text":"1 NWays ( dataset , n = 2 ) [Source] Description Keeps samples from N random labels present in the task description. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels.","title":"NWays"},{"location":"docs/learn2learn.data/#kshots","text":"1 KShots ( dataset , k = 1 , replacement = False ) [Source] Description Keeps K samples for each present labels. Arguments dataset (Dataset) - The dataset from which to load the sample. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample with replacement.","title":"KShots"},{"location":"docs/learn2learn.data/#filterlabels","text":"1 FilterLabels ( dataset , labels ) [Source] Description Removes samples that do not belong to the given set of labels. Arguments dataset (Dataset) - The dataset from which to load the sample. labels (list) - The list of labels to include.","title":"FilterLabels"},{"location":"docs/learn2learn.data/#fusednwayskshots","text":"1 FusedNWaysKShots ( dataset , n = 2 , k = 1 , replacement = False , filter_labels = None ) [Source] Description Efficient implementation of FilterLabels, NWays, and KShots. Arguments dataset (Dataset) - The dataset from which to load the sample. n (int, optional , default=2) - Number of labels to sample from the task description's labels. k (int, optional , default=1) - The number of samples per label. replacement (bool, optional , default=False) - Whether to sample shots with replacement. filter_labels (list, optional , default=None) - The list of labels to include. Defaults to all labels in the dataset.","title":"FusedNWaysKShots"},{"location":"docs/learn2learn.data/#remaplabels","text":"1 RemapLabels ( dataset , shuffle = True ) [Source] Description Given samples from K classes, maps the labels to 0, ..., K. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"RemapLabels"},{"location":"docs/learn2learn.data/#consecutivelabels","text":"1 ConsecutiveLabels ( dataset ) [Source] Description Re-orders the samples in the task description such that they are sorted in consecutive order. Note: when used before RemapLabels , the labels will be homogeneously clustered, but in no specific order. Arguments dataset (Dataset) - The dataset from which to load the sample.","title":"ConsecutiveLabels"},{"location":"docs/learn2learn.gym/","text":"learn2learn.gym \u00b6 MetaEnv \u00b6 1 MetaEnv ( task = None ) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then 1 2 3 4 5 6 env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations. AsyncVectorEnv \u00b6 1 AsyncVectorEnv ( env_fns , env = None ) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations. learn2learn.gym.envs.mujoco \u00b6 HalfCheetahForwardBackwardEnv \u00b6 1 HalfCheetahForwardBackwardEnv ( task = None ) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntForwardBackwardEnv \u00b6 1 AntForwardBackwardEnv ( task = None ) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. AntDirectionEnv \u00b6 1 AntDirectionEnv ( task = None ) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidForwardBackwardEnv \u00b6 1 HumanoidForwardBackwardEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. HumanoidDirectionEnv \u00b6 1 HumanoidDirectionEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG]. learn2learn.gym.envs.particles \u00b6 Particles2DEnv \u00b6 1 Particles2DEnv ( task = None ) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation.","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#learn2learngym","text":"","title":"learn2learn.gym"},{"location":"docs/learn2learn.gym/#metaenv","text":"1 MetaEnv ( task = None ) [Source] Description Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the values of these parameters as values. Environments must then implement functions to get, set and sample tasks. The flow is then 1 2 3 4 5 6 env = EnvClass() tasks = env.sample_tasks(num_tasks) for task in tasks: env.set_task(task) *training code here* ... Credit Adapted from Tristan Deleu and Jonas Rothfuss' implementations.","title":"MetaEnv"},{"location":"docs/learn2learn.gym/#asyncvectorenv","text":"1 AsyncVectorEnv ( env_fns , env = None ) [Source] Description Asynchronous vectorized environment for working with l2l MetaEnvs. Allows multiple environments to be run as separate processes. Credit Adapted from OpenAI and Tristan Deleu's implementations.","title":"AsyncVectorEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsmujoco","text":"","title":"learn2learn.gym.envs.mujoco"},{"location":"docs/learn2learn.gym/#halfcheetahforwardbackwardenv","text":"1 HalfCheetahForwardBackwardEnv ( task = None ) [Source] Description This environment requires the half-cheetah to learn to run forward or backward. At each time step the half-cheetah receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should move backward and +1 indicates the half-cheetah should move forward. The velocity is calculated as the distance (in the target direction) of the half-cheetah's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HalfCheetahForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antforwardbackwardenv","text":"1 AntForwardBackwardEnv ( task = None ) [Source] Description This environment requires the ant to learn to run forward or backward. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should move backward and +1 indicates the ant should move forward. The velocity is calculated as the distance (in the direction of the plane) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#antdirectionenv","text":"1 AntDirectionEnv ( task = None ) [Source] Description This environment requires the Ant to learn to run in a random direction in the XY plane. At each time step the ant receives a signal composed of a control cost and a reward equal to its average velocity in the direction of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the ant's torso position before and after taking the specified action divided by a small value dt. As noted in [1], a small positive bonus is added to the reward to stop the ant from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"AntDirectionEnv"},{"location":"docs/learn2learn.gym/#humanoidforwardbackwardenv","text":"1 HumanoidForwardBackwardEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run forward or backward. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should move backward and +1 indicates the humanoid should move forward. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidForwardBackwardEnv"},{"location":"docs/learn2learn.gym/#humanoiddirectionenv","text":"1 HumanoidDirectionEnv ( task = None ) [Source] Description This environment requires the humanoid to learn to run in a random direction in the XY plane. At each time step the humanoid receives a signal composed of a control cost and a reward equal to its average velocity in the target direction. The tasks are 2d-arrays sampled uniformly along the unit circle. The target direction is indicated by the vector from the origin to the sampled point. The velocity is calculated as the distance (in the target direction) of the humanoid's torso position before and after taking the specified action divided by a small value dt. A small positive bonus is added to the reward to stop the humanoid from prematurely ending the episode. Credit Adapted from Jonas Rothfuss' implementation. References Finn et al. 2017. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" arXiv [cs.LG]. Rothfuss et al. 2018. \"ProMP: Proximal Meta-Policy Search.\" arXiv [cs.LG].","title":"HumanoidDirectionEnv"},{"location":"docs/learn2learn.gym/#learn2learngymenvsparticles","text":"","title":"learn2learn.gym.envs.particles"},{"location":"docs/learn2learn.gym/#particles2denv","text":"1 Particles2DEnv ( task = None ) [Source] Description Each task is defined by the location of the goal. A point mass receives a directional force and moves accordingly (clipped in [-0.1,0.1]). The reward is equal to the negative distance from the goal. Credit Adapted from Jonas Rothfuss' implementation.","title":"Particles2DEnv"},{"location":"docs/learn2learn/","text":"learn2learn.utils \u00b6 clone_module \u00b6 1 clone_module ( module ) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example 1 2 3 4 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate all the way to net. detach_module \u00b6 1 detach_module ( module ) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example 1 2 3 4 5 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) detach_module ( clone ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate on clone, not net. magic_box \u00b6 1 magic_box ( x ) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example 1 2 loss = ( magic_box ( cum_log_probs ) * advantages ) . mean () # loss is the mean advantage loss . backward ()","title":"learn2learn"},{"location":"docs/learn2learn/#learn2learnutils","text":"","title":"learn2learn.utils"},{"location":"docs/learn2learn/#clone_module","text":"1 clone_module ( module ) [Source] Description Creates a copy of a module, whose parameters/buffers/submodules are created using PyTorch's torch.clone(). This implies that the computational graph is kept, and you can compute the derivatives of the new modules' parameters w.r.t the original parameters. Arguments module (Module) - Module to be cloned. Return (Module) - The cloned module. Example 1 2 3 4 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate all the way to net.","title":"clone_module"},{"location":"docs/learn2learn/#detach_module","text":"1 detach_module ( module ) [Source] Description Detaches all parameters/buffers of a previously cloned module from its computational graph. Note: detach works in-place, so it does not return a copy. Arguments module (Module) - Module to be detached. Example 1 2 3 4 5 net = nn . Sequential ( Linear ( 20 , 10 ), nn . ReLU (), nn . Linear ( 10 , 2 )) clone = clone_module ( net ) detach_module ( clone ) error = loss ( clone ( X ), y ) error . backward () # Gradients are back-propagate on clone, not net.","title":"detach_module"},{"location":"docs/learn2learn/#magic_box","text":"1 magic_box ( x ) [Source] Description The magic box operator, which evaluates to 1 but whose gradient is dx : \\boxdot (x) = \\exp(x - \\bot(x)) where \\bot is the stop-gradient (or detach) operator. This operator is useful when computing higher-order derivatives of stochastic graphs. For more informations, please refer to the DiCE paper. (Reference 1) References Foerster et al. 2018. \"DiCE: The Infinitely Differentiable Monte-Carlo Estimator.\" arXiv. Arguments x (Variable) - Variable to transform. Return (Variable) - Tensor of 1, but it's gradient is the gradient of x. Example 1 2 loss = ( magic_box ( cum_log_probs ) * advantages ) . mean () # loss is the mean advantage loss . backward ()","title":"magic_box"},{"location":"docs/learn2learn.text/","text":"NewsClassification \u00b6 1 NewsClassification ( root , train = True , transform = None , download = False ) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.text/#newsclassification","text":"1 NewsClassification ( root , train = True , transform = None , download = False ) [Source] Description References TODO: Cite ... Arguments Example","title":"NewsClassification"},{"location":"docs/learn2learn.vision/","text":"learn2learn.vision \u00b6 learn2learn.vision.models \u00b6 Description A set of commonly used models for meta-learning vision tasks. OmniglotFC \u00b6 1 OmniglotFC ( input_size , output_size , sizes = None ) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example 1 2 3 net = OmniglotFC ( input_size = 28 ** 2 , output_size = 10 , sizes = [ 64 , 64 , 64 ]) OmniglotCNN \u00b6 1 OmniglotCNN ( output_size = 5 , hidden_size = 64 , layers = 4 ) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = OmniglotCNN ( output_size = 20 , hidden_size = 128 , layers = 3 ) MiniImagenetCNN \u00b6 1 MiniImagenetCNN ( output_size , hidden_size = 32 , layers = 4 ) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = MiniImagenetCNN ( output_size = 20 , hidden_size = 128 , layers = 3 ) learn2learn.vision.datasets \u00b6 Description Some datasets commonly used in meta-learning vision tasks. FullOmniglot \u00b6 1 FullOmniglot ( root , transform = None , target_transform = None , download = False ) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 4 5 6 7 8 omniglot = l2l . vision . datasets . FullOmniglot ( root = './data' , transform = transforms . Compose ([ transforms . Resize ( 28 , interpolation = LANCZOS ), transforms . ToTensor (), lambda x : 1.0 - x , ]), download = True ) omniglot = l2l . data . MetaDataset ( omniglot ) MiniImagenet \u00b6 1 MiniImagenet ( root , mode = 'train' , transform = None , target_transform = None ) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . MiniImagenet ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways ) TieredImagenet \u00b6 1 2 3 4 5 TieredImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The tiered -ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository. Like mini -ImageNet, tiered -ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100. The train-validation-test split is made such that classes from similar categories are in the same splits. There are 34 categories each containing between 10 and 30 classes. Of these categories, 20 (351 classes; 448,695 images) are used for training, 6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing. References Ren et al, 2018. \"Meta-Learning for Semi-Supervised Few-Shot Classification.\" ICLR '18. Ren Mengye. 2018. \"few-shot-ssl-public\". https://github.com/renmengye/few-shot-ssl-public Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . TieredImagenet ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) FC100 \u00b6 1 FC100 ( root , mode = 'train' , transform = None , target_transform = None ) [Source] Description The FC100 dataset was originally introduced by Oreshkin et al., 2018. It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are split so as to minimize the information overlap between splits. The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training, 4 (20 classes) for validation, and 4 (20 classes) for testing. Each class contains 600 images. The specific splits are provided in the Supplementary Material of the paper. Our data is downloaded from the link provided by [2]. References Oreshkin et al. 2018. \"TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning.\" NeurIPS. Kwoonjoon Lee. 2019. \"MetaOptNet.\" https://github.com/kjunelee/MetaOptNet Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . FC100 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) CIFARFS \u00b6 1 CIFARFS ( root , mode = 'train' , transform = None , target_transform = None ) [Source] Description The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019. It consists of 60'000 colour images of sizes 32x32 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019. References Bertinetto et al. 2019. \"Meta-learning with differentiable closed-form solvers\". ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . CIFARFS ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways ) VGGFlower102 \u00b6 1 2 3 4 5 VGGFlower102 ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images. We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al. References Nilsback, M. and A. Zisserman. 2006. \"A Visual Vocabulary for Flower Classification.\" CVPR '06. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. https://www.robots.ox.ac.uk/~vgg/data/flowers/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . VGGFlower102 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) FGVCAircraft \u00b6 1 2 3 4 5 FGVCAircraft ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 10,200 images of aircraft (102 classes, each 100 images). We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al. References Maji et al. 2013. \"Fine-Grained Visual Classification of Aircraft.\" arXiv [cs.CV]. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . FGVCAircraft ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 ) learn2learn.vision.transforms \u00b6 Description A set of transformations commonly used in meta-learning vision tasks. RandomClassRotation \u00b6 1 RandomClassRotation ( dataset , degrees ) [Source] Description Samples rotations from a given list uniformly at random, and applies it to all images from a given class. Arguments degrees (list) - The rotations to be sampled. Example 1 transform = RandomClassRotation ([ 0 , 90 , 180 , 270 ])","title":"learn2learn.vision"},{"location":"docs/learn2learn.vision/#learn2learnvision","text":"","title":"learn2learn.vision"},{"location":"docs/learn2learn.vision/#learn2learnvisionmodels","text":"Description A set of commonly used models for meta-learning vision tasks.","title":"learn2learn.vision.models"},{"location":"docs/learn2learn.vision/#omniglotfc","text":"1 OmniglotFC ( input_size , output_size , sizes = None ) [Source] Description The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016. References Santoro et al. 2016. \u201cMeta-Learning with Memory-Augmented Neural Networks.\u201d ICML. Arguments input_size (int) - The dimensionality of the input. output_size (int) - The dimensionality of the output. sizes (list, optional , default=None) - A list of hidden layer sizes. Example 1 2 3 net = OmniglotFC ( input_size = 28 ** 2 , output_size = 10 , sizes = [ 64 , 64 , 64 ])","title":"OmniglotFC"},{"location":"docs/learn2learn.vision/#omniglotcnn","text":"1 OmniglotCNN ( output_size = 5 , hidden_size = 64 , layers = 4 ) Source Description The convolutional network commonly used for Omniglot, as described by Finn et al, 2017. This network assumes inputs of shapes (1, 28, 28). References Finn et al. 2017. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\u201d ICML. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=64) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = OmniglotCNN ( output_size = 20 , hidden_size = 128 , layers = 3 )","title":"OmniglotCNN"},{"location":"docs/learn2learn.vision/#miniimagenetcnn","text":"1 MiniImagenetCNN ( output_size , hidden_size = 32 , layers = 4 ) [Source] Description The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017. This network assumes inputs of shapes (3, 84, 84). References Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments output_size (int) - The dimensionality of the network's output. hidden_size (int, optional , default=32) - The dimensionality of the hidden representation. layers (int, optional , default=4) - The number of convolutional layers. Example 1 model = MiniImagenetCNN ( output_size = 20 , hidden_size = 128 , layers = 3 )","title":"MiniImagenetCNN"},{"location":"docs/learn2learn.vision/#learn2learnvisiondatasets","text":"Description Some datasets commonly used in meta-learning vision tasks.","title":"learn2learn.vision.datasets"},{"location":"docs/learn2learn.vision/#fullomniglot","text":"1 FullOmniglot ( root , transform = None , target_transform = None , download = False ) [Source] Description This class provides an interface to the Omniglot dataset. The Omniglot dataset was introduced by Lake et al., 2015. Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples. While the original dataset is separated in background and evaluation sets, this class concatenates both sets and leaves to the user the choice of classes splitting as was done in Ravi and Larochelle, 2017. The background and evaluation splits are available in the torchvision package. References Lake et al. 2015. \u201cHuman-Level Concept Learning through Probabilistic Program Induction.\u201d Science. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 4 5 6 7 8 omniglot = l2l . vision . datasets . FullOmniglot ( root = './data' , transform = transforms . Compose ([ transforms . Resize ( 28 , interpolation = LANCZOS ), transforms . ToTensor (), lambda x : 1.0 - x , ]), download = True ) omniglot = l2l . data . MetaDataset ( omniglot )","title":"FullOmniglot"},{"location":"docs/learn2learn.vision/#miniimagenet","text":"1 MiniImagenet ( root , mode = 'train' , transform = None , target_transform = None ) [Source] Description The mini -ImageNet dataset was originally introduced by Vinyals et al., 2016. It consists of 60'000 colour images of sizes 84x84 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017. References Vinyals et al. 2016. \u201cMatching Networks for One Shot Learning.\u201d NeurIPS. Ravi and Larochelle. 2017. \u201cOptimization as a Model for Few-Shot Learning.\u201d ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . MiniImagenet ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways )","title":"MiniImagenet"},{"location":"docs/learn2learn.vision/#tieredimagenet","text":"1 2 3 4 5 TieredImagenet ( root , mode = 'train' , transform = None , target_transform = None , download = False ) [Source] Description The tiered -ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository. Like mini -ImageNet, tiered -ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100. The train-validation-test split is made such that classes from similar categories are in the same splits. There are 34 categories each containing between 10 and 30 classes. Of these categories, 20 (351 classes; 448,695 images) are used for training, 6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing. References Ren et al, 2018. \"Meta-Learning for Semi-Supervised Few-Shot Classification.\" ICLR '18. Ren Mengye. 2018. \"few-shot-ssl-public\". https://github.com/renmengye/few-shot-ssl-public Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . TieredImagenet ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"TieredImagenet"},{"location":"docs/learn2learn.vision/#fc100","text":"1 FC100 ( root , mode = 'train' , transform = None , target_transform = None ) [Source] Description The FC100 dataset was originally introduced by Oreshkin et al., 2018. It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are split so as to minimize the information overlap between splits. The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training, 4 (20 classes) for validation, and 4 (20 classes) for testing. Each class contains 600 images. The specific splits are provided in the Supplementary Material of the paper. Our data is downloaded from the link provided by [2]. References Oreshkin et al. 2018. \"TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning.\" NeurIPS. Kwoonjoon Lee. 2019. \"MetaOptNet.\" https://github.com/kjunelee/MetaOptNet Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . FC100 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"FC100"},{"location":"docs/learn2learn.vision/#cifarfs","text":"1 CIFARFS ( root , mode = 'train' , transform = None , target_transform = None ) [Source] Description The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019. It consists of 60'000 colour images of sizes 32x32 pixels. The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples. The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019. References Bertinetto et al. 2019. \"Meta-learning with differentiable closed-form solvers\". ICLR. Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. Example 1 2 3 train_dataset = l2l . vision . datasets . CIFARFS ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskGenerator ( dataset = train_dataset , ways = ways )","title":"CIFARFS"},{"location":"docs/learn2learn.vision/#vggflower102","text":"1 2 3 4 5 VGGFlower102 ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images. We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al. References Nilsback, M. and A. Zisserman. 2006. \"A Visual Vocabulary for Flower Classification.\" CVPR '06. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. https://www.robots.ox.ac.uk/~vgg/data/flowers/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . VGGFlower102 ( root = './data' , mode = 'train' ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"VGGFlower102"},{"location":"docs/learn2learn.vision/#fgvcaircraft","text":"1 2 3 4 5 FGVCAircraft ( root , mode = 'all' , transform = None , target_transform = None , download = False ) [Source] Description The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020. The dataset consists of 10,200 images of aircraft (102 classes, each 100 images). We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al. References Maji et al. 2013. \"Fine-Grained Visual Classification of Aircraft.\" arXiv [cs.CV]. Triantafillou et al. 2019. \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\" ICLR '20. http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/ Arguments root (str) - Path to download the data. mode (str, optional , default='train') - Which split to use. Must be 'train', 'validation', or 'test'. transform (Transform, optional , default=None) - Input pre-processing. target_transform (Transform, optional , default=None) - Target pre-processing. download (bool, optional , default=False) - Whether to download the dataset. Example 1 2 3 train_dataset = l2l . vision . datasets . FGVCAircraft ( root = './data' , mode = 'train' , download = True ) train_dataset = l2l . data . MetaDataset ( train_dataset ) train_generator = l2l . data . TaskDataset ( dataset = train_dataset , num_tasks = 1000 )","title":"FGVCAircraft"},{"location":"docs/learn2learn.vision/#learn2learnvisiontransforms","text":"Description A set of transformations commonly used in meta-learning vision tasks.","title":"learn2learn.vision.transforms"},{"location":"docs/learn2learn.vision/#randomclassrotation","text":"1 RandomClassRotation ( dataset , degrees ) [Source] Description Samples rotations from a given list uniformly at random, and applies it to all images from a given class. Arguments degrees (list) - The rotations to be sampled. Example 1 transform = RandomClassRotation ([ 0 , 90 , 180 , 270 ])","title":"RandomClassRotation"},{"location":"tutorials/getting_started/","text":"Getting Started \u00b6 learn2learn is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules. Installing \u00b6 A pip package is available, updated periodically. Use the command: pip install -U learn2learn For the most update-to-date version clone the repository and use: pip install -e . When installing from sources, make sure that Cython is installed: pip install cython . Info While learn2learn is actively used in current research projects, it is still in development. Breaking changes might occur. Development \u00b6 To simplify the development process, the following commands can be executed from the cloned sources: make build - Builds learn2learn in place. make clean - Cleans previous installation. make lint - Runs linting on the codebase. make lint-examples - Runs linting on the examples. make tests - Runs a light testing suite. (i.e. the Travis one) make alltests - Runs an extensive testing suite. (much longer) make docs - Builds the documentation and serves the website locally. Tip If you encounter a problem, feel free to an open an issue and we'll look into it.","title":"Getting Started"},{"location":"tutorials/getting_started/#getting-started","text":"learn2learn is a meta-learning library providing three levels of functionality for users. At a high level, there are many examples using meta-learning algorithms to train on a myriad of datasets/environments. At a mid level, it provides a functional interface for several popular meta-learning algorithms as well as a data loader to make it easier to import other data sets. At a low level, it provides extended functionality for modules.","title":"Getting Started"},{"location":"tutorials/getting_started/#installing","text":"A pip package is available, updated periodically. Use the command: pip install -U learn2learn For the most update-to-date version clone the repository and use: pip install -e . When installing from sources, make sure that Cython is installed: pip install cython . Info While learn2learn is actively used in current research projects, it is still in development. Breaking changes might occur.","title":"Installing"},{"location":"tutorials/getting_started/#development","text":"To simplify the development process, the following commands can be executed from the cloned sources: make build - Builds learn2learn in place. make clean - Cleans previous installation. make lint - Runs linting on the codebase. make lint-examples - Runs linting on the examples. make tests - Runs a light testing suite. (i.e. the Travis one) make alltests - Runs an extensive testing suite. (much longer) make docs - Builds the documentation and serves the website locally. Tip If you encounter a problem, feel free to an open an issue and we'll look into it.","title":"Development"}]}